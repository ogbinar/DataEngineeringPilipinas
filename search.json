[
  {
    "objectID": "technical-knowledgebase.html",
    "href": "technical-knowledgebase.html",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "This section provides a comprehensive overview of various technologies and tools in the field of data engineering.\n\n\n\nPostgreSQL: An open-source object-relational database system known for its reliability and robust features.\nMySQL: The most popular open-source SQL database management system, developed by Oracle Corporation.\nAmazon Relational Database System (RDS): A managed service for setting up and scaling databases in the cloud, supporting multiple database engines.\n\n\n\n\n\nAmazon Redshift: A cloud data warehouse known for its fast performance, available in both provisioned and serverless configurations.\nGoogle BigQuery: A serverless enterprise data warehouse that offers high scalability and cost-effectiveness.\n\n\n\n\n\nRedis: An open-source, in-memory key-value store known for its versatility and performance.\nAmazon DynamoDB: A fully managed, serverless NoSQL database designed for modern applications.\n\n\n\n\n\nAmazon S3: Offers scalable and secure object storage services.\nAzure Blob Storage: Ideal for storing large-scale cloud-native workloads, archives, and data lakes.\nGoogle Cloud Storage: A flexible service for storing and retrieving any amount of data.\n\n\n\n\n\nApache Kafka: A distributed event streaming platform with various implementations:\n\nConfluent’s Apache Kafka: A fully managed service with expert support.\nAmazon MSK: Amazon’s fully managed Kafka service.\n\nAWS SDK for pandas (AWS Wrangler): Extends pandas library to AWS, allowing seamless integration with AWS data services.\nAWS Kinesis: A cloud-based service for real-time data processing.\nAirbyte: An open-source data integration platform for ELT pipelines.\nPentaho Data Integration (Kettle): Includes both a core data integration engine and a graphical user interface for defining jobs and transformations.\n\n\n\n\n\nApache Avro: A serialization format ideal for streaming data pipelines.\nApache Parquet: An open-source columnar data file format.\nApache ORC: Optimized for Hadoop workloads.\n\n\n\n\n\nDelta Lake: An open-source storage framework by Databricks for building Lakehouse architecture.\nApache Iceberg: An open table format for huge analytical datasets, developed by Netflix.\nApache Hudi: A framework for managing large analytical datasets, created by Uber.\n\n\n\n\n\n\n\nApache Spark: A versatile engine for big data processing, available in various languages like Python (PySpark), Scala, Java, and R.\n\n\n\n\n\nPresto: A distributed SQL query engine for big data.\nApache Hive: Built on Hadoop, it facilitates reading, writing, and managing large datasets.\nApache Drill: An open-source SQL query engine.\nTrino: A SQL query engine designed for large data sets.\n\n\n\n\n\nAWS Elastic MapReduce (EMR): A cloud big data platform for processing large datasets.\nAWS Glue: A serverless data integration service.\n\n\n\n\n\n\nSpark Streaming: A part of Apache Spark for processing live data streams.\nSpark Structured Streaming: A stream processing engine built on Spark SQL.\nApache Flink: A framework for stateful computations over data streams.\nApache Storm: A system for processing streaming data in real-time.\n\n\n\n\nApache Druid: A high-performance real-time analytics database.\nApache Pinot: A real-time distributed OLAP datastore.\n\n\n\n\n\n\nApache Airflow: An open-source platform for managing complex computational workflows and data processing pipelines.\nMage: A modern replacement for Airflow for transforming and integrating data.\nDagster: An orchestration platform for data assets.\nPrefect: A workflow orchestration tool for data pipelines.\nKestra: An orchestrator for both scheduled and event-driven workflows.\nAWS Step Functions: Coordinates components of distributed applications.\n\n\n\n\n\n\n\nData Build Tool (dbt): A transformation workflow that follows software engineering best practices.\nSQLMesh: An open-source data transformation framework for SQL and Python.\n\n\n\n\n\n\n\n\nDataHub Project: An extensible metadata platform by LinkedIn.\nOpenMetadata: A platform for discovering, collaborating, and managing data.\nApache Atlas: An open-source metadata and governance framework.\nAmundsen: A data discovery and metadata engine by Lyft.\n\n\n\n\n\nGreat Expectations: A platform for data quality and observability.\n\n\n\n\n\n\nDatabricks: Offers a unified Data Lakehouse platform.\nSnowflake: A cloud-native data warehouse platform."
  },
  {
    "objectID": "technical-knowledgebase.html#relational-databases",
    "href": "technical-knowledgebase.html#relational-databases",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "PostgreSQL: An open-source object-relational database system known for its reliability and robust features.\nMySQL: The most popular open-source SQL database management system, developed by Oracle Corporation.\nAmazon Relational Database System (RDS): A managed service for setting up and scaling databases in the cloud, supporting multiple database engines."
  },
  {
    "objectID": "technical-knowledgebase.html#columnar-databases",
    "href": "technical-knowledgebase.html#columnar-databases",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Amazon Redshift: A cloud data warehouse known for its fast performance, available in both provisioned and serverless configurations.\nGoogle BigQuery: A serverless enterprise data warehouse that offers high scalability and cost-effectiveness."
  },
  {
    "objectID": "technical-knowledgebase.html#key-value-stores",
    "href": "technical-knowledgebase.html#key-value-stores",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Redis: An open-source, in-memory key-value store known for its versatility and performance.\nAmazon DynamoDB: A fully managed, serverless NoSQL database designed for modern applications."
  },
  {
    "objectID": "technical-knowledgebase.html#object-storage",
    "href": "technical-knowledgebase.html#object-storage",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Amazon S3: Offers scalable and secure object storage services.\nAzure Blob Storage: Ideal for storing large-scale cloud-native workloads, archives, and data lakes.\nGoogle Cloud Storage: A flexible service for storing and retrieving any amount of data."
  },
  {
    "objectID": "technical-knowledgebase.html#data-ingestion",
    "href": "technical-knowledgebase.html#data-ingestion",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Kafka: A distributed event streaming platform with various implementations:\n\nConfluent’s Apache Kafka: A fully managed service with expert support.\nAmazon MSK: Amazon’s fully managed Kafka service.\n\nAWS SDK for pandas (AWS Wrangler): Extends pandas library to AWS, allowing seamless integration with AWS data services.\nAWS Kinesis: A cloud-based service for real-time data processing.\nAirbyte: An open-source data integration platform for ELT pipelines.\nPentaho Data Integration (Kettle): Includes both a core data integration engine and a graphical user interface for defining jobs and transformations."
  },
  {
    "objectID": "technical-knowledgebase.html#data-formats",
    "href": "technical-knowledgebase.html#data-formats",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Avro: A serialization format ideal for streaming data pipelines.\nApache Parquet: An open-source columnar data file format.\nApache ORC: Optimized for Hadoop workloads."
  },
  {
    "objectID": "technical-knowledgebase.html#data-storage-framework",
    "href": "technical-knowledgebase.html#data-storage-framework",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Delta Lake: An open-source storage framework by Databricks for building Lakehouse architecture.\nApache Iceberg: An open table format for huge analytical datasets, developed by Netflix.\nApache Hudi: A framework for managing large analytical datasets, created by Uber."
  },
  {
    "objectID": "technical-knowledgebase.html#batch-processing",
    "href": "technical-knowledgebase.html#batch-processing",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Spark: A versatile engine for big data processing, available in various languages like Python (PySpark), Scala, Java, and R.\n\n\n\n\n\nPresto: A distributed SQL query engine for big data.\nApache Hive: Built on Hadoop, it facilitates reading, writing, and managing large datasets.\nApache Drill: An open-source SQL query engine.\nTrino: A SQL query engine designed for large data sets.\n\n\n\n\n\nAWS Elastic MapReduce (EMR): A cloud big data platform for processing large datasets.\nAWS Glue: A serverless data integration service."
  },
  {
    "objectID": "technical-knowledgebase.html#stream-processing",
    "href": "technical-knowledgebase.html#stream-processing",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Spark Streaming: A part of Apache Spark for processing live data streams.\nSpark Structured Streaming: A stream processing engine built on Spark SQL.\nApache Flink: A framework for stateful computations over data streams.\nApache Storm: A system for processing streaming data in real-time.\n\n\n\n\nApache Druid: A high-performance real-time analytics database.\nApache Pinot: A real-time distributed OLAP datastore."
  },
  {
    "objectID": "technical-knowledgebase.html#workflow-orchestration",
    "href": "technical-knowledgebase.html#workflow-orchestration",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Airflow: An open-source platform for managing complex computational workflows and data processing pipelines.\nMage: A modern replacement for Airflow for transforming and integrating data.\nDagster: An orchestration platform for data assets.\nPrefect: A workflow orchestration tool for data pipelines.\nKestra: An orchestrator for both scheduled and event-driven workflows.\nAWS Step Functions: Coordinates components of distributed applications."
  },
  {
    "objectID": "technical-knowledgebase.html#data-transformation",
    "href": "technical-knowledgebase.html#data-transformation",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Data Build Tool (dbt): A transformation workflow that follows software engineering best practices.\nSQLMesh: An open-source data transformation framework for SQL and Python."
  },
  {
    "objectID": "technical-knowledgebase.html#data-governance",
    "href": "technical-knowledgebase.html#data-governance",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "DataHub Project: An extensible metadata platform by LinkedIn.\nOpenMetadata: A platform for discovering, collaborating, and managing data.\nApache Atlas: An open-source metadata and governance framework.\nAmundsen: A data discovery and metadata engine by Lyft.\n\n\n\n\n\nGreat Expectations: A platform for data quality and observability."
  },
  {
    "objectID": "technical-knowledgebase.html#data-platforms",
    "href": "technical-knowledgebase.html#data-platforms",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Databricks: Offers a unified Data Lakehouse platform.\nSnowflake: A cloud-native data warehouse platform."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Free Resources",
    "section": "",
    "text": "IBM Data Engineering Basics for Everyone on edX - An introductory course to data engineering principles offered by IBM on edX.\nGoogle Cloud Data Engineering Path - A learning path designed to master data engineering on the Google Cloud Platform.\nMeta Database Engineer Professional Certificate on Coursera - A professional certification program by Meta on Coursera, focusing on database engineering.\nBig Data Specialization by UC San Diego on Coursera - A series of courses by UC San Diego on Coursera, covering big data analysis techniques and tools.\nData Engineering Zoomcamp on GitHub - A free, self-paced data engineering bootcamp hosted on GitHub, covering a range of tools and practices."
  },
  {
    "objectID": "resources.html#data-engineering-training",
    "href": "resources.html#data-engineering-training",
    "title": "Free Resources",
    "section": "",
    "text": "IBM Data Engineering Basics for Everyone on edX - An introductory course to data engineering principles offered by IBM on edX.\nGoogle Cloud Data Engineering Path - A learning path designed to master data engineering on the Google Cloud Platform.\nMeta Database Engineer Professional Certificate on Coursera - A professional certification program by Meta on Coursera, focusing on database engineering.\nBig Data Specialization by UC San Diego on Coursera - A series of courses by UC San Diego on Coursera, covering big data analysis techniques and tools.\nData Engineering Zoomcamp on GitHub - A free, self-paced data engineering bootcamp hosted on GitHub, covering a range of tools and practices."
  },
  {
    "objectID": "resources.html#data-engineering-books",
    "href": "resources.html#data-engineering-books",
    "title": "Free Resources",
    "section": "Data Engineering Books",
    "text": "Data Engineering Books\n\nFundamentals of Data Engineering - A guide by Redpanda to the basics of data engineering, covering essential concepts and practices.\nDatabase Technology Overview - An overview of database technology fundamentals.\nErwin in Database Design - Insights into using Erwin for database design.\nModern Data Engineering Playbook - ThoughtWorks presents strategies for modern data engineering, focusing on scalable, efficient solutions.\nThe Data Engineering Cookbook - A comprehensive guide for data engineering practices.\nData Engineering Design Patterns (DEDP) - An exploration of convergent evolution in data engineering, emphasizing design patterns for data systems."
  },
  {
    "objectID": "resources.html#sql-tutorials-and-courses",
    "href": "resources.html#sql-tutorials-and-courses",
    "title": "Free Resources",
    "section": "SQL Tutorials and Courses",
    "text": "SQL Tutorials and Courses\n\nW3 Schools SQL Tutorial - A comprehensive tutorial for SQL beginners.\nSQLBolt - Practice with real-world datasets and challenging SQL problems to deepen your skills."
  },
  {
    "objectID": "resources.html#interactive-sql-learning-games",
    "href": "resources.html#interactive-sql-learning-games",
    "title": "Free Resources",
    "section": "Interactive SQL Learning Games",
    "text": "Interactive SQL Learning Games\n\nSQL Murder Mystery - Solve a captivating murder case using your SQL skills in this immersive game-like environment. Suitable for beginners.\nSQL Island - Navigate through an adventure on SQL Island to learn SQL commands. Remember to change the language to English via the hamburger icon.\nSchemaVerse - A space-based strategy game where you use SQL commands to control your fleet and conquer the universe.\nLost at SQL - The SQL Learning Game - Enhance your SQL skills through this engaging learning game."
  },
  {
    "objectID": "resources.html#sql-practice-and-challenges",
    "href": "resources.html#sql-practice-and-challenges",
    "title": "Free Resources",
    "section": "SQL Practice and Challenges",
    "text": "SQL Practice and Challenges\n\n8 Week SQL Challenge - Intermediate/Advanced SQL challenges through interactive projects.\nHackerRank SQL Challenges - Test your mettle against others in coding challenges that push your SQL boundaries. Suitable for Intermediate-Advanced learners.\nCodewars SQL Kata - Hone your understanding with kata-style SQL exercises, practicing diverse concepts.\nSQLZoo - Practice with a wide range of exercises at different difficulty levels, mastering SQL fundamentals."
  },
  {
    "objectID": "resources.html#spreadsheets-for-data-analysis",
    "href": "resources.html#spreadsheets-for-data-analysis",
    "title": "Free Resources",
    "section": "Spreadsheets for Data Analysis",
    "text": "Spreadsheets for Data Analysis\n\nExcel Basics and Advanced Techniques\n\nEssential Spreadsheets Book 1 - University of York - Introduction to Excel.\nEssential Spreadsheets Book 2 - University of York - Advanced Excel techniques.\nExcel Video Training - Microsoft - Official tutorials from Microsoft.\n\n\n\nGoogle Sheets for Analysis\n\nGoogle Sheets Training & Help - Official Google Sheets guide.\nData Analysis in Google Sheets - Measure School - Analyzing data using Google Sheets.\nHow to Analyze Data in Google Sheets - Data analysis techniques in Google Sheets.\n\n\n\nComprehensive Data Analysis with Excel\n\nData Analysis Excel - Simplilearn - Simplilearn’s guide on Excel data analysis.\nExcel Data Analysis - TutorialsPoint - Detailed tutorials on Excel data analysis."
  },
  {
    "objectID": "resources.html#learn-python",
    "href": "resources.html#learn-python",
    "title": "Free Resources",
    "section": "Learn Python",
    "text": "Learn Python\nPython is a versatile and powerful programming language that’s great for beginners and professionals alike. Here are some resources to get started or enhance your Python skills:\n\nPython Official Getting Started Guide - Official Python documentation and guide on getting started with Python.\nLearnpython.org - A free interactive Python tutorial for people who want to learn Python, starting from the basics.\nOpenClassrooms - Learn Programming with Python - A course designed to introduce you to programming using Python.\nPrinciples of Computation with Python - Carnegie Mellon University - An open & free course by CMU focusing on computational principles using Python.\nCodecademy Python Course - Interactive Python programming courses for all levels.\nMIT OpenCourseWare - Introduction to Computer Science and Programming in Python - A course by MIT that introduces the fundamental ideas of computing using Python.\nHarvard University - Python Programming - Various Python programming courses offered by Harvard University.\nfreeCodeCamp - Learn Python - A collection of free Python courses for beginners to help learn Python programming.\nGoogle Cloud - Python on Google Cloud - Learn how to use Python with Google Cloud services for building and deploying applications."
  },
  {
    "objectID": "resources.html#learn-r",
    "href": "resources.html#learn-r",
    "title": "Free Resources",
    "section": "Learn R",
    "text": "Learn R\nR is a programming language and environment commonly used for statistical computing and graphics. The following resources provide comprehensive guides, tutorials, and courses for beginners to advanced users interested in learning R:\n\nCodecademy - Learn R - An interactive platform offering a course designed to get you started with R programming.\nW3Schools R Tutorial - Provides a quick and easy understanding of R, covering basics to advanced topics.\nRStudio Education - Learn R - Beginner resources compiled by RStudio, aiming to make learning R easier and more effective.\nProgramiz - Learn R Programming - Offers R tutorials for beginners to learn R programming online.\nCodecademy - R Catalog - Discover more R courses offered by Codecademy to deepen your understanding and skills.\nSwirl - Learn R, in R - A platform that offers interactive R programming lessons directly within the R console.\nHands-On Programming with R - Teaches how to perform data analysis with R through practical examples, covering basics to more advanced topics."
  },
  {
    "objectID": "resources.html#statistics-lessons-on-youtube",
    "href": "resources.html#statistics-lessons-on-youtube",
    "title": "Free Resources",
    "section": "Statistics Lessons on YouTube",
    "text": "Statistics Lessons on YouTube\n\nCrashCourse Statistics - Quick statistics overview.\nStatQuest - Statistics Fundamentals - Fundamental statistics concepts."
  },
  {
    "objectID": "resources.html#data-science-tutorials",
    "href": "resources.html#data-science-tutorials",
    "title": "Free Resources",
    "section": "Data Science Tutorials",
    "text": "Data Science Tutorials\n\nData Science for Beginners by Microsoft - Beginner-friendly data science materials on GitHub.\nHarvard CS50’s Introduction to Programming with Python - Full university course available on YouTube."
  },
  {
    "objectID": "resources.html#web-scraping-with-python",
    "href": "resources.html#web-scraping-with-python",
    "title": "Free Resources",
    "section": "Web Scraping with Python",
    "text": "Web Scraping with Python\nWeb scraping is a method used to extract data from websites. Python offers several libraries and tools for web scraping. Here are some essential resources to get started or enhance your web scraping skills:\n\nBeautiful Soup Documentation - Official documentation for Beautiful Soup, a Python library designed for quick turnaround projects like screen-scraping.\nPython Requests Library - Official documentation for Requests, a simple HTTP library for Python, used to send HTTP requests easily.\nScrape Quotes - A practice website designed for scraping quotes from famous authors.\nScrape This Site - A website that offers lessons and challenges for web scraping practices.\nfreeCodeCamp Web Scraping Python Tutorial - A comprehensive guide on how to scrape data from a website using Python.\nGeeksforGeeks Python Web Scraping Tutorial - Offers a tutorial on web scraping using Python, covering basics to advanced topics.\nBeautiful Soup Web Scraper Python - Real Python - A tutorial that explains how to use Beautiful Soup for web scraping effectively.\nPython Web Scraping - A Practical Introduction - Real Python - Provides a practical introduction to web scraping using Python, including setting up your environment and parsing HTML.\nFullstack Python - Provides free tutorials and guides on how to do both front end and back end in Python."
  },
  {
    "objectID": "resources.html#portfolio-building-resources",
    "href": "resources.html#portfolio-building-resources",
    "title": "Free Resources",
    "section": "Portfolio Building Resources",
    "text": "Portfolio Building Resources\nBuilding a professional portfolio is crucial for showcasing your skills and projects to potential employers or clients. Here are some valuable resources to help you create and enhance your portfolio:\n\nCreate a Portfolio Website with AlexTheAnalyst - A comprehensive tutorial on creating and hosting a portfolio using GitHub Pages. Perfect for beginners looking to establish an online presence.\nGoogle Analytics Tutorials:\n\nGoogle Analytics for Beginners - Learn how to track your portfolio or website visitors, understand your audience, and improve your site based on analytics data.\nAdvanced Google Analytics - Dive deeper into Google Analytics to leverage more complex tracking and analysis techniques for your site.\n\nYouTube Playlist on Data Engineering Projects - A curated list of video tutorials and project ideas for aspiring data engineers, covering various tools and technologies.\nData Engineer Portfolio Project Ideas - Provides a range of project ideas and guidance on building a portfolio that stands out for data engineering roles.\nData Engineering Projects for Beginners - Simplilearn - Offers tutorials and project ideas for beginners in data engineering, helping you to start building your portfolio with practical experience."
  },
  {
    "objectID": "resources.html#foundational-data-science-courses",
    "href": "resources.html#foundational-data-science-courses",
    "title": "Free Resources",
    "section": "Foundational Data Science Courses",
    "text": "Foundational Data Science Courses\n\nThe Open Source Data Science Masters - A comprehensive curriculum for self-study in data science.\nData Science Essentials- Microsoft - Foundation course on edX covering essential data science concepts.\nHarvard CS109 Data Science - In-depth course covering data science methodologies and Python.\nData Science Fundamentals – IBM - Covers fundamentals of data science through hands-on practice on the Cognitive Class platform."
  },
  {
    "objectID": "resources.html#introduction-to-data-science",
    "href": "resources.html#introduction-to-data-science",
    "title": "Free Resources",
    "section": "Introduction to Data Science",
    "text": "Introduction to Data Science\n\nIntroduction to Data Science by Jeff Hammerbacher at UC, Berkeley - Lectures and materials on the introduction to data science.\nIntroduction to Data Science @coursera - Offers a broad introduction to data science through Coursera.\nIntroduction to Data Science @UofWashington - Another Coursera course providing foundational knowledge in data science."
  },
  {
    "objectID": "resources.html#specialized-topics-in-data-science",
    "href": "resources.html#specialized-topics-in-data-science",
    "title": "Free Resources",
    "section": "Specialized Topics in Data Science",
    "text": "Specialized Topics in Data Science\n\nLearning from Data – California Institute of Technology & UCBerkeley - Focus on machine learning and data analysis.\nProcess Mining - Data Science in Action @TUEindhoven - Specialized course on process mining techniques.\nPattern Discovery in Data Mining @UoIllinois - Course on discovering patterns in datasets.\nIntroduction to Data Mining @MIT - Lecture notes and materials on data mining concepts.\nMining Massive Datasets @Stanford - Advanced course on dealing with massive datasets."
  },
  {
    "objectID": "resources.html#statistical-analysis-and-data-wrangling",
    "href": "resources.html#statistical-analysis-and-data-wrangling",
    "title": "Free Resources",
    "section": "Statistical Analysis and Data Wrangling",
    "text": "Statistical Analysis and Data Wrangling\n\nStatistical Thinking and Data Analysis @MIT & DukeUni - Courses focused on statistical thinking and data analysis techniques.\nOpen intro to Statistics - Provides a solid introduction to statistics.\nRegression Analysis - Learn approaches for analyzing multivariate data sets, emphasizing analysis of variance, linear regression, and logistic regression.\nIntroduction to Data Wrangling at the School of Data - Focuses on data cleaning and preparation techniques.\nElements of Statistical Learning by Hastie, Tibshirani, and Friedman\nPattern Recognition and Machine Learning by Bishop\nMathematics for Machine Learning by Deisenroth, Faisal, Soon Ong\nMachine Learning - a Probabilistic Perspective by Murphy\nProbabilistic Machine Learning - An Introduction by Murphy"
  },
  {
    "objectID": "resources.html#programming-for-data-science",
    "href": "resources.html#programming-for-data-science",
    "title": "Free Resources",
    "section": "Programming for Data Science",
    "text": "Programming for Data Science\n\nPython for Data Science - Course by Cognitive Class on using Python for data science tasks.\nData Science - R Basics - Harvard University - Introduction to using R for data science."
  },
  {
    "objectID": "resources.html#online-learning-platforms-and-resources",
    "href": "resources.html#online-learning-platforms-and-resources",
    "title": "Free Resources",
    "section": "Online Learning Platforms and Resources",
    "text": "Online Learning Platforms and Resources\n\nCognitive Class - Interactive platform offering courses on data science and AI.\nJovian - For sharing and collaborating on Jupyter notebooks.\nDataPen - Interactive tutorials on data science and machine learning.\nData Analytics Educational Resources - Collection of learning materials for data analytics.\nExecutive Levels Data Software Training - Offers training on various data software tools.\nSQLZOO - Interactive SQL tutorials for hands-on learning."
  },
  {
    "objectID": "resources.html#additional-learning-resources",
    "href": "resources.html#additional-learning-resources",
    "title": "Free Resources",
    "section": "Additional Learning Resources",
    "text": "Additional Learning Resources\n\nLearn Git Branching - Learn about Git\nGitHub - Platform for code sharing and collaboration.\nPLURALSIGHT - Offers video courses on a wide range of tech topics, including data science.\nhackerRank - Platform for practicing coding and data science skills.\nWorld Quant - Provides resources for quantitative analysis.\nReal World Data Science Use Cases - A showcase for data science in action\nSeattleDataGuy’s Newsletter - Learn About End-To-End Data Flows (Data Engineering, MLOps, and Data Science)"
  },
  {
    "objectID": "resources.html#training-and-internships",
    "href": "resources.html#training-and-internships",
    "title": "Free Resources",
    "section": "Training and Internships",
    "text": "Training and Internships\n\nThe Sparks Foundation - A remote one-month internship in Data Science and Business Analytics.\nFor The Women Foundation - FTW is a nonprofit organization providing free data science and technology training for women"
  },
  {
    "objectID": "resources.html#online-learning-platforms",
    "href": "resources.html#online-learning-platforms",
    "title": "Free Resources",
    "section": "Online Learning Platforms",
    "text": "Online Learning Platforms\n\nDataquest - An interactive learning platform focusing on data science and analytics skills.\nDatacamp - Offers hands-on courses on data science, Python, R, and SQL among others.\nCodecademy - Provides interactive programming courses across many different technology areas.\nSoloLearn - A mobile-first platform offering courses on a wide array of programming languages.\nW3Schools - A comprehensive resource for learning web development technologies and languages.\nKhan Academy - Free online courses in various subjects, including computer programming.\nCoursera - Online courses, specializations, and degrees from universities and educational institutions.\nedX - Access to online university-level courses in a wide range of disciplines.\nfreeCodeCamp - An open-source community providing free coding bootcamp and certifications.\nUdacity - Specializes in technology courses that offer Nanodegrees and certifications.\nThe Odin Project - A free coding curriculum that provides a complete path to web development.\nMozilla Developer Network (MDN) - Documentation and learning resources for web developers by Mozilla.\nKaggle - A platform for predictive modelling and analytics competitions and datasets."
  },
  {
    "objectID": "resources.html#cloud-resources",
    "href": "resources.html#cloud-resources",
    "title": "Free Resources",
    "section": "Cloud Resources",
    "text": "Cloud Resources\n\nAWS Training and Certification - Training and certification resources for Amazon Web Services.\nGoogle Cloud Training - Training resources for Google Cloud Platform.\nMicrosoft Learn - Learning resources for Microsoft technologies, including Azure.\nCloud Free Tier Comparison - Articles comparing free tier offers from AWS, Azure, GCP, and Oracle Cloud."
  },
  {
    "objectID": "resources.html#open-courseware",
    "href": "resources.html#open-courseware",
    "title": "Free Resources",
    "section": "Open Courseware",
    "text": "Open Courseware\n\nData Analysis with R - Comprehensive R course for data analysis.\nData Engineering Zoomcamp - Free, self-paced online course on data engineering.\nData Science in a Box w/ R - Curriculum for teaching and learning data science using R.\nThe Open Source Data Science Masters - Curriculum for a self-taught education in data science.\nFast.ai Courses - Practical deep learning for coders, taught by fast.ai.\nDive into Deep Learning - An interactive deep learning book with code, math, and discussions.\nGIS Programming Roadmap on GitHub - A roadmap for learning GIS programming.\nIntel Machine Learning Course - This course provides an overview of machine learning fundamentals on modern Intel architecture.\nIntel Deep Learning Course - This course provides an introduction to deep learning on modern Intel architecture."
  },
  {
    "objectID": "resources.html#open-books",
    "href": "resources.html#open-books",
    "title": "Free Resources",
    "section": "Open Books",
    "text": "Open Books\n\nData Science and R Programming\n\nR for Data Science - Learn data science with R.\nData Science - R Basics - Foundation of data science in R.\nR Graphics Cookbook - Data visualization with ggplot2 in R.\nGeocomputation with R - Spatial data analysis with R.\nExploratory Data Analysis with R - Techniques for EDA using R.\nThe Art of Data Science - The process of data analysis.\nR for Data Science (Second Edition) - Updated guide for data science with R.\nR Packages - Creating R packages.\nR datasciencebook - R datasciencebook.\n\n\n\nPython Programming\n\nPython for Everybody - Introduction to programming using Python.\nModern Polars - Using Polars library in Python for data manipulation.\nPython datasciencebook - Python datasciencebook.\n\n\n\nVisualization and Web Scraping\n\nData Visualization - A Practical Introduction - Introduction to data visualization.\nWeb Scraping with R - Techniques for web scraping using R.\nInteractive Data Visualization - Learning interactive data visualization.\n\n\n\nAdvanced Topics and Specialized Areas\n\nTelling Stories with Data - Narrative techniques in data communication.\nSpatial Data Science - Spatial data analysis.\nHands-On Programming with R - Practical programming with R.\nPython Geospatial Analysis - Geospatial analysis with Python.\nTime Series Analysis with R - Time series analysis techniques.\nCookbook for R Polars - Using Polars package in R for data frames.\nNFL Analytics with R - Analyzing NFL data with R.\nData Management in R - Strategies for data management with R.\nmlr3book - Machine learning in R with mlr3.\nPython for Geocomputation - Geocomputational analysis with Python.\nRaps with R - Music and analysis with R.\n\n\n\nMiscellaneous\n\nOpen Books by Open UMN - Free educational textbooks, including data science and statistics.\nPandas for Everyone - Comprehensive guide to using Pandas for data analysis.\nSaylor Academy - Free and open online courses for people everywhere.\nGoalkicker - Programming Notes for Professionals - Free programming books on various topics.\nPython Programming Tutorials - Comprehensive resource for learning Python.\nScratch - Imagine, Program, Share - Create stories, games, and animations.\nWaggle Dance - An interactive Python tutorial.\nInvent with Python - Books for learning Python with a focus on making things.\nOpen Source Society University - Computer Science - Path to a free self-taught education in Computer Science.\nLearn Python the Hard Way - Free eBook Download - A book for learning Python programming.\nData Analysis with Python - Spring 2020 - University of Helsinki’s course on Python for data analysis.\nExercism - Code practice and mentorship for everyone.\nGit and GitHub Tutorial for Beginners - Introduction to Git and GitHub.\nRStudio - Books - Resources for learning R programming."
  },
  {
    "objectID": "partners.html",
    "href": "partners.html",
    "title": "Partnerships",
    "section": "",
    "text": "Power BI Pilipinas - A hub for Power BI users in the Philippines.\nR Users Group - Philippines - Facebook page for R programming language users.\nAnalytics Association of the Philippines - The official website for the Analytics Association of the Philippines.\nDataSense Analytics Group - World-class training and development programs developed by top teachers and industry practitioners."
  },
  {
    "objectID": "partners.html#data-science-and-analytics",
    "href": "partners.html#data-science-and-analytics",
    "title": "Partnerships",
    "section": "",
    "text": "Power BI Pilipinas - A hub for Power BI users in the Philippines.\nR Users Group - Philippines - Facebook page for R programming language users.\nAnalytics Association of the Philippines - The official website for the Analytics Association of the Philippines.\nDataSense Analytics Group - World-class training and development programs developed by top teachers and industry practitioners."
  },
  {
    "objectID": "partners.html#artificial-intelligence-and-machine-learning",
    "href": "partners.html#artificial-intelligence-and-machine-learning",
    "title": "Partnerships",
    "section": "Artificial Intelligence and Machine Learning",
    "text": "Artificial Intelligence and Machine Learning\n\nPractical AI Philippines Meetup - A Meetup group for practical AI discussions.\nGen AI Philippines - Advancing the Filipino Community with AI Innovation"
  },
  {
    "objectID": "partners.html#software-development-open-source-innovation",
    "href": "partners.html#software-development-open-source-innovation",
    "title": "Partnerships",
    "section": "Software Development, Open Source, & Innovation",
    "text": "Software Development, Open Source, & Innovation\n\nPython Philippines - A community for Python enthusiasts in the Philippines.\nOpen Source Software PH - Open Source Software Philippines or OSSPH is a developer-led initiative to grow the community of developers building open source software across the Philippines\nArduino Day Philippines - Official Facebook Page of Arduino Day Philippines\nDEVCON - DEVCON Philippines is the largest community of software developers, geeks, and tech future makers.\nFilipino Web Development Peers - A group for forward-thinking individuals in various fields.\nJava User Group Philippines - Java User Group Philippines - A community for Java developers, students and enthusiasts in the Philippines to connect, share, and grow."
  },
  {
    "objectID": "partners.html#career-transition-and-volunteer-opportunities",
    "href": "partners.html#career-transition-and-volunteer-opportunities",
    "title": "Partnerships",
    "section": "Career Transition and Volunteer Opportunities",
    "text": "Career Transition and Volunteer Opportunities\n\nEudoxyz - Eudoxyz is an online community dedicated to cater professionals and aspirants in their respective journey.\nTech Career Shifter - A community for those looking to shift their careers into tech.\nTech Opportunities Philippines - This group is a hub for tech enthusiasts and like-minded individuals passionate about sharing exciting opportunities.\nCode.Sydney - Code.Sydney is a volunteering organisation that supports beginner developers transition to gain paid employment while helping non-profit and charity organisations with their app needs. Below are some of our projects."
  },
  {
    "objectID": "partners.html#student-organizations",
    "href": "partners.html#student-organizations",
    "title": "Partnerships",
    "section": "Student Organizations",
    "text": "Student Organizations\n\nStudent Developers Philippines - A space for student developers in the Philippines to collaborate and share knowledge.\nUP Data Science Society - The UP Data Science Society is a system-wide organization dedicated to empowering data enthusiasts\nRTU Organization of Statistics Students - The Official Page of the Organization of Statistics Students in Rizal Technological University - Boni\nGoogle Developer Student Clubs PLM - Google Developer Student Clubs PLM is a premiere student community of Haribons that shares a common\nThe SYNTAX Org - Community for explorers of the tech’s multiverse. Always #TowardsExcellence\nMicrosoft Student Community - TIP Manila - Microsoft Student Community is a tech community in T.I.P. Manila driven by a passion for different technologies."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Open Data Sources",
    "section": "",
    "text": "Explore various Philippine data sources for diverse data projects:\n\n\n\nPhilippine Standard Geographic Code (PSGC): A comprehensive list of administrative divisions in the Philippines.\nCOVID Data via DOH Data Drop: Official Department of Health COVID-19 data repository.\nPhilippine Statistics Authority (PSA): Various statistical datasets available for public use.\nPSA Annual Poverty Indicators Survey (API): Data on annual poverty indicators in the Philippines.\nPSA Family Income and Expenditure Survey (FIES): Information on family incomes and expenditures.\nCities and Municipalities Competitiveness Index (CMCI): Data assessing the competitiveness of cities and municipalities.\nBSP Banks Directory: Directory of banks and financial institutions in the Philippines.\nBSP Financial Service Access Points: A listing of financial service access points in the Philippines.\nUnified Accounts Code Structure (UACS): Standard for government accounting in the Philippines.\nOpen Data Philippines (beta): Portal for various government datasets.\nWorld Bank Philippines Data: Country-specific economic data and indicators.\nAwesome Data Philippines by BNHR: A curated collection of Philippine data resources.\n\n\n\n\n\nHydroSheds Data: Hydrological data and high-resolution mapping.\nEarthquake Data: Seismic activity data from around the world.\nHazard Data: Data from the World Environment Situation Room.\nNatural Resources Data: Datasets related to the Earth’s natural resources.\nTyphoon Data: Tracking and information on typhoons affecting the Philippines.\nClimate Data: Climate datasets provided by NOAA.\nLand Cover by ESRI: Worldwide data on land cover.\nMarine Boundaries: Detailed information on global marine regions.\nCoral Reefs: Data and mapping of coral reefs.\nPhilippines - Subnational Administrative Boundaries: Data on the administrative boundaries within the Philippines.\n\n\n\n\n\nEconomic and Social Database: Data provided by the Philippine Institute for Development Studies.\n\n\n\n\n\nGoogle Datasets: Explore datasets offered by Google Cloud.\nGoogle Research Datasets: Find datasets for research projects.\nBigQuery Public Datasets: Access a variety of public datasets through Google BigQuery.\n\n\n\n\n\nWorld Health Organization (WHO) Collections: Global health data.\nHealthcare Images: Public datasets of healthcare images.\nGenome Datasets: Data from the International Genome project.\nNOAA Data Products: Datasets from the National Oceanic and Atmospheric Administration.\nClimate Data: Comprehensive climate datasets.\n\n\n\n\n\nUnicef Data Collections: Data on global child welfare.\nUS Population Data: Population statistics from the US Bureau of Labor Statistics.\nStanford Open Policing Project: Data on police traffic stops.\nScikit-Learn Datasets: Datasets available in the Scikit-Learn library.\nPyTorch Datasets: Datasets available in PyTorch.\nHuggingface Datasets: A hub for various ML datasets.\nWikipedia ML Research Datasets: A list of datasets for machine learning research.\n\n\n\n\n\nWorld Health Organization (WHO) Collections: Access global health data.\nHealthcare Images: Explore public datasets of healthcare images.\nGenome Datasets: Utilize data from the International Genome project.\nNOAA Data Products: Find datasets from the National Oceanic and Atmospheric Administration.\nClimate Data: Research comprehensive climate datasets.\n\n\n\n\n\nUnicef Data Collections: Data on global child welfare.\nUS Population Data: Statistics from the US Bureau of Labor Statistics.\nStanford Open Policing Project: Information on police traffic stops.\nScikit-Learn Datasets: Datasets within the Scikit-Learn library.\nPyTorch Datasets: Datasets available through PyTorch.\nHuggingface Datasets: A platform offering various ML datasets.\nWikipedia ML Research Datasets: A comprehensive list of datasets for machine learning research.\nOECD\nDatahub: A free and open CKAN instance for hosting and sharing datasets.\nKaggle: A platform for data science competitions, and hosting, publishing, and analyzing data.\ndata.world: Described as a “social network for data people,” allows users to host and share data.\nAcademic Torrents: A BitTorrent-based platform for the academic community to download and share large datasets for free."
  },
  {
    "objectID": "datasets.html#philippine-data-sources",
    "href": "datasets.html#philippine-data-sources",
    "title": "Open Data Sources",
    "section": "",
    "text": "Explore various Philippine data sources for diverse data projects:\n\n\n\nPhilippine Standard Geographic Code (PSGC): A comprehensive list of administrative divisions in the Philippines.\nCOVID Data via DOH Data Drop: Official Department of Health COVID-19 data repository.\nPhilippine Statistics Authority (PSA): Various statistical datasets available for public use.\nPSA Annual Poverty Indicators Survey (API): Data on annual poverty indicators in the Philippines.\nPSA Family Income and Expenditure Survey (FIES): Information on family incomes and expenditures.\nCities and Municipalities Competitiveness Index (CMCI): Data assessing the competitiveness of cities and municipalities.\nBSP Banks Directory: Directory of banks and financial institutions in the Philippines.\nBSP Financial Service Access Points: A listing of financial service access points in the Philippines.\nUnified Accounts Code Structure (UACS): Standard for government accounting in the Philippines.\nOpen Data Philippines (beta): Portal for various government datasets.\nWorld Bank Philippines Data: Country-specific economic data and indicators.\nAwesome Data Philippines by BNHR: A curated collection of Philippine data resources.\n\n\n\n\n\nHydroSheds Data: Hydrological data and high-resolution mapping.\nEarthquake Data: Seismic activity data from around the world.\nHazard Data: Data from the World Environment Situation Room.\nNatural Resources Data: Datasets related to the Earth’s natural resources.\nTyphoon Data: Tracking and information on typhoons affecting the Philippines.\nClimate Data: Climate datasets provided by NOAA.\nLand Cover by ESRI: Worldwide data on land cover.\nMarine Boundaries: Detailed information on global marine regions.\nCoral Reefs: Data and mapping of coral reefs.\nPhilippines - Subnational Administrative Boundaries: Data on the administrative boundaries within the Philippines.\n\n\n\n\n\nEconomic and Social Database: Data provided by the Philippine Institute for Development Studies.\n\n\n\n\n\nGoogle Datasets: Explore datasets offered by Google Cloud.\nGoogle Research Datasets: Find datasets for research projects.\nBigQuery Public Datasets: Access a variety of public datasets through Google BigQuery.\n\n\n\n\n\nWorld Health Organization (WHO) Collections: Global health data.\nHealthcare Images: Public datasets of healthcare images.\nGenome Datasets: Data from the International Genome project.\nNOAA Data Products: Datasets from the National Oceanic and Atmospheric Administration.\nClimate Data: Comprehensive climate datasets.\n\n\n\n\n\nUnicef Data Collections: Data on global child welfare.\nUS Population Data: Population statistics from the US Bureau of Labor Statistics.\nStanford Open Policing Project: Data on police traffic stops.\nScikit-Learn Datasets: Datasets available in the Scikit-Learn library.\nPyTorch Datasets: Datasets available in PyTorch.\nHuggingface Datasets: A hub for various ML datasets.\nWikipedia ML Research Datasets: A list of datasets for machine learning research.\n\n\n\n\n\nWorld Health Organization (WHO) Collections: Access global health data.\nHealthcare Images: Explore public datasets of healthcare images.\nGenome Datasets: Utilize data from the International Genome project.\nNOAA Data Products: Find datasets from the National Oceanic and Atmospheric Administration.\nClimate Data: Research comprehensive climate datasets.\n\n\n\n\n\nUnicef Data Collections: Data on global child welfare.\nUS Population Data: Statistics from the US Bureau of Labor Statistics.\nStanford Open Policing Project: Information on police traffic stops.\nScikit-Learn Datasets: Datasets within the Scikit-Learn library.\nPyTorch Datasets: Datasets available through PyTorch.\nHuggingface Datasets: A platform offering various ML datasets.\nWikipedia ML Research Datasets: A comprehensive list of datasets for machine learning research.\nOECD\nDatahub: A free and open CKAN instance for hosting and sharing datasets.\nKaggle: A platform for data science competitions, and hosting, publishing, and analyzing data.\ndata.world: Described as a “social network for data people,” allows users to host and share data.\nAcademic Torrents: A BitTorrent-based platform for the academic community to download and share large datasets for free."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html",
    "href": "content/table_index/rmd/how_db_works.html",
    "title": "How SQL DB server works",
    "section": "",
    "text": "In the previous chapter, we set up our sandbox table sample_table with 4 columns and 50 million rows. Now, we will execute a few queries to understand how SQL databases retrieve the rows we request."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#connect-to-the-db",
    "href": "content/table_index/rmd/how_db_works.html#connect-to-the-db",
    "title": "How SQL DB server works",
    "section": "Connect to the DB",
    "text": "Connect to the DB"
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#initial-exploration",
    "href": "content/table_index/rmd/how_db_works.html#initial-exploration",
    "title": "How SQL DB server works",
    "section": "Initial exploration",
    "text": "Initial exploration\nLet’s take a look at what we are dealing with. We know we have entry_date in the table sorted in ascending order."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#start-mysql-profiling",
    "href": "content/table_index/rmd/how_db_works.html#start-mysql-profiling",
    "title": "How SQL DB server works",
    "section": "Start MySQL profiling",
    "text": "Start MySQL profiling"
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#query-1-retrieve-just-the-first-day",
    "href": "content/table_index/rmd/how_db_works.html#query-1-retrieve-just-the-first-day",
    "title": "How SQL DB server works",
    "section": "Query 1: Retrieve just the first day",
    "text": "Query 1: Retrieve just the first day\nWe know that the first day we have data for is at the top of the table.\nIt takes around 17 seconds.\nThe DBI library performed commit DB commands after the query, and again, after the SHOW PROFILES command.\nIf you are using a GUI client to connect to your DB, it may show a time for the query to execute. In MySQL Workbench, this would be shown as Duration/Fetch. And for this query, MySQL Workbench shows 0.029 sec / 17.224 sec. The sum of these is roughly the same as the duration shown in the profiles result.\nThe majority of the time is spent in executing."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#query-2-retrieve-just-the-last-day",
    "href": "content/table_index/rmd/how_db_works.html#query-2-retrieve-just-the-last-day",
    "title": "How SQL DB server works",
    "section": "Query 2: Retrieve just the last day",
    "text": "Query 2: Retrieve just the last day\nLet’s do the same for the last day we have, 2023-12-31. We know that the last day of the data we have is at the end of the table."
  },
  {
    "objectID": "content/table_index/readme.html",
    "href": "content/table_index/readme.html",
    "title": "Table Indexes",
    "section": "",
    "text": "Create and populate table\nHow an RDBMS retrieves data\nIndexed table\n[TODO] Filter by year\n[TODO] Group by year\n\n\n\n\n\nUnraid 6.11.5\nDocker 20.10.21\ni7-4770S 4C/8T\n16GB DDR3\n500GB Crucial MX500 SSD\nOther hardware exist but are not used in this demonstration\n\n\n\n\nThe RDBMS used for this demonstration is MySQL 8.1.0. It is running as a Docker container on top of an Unraid 6.11.5 server. The container instance is running un-constrained. It has full access to the CPU and memory resources. The MySQL data files reside on the SSD. Only the MySQL container will be running during the query executions.\nThe principles and (relative) results will be largely the same when other RDBMS are used. The syntax may be different, however.\n\n\nQuickest RDBMS for me to set up and tear down.\n\n\n\n\nThe raw source documents are written in R Markdown using R 4.3.2. R Markdown is similar in structure and function to a .ipynb Python notebook you may be familiar with.\n\n\nAs data engineers, you will have to support a variety of languages/environments/platforms used by different data-end-users. This is a way to expose you to a language other than Python.\n\n\n\n\n\nNone of the code presented are production-ready. It is advisable to use error-tolerant practices such as tryCatch() and DB transactions when working with DBs.\nAll queries are run only once. Ideally, each should be run at least 5 times, then the mean of each run is taken."
  },
  {
    "objectID": "content/table_index/readme.html#toc",
    "href": "content/table_index/readme.html#toc",
    "title": "Table Indexes",
    "section": "",
    "text": "Create and populate table\nHow an RDBMS retrieves data\nIndexed table\n[TODO] Filter by year\n[TODO] Group by year"
  },
  {
    "objectID": "content/table_index/readme.html#server-platform",
    "href": "content/table_index/readme.html#server-platform",
    "title": "Table Indexes",
    "section": "",
    "text": "Unraid 6.11.5\nDocker 20.10.21\ni7-4770S 4C/8T\n16GB DDR3\n500GB Crucial MX500 SSD\nOther hardware exist but are not used in this demonstration"
  },
  {
    "objectID": "content/table_index/readme.html#mysql-rdbms",
    "href": "content/table_index/readme.html#mysql-rdbms",
    "title": "Table Indexes",
    "section": "",
    "text": "The RDBMS used for this demonstration is MySQL 8.1.0. It is running as a Docker container on top of an Unraid 6.11.5 server. The container instance is running un-constrained. It has full access to the CPU and memory resources. The MySQL data files reside on the SSD. Only the MySQL container will be running during the query executions.\nThe principles and (relative) results will be largely the same when other RDBMS are used. The syntax may be different, however.\n\n\nQuickest RDBMS for me to set up and tear down."
  },
  {
    "objectID": "content/table_index/readme.html#r-markdown",
    "href": "content/table_index/readme.html#r-markdown",
    "title": "Table Indexes",
    "section": "",
    "text": "The raw source documents are written in R Markdown using R 4.3.2. R Markdown is similar in structure and function to a .ipynb Python notebook you may be familiar with.\n\n\nAs data engineers, you will have to support a variety of languages/environments/platforms used by different data-end-users. This is a way to expose you to a language other than Python."
  },
  {
    "objectID": "content/table_index/readme.html#disclaimers",
    "href": "content/table_index/readme.html#disclaimers",
    "title": "Table Indexes",
    "section": "",
    "text": "None of the code presented are production-ready. It is advisable to use error-tolerant practices such as tryCatch() and DB transactions when working with DBs.\nAll queries are run only once. Ideally, each should be run at least 5 times, then the mean of each run is taken."
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html",
    "href": "content/projects/ETL-SALES/readme.html",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "This project is an Extract, Transform, Load (ETL) package designed to merge data from various sources, including CSV files, Excel spreadsheets, and a PostgreSQL database, into a single destination database. The ETL process involves cleaning, transforming, and loading data to facilitate efficient analysis and reporting.\n\n\n\n\nExtracts data from CSV files, Excel spreadsheets, and a PostgreSQL database.\nApplies customizable transformations to clean and format the data.\nLoads the transformed data into a single destination database.\n\n\n\n\nRun the ETL pipeline: python PIPELINE_EXEC.py\n\n\n\n420171277_408831038167542_8129673976330505678_n\n\n\ndata set from link : DATA_SOURCE\nI separate csv files into different file sources to demonstrate extracting from different file sources.\nplease see below:\n\n\njanuary to april sales as excel (Sales_January_April_2019)\n\n\n\nmay to august sales as csv(sales_csv folder)\n\n\n\nseptember to december as database (sales_db)"
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html#overview",
    "href": "content/projects/ETL-SALES/readme.html#overview",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "This project is an Extract, Transform, Load (ETL) package designed to merge data from various sources, including CSV files, Excel spreadsheets, and a PostgreSQL database, into a single destination database. The ETL process involves cleaning, transforming, and loading data to facilitate efficient analysis and reporting."
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html#features",
    "href": "content/projects/ETL-SALES/readme.html#features",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "Extracts data from CSV files, Excel spreadsheets, and a PostgreSQL database.\nApplies customizable transformations to clean and format the data.\nLoads the transformed data into a single destination database."
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html#usage",
    "href": "content/projects/ETL-SALES/readme.html#usage",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "Run the ETL pipeline: python PIPELINE_EXEC.py\n\n\n\n420171277_408831038167542_8129673976330505678_n\n\n\ndata set from link : DATA_SOURCE\nI separate csv files into different file sources to demonstrate extracting from different file sources.\nplease see below:\n\n\njanuary to april sales as excel (Sales_January_April_2019)\n\n\n\nmay to august sales as csv(sales_csv folder)\n\n\n\nseptember to december as database (sales_db)"
  },
  {
    "objectID": "content/Cloud-Free-Tier-Comparison/LICENSE.html",
    "href": "content/Cloud-Free-Tier-Comparison/LICENSE.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "MIT License\nCopyright (c) 2020 Cloud Study Network.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "community.html",
    "href": "community.html",
    "title": "Community Content",
    "section": "",
    "text": "As a community, we want to support our homegrown content creators and experts. Please check them out as they champion data advocacy:\n\nDoc Ligot\nSherwin Pelayo\nKyle Escosia\nSandy C. Lauguico\nJosh Dev\nKuya Dev\nDev Stuff with JP\nPower BI Pilipinas\nAlex Gamboa\nAemyn Obinguar\nFurtim Dev\nProf. Bob S.\nDanielle Meer\nAtcha Abe\nThe Agile Geek"
  },
  {
    "objectID": "community.html#people-and-pages",
    "href": "community.html#people-and-pages",
    "title": "Community Content",
    "section": "",
    "text": "As a community, we want to support our homegrown content creators and experts. Please check them out as they champion data advocacy:\n\nDoc Ligot\nSherwin Pelayo\nKyle Escosia\nSandy C. Lauguico\nJosh Dev\nKuya Dev\nDev Stuff with JP\nPower BI Pilipinas\nAlex Gamboa\nAemyn Obinguar\nFurtim Dev\nProf. Bob S.\nDanielle Meer\nAtcha Abe\nThe Agile Geek"
  },
  {
    "objectID": "community.html#blogs-and-articles",
    "href": "community.html#blogs-and-articles",
    "title": "Community Content",
    "section": "Blogs and Articles",
    "text": "Blogs and Articles\n\nSnowflake in the Philippines - Kyle Escosia’s insights on Snowflake’s rise in the Philippines.\nUPSERTS and DELETS using AWS Glue and Delta Lake - A guide on using AWS Glue with Delta Lake for data operations.\nTable Indexes - An educational piece on the importance and implementation of table indexes.\nData Manipulation with Pandas Library Data Manipulation with Pandas Library by Bryan Castillo"
  },
  {
    "objectID": "community.html#projects",
    "href": "community.html#projects",
    "title": "Community Content",
    "section": "Projects",
    "text": "Projects\n\nBUILDING YOUR FIRSTEND-TO-END DATA PORTFOLIO by Josh Dev - Gagawa tayo ng end-to-end data project that involves data pipeline, dashboard, and machine learning pipeline to experience every main data role: data engineer, data analyst, and data scientist.\nBasic ETL project\nETL Sales\nETL Project with Azure Databricks - A walkthrough of an ETL project using Azure Databricks.\nContainerization - An exploration of containerization in data engineering."
  },
  {
    "objectID": "community.html#videos-and-presentations",
    "href": "community.html#videos-and-presentations",
    "title": "Community Content",
    "section": "Videos and Presentations",
    "text": "Videos and Presentations\n\nA brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\n An introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\n An online event with key thought leaders and content creators in the Filipino tech community.\n\nA panel discussion hosted by R User’s Group Philippines on transitioning to a career in data.\n Kyle Escosia’s talk on Big Data and Analytics, recorded live at an AWS Siklab Pilipinas event.\n A session by Kyle Escosia on creating a serverless data lake in AWS.\n\nThe “Kwentuhan Meetup” is an online event for members of the Data Engineering Pilipinas group, hosted on Discord."
  },
  {
    "objectID": "community.html#doc-ligot-interviews",
    "href": "community.html#doc-ligot-interviews",
    "title": "Community Content",
    "section": "Doc Ligot Interviews",
    "text": "Doc Ligot Interviews\n\nKuya Dev\n\n\n\nJosh V.\n\n\n\nSherwin Pelayo\n\n\n\nGerard\n\n\n\nXavier Puspus\n\n\n\nNoemi\n\n\n\nAemy Obinguar\n\n\n\nSandy Lauguico\n\n\n\nNeil Bacon’s Transition: Discover how Leoneil Bacon moved from Biology to Data.\n\n\n\nTrisha Nicdao’s Journey: Insights into Trisha’s shift from ESG to Data Analytics, touching on sustainability and GenZ perspectives.\n\n\n\nAngel Felismino’s Career Path: From Computer Science to leading in tech talent acquisition.\n\n\n\nEmmanuel Irog-Irog’s Innovations: Discussing RAG and LLMs with a focus on recent projects.\n\n\n\nJerel John Velarde’s Vision: Using data for enhancing democracy in AI startups."
  },
  {
    "objectID": "community.html#mentoring-program-2024",
    "href": "community.html#mentoring-program-2024",
    "title": "Community Content",
    "section": "Mentoring Program 2024",
    "text": "Mentoring Program 2024\n\nObjective\nTo create a network of mentors and mentees within the DE Pilipinas community, focusing on personalized advice and skill development in data engineering.\n\n\nThree Easy Steps\n\n\nSign Up & Share: Fill out a quick form to tell us about your skills, interests, and what you’re looking for—whether you want to mentor, learn, or both!\nPick Your Match: Browse profiles to find your perfect mentor or mentee. Start the conversation to see if you click, and set up a call to seal the deal.\nLearn & Grow Together: Dive into your mentorship journey, set goals, and check in regularly. Your feedback helps us make the experience even better for everyone.\n\n\n\nData Collection\n\nUtilize forms to collect information from potential mentors and mentees, including areas of expertise or interest, preferred contact methods, and availability.\nInformation to be updated regularly to ensure it reflects current details.\nParticipants must give consent for their information to be shared for pairing purposes.\nMentors register here.\nMentees register here.\n\n\n\nPairing Mechanism\n\nSelf-Initiated Pairing: Mentors and mentees can select their pairs based on the information provided in the forms.\nBoth mentors and mentees are urged to initiate contact, ideally outlining why they believe a partnership would be beneficial and providing relevant information to help the other party determine if there’s a mentor-mentee fit.\nOnce both parties initially agree to the pairing, they are encouraged to conduct a needs assessment call to confirm compatibility before officially commencing the program.\nA mentor can have multiple mentees, but a mentee can have one mentor at a time. One can be both a mentor and a mentee.\nTo find a mentor and/or a mentee, click here.\n\n\n\nMentor Commitment\n\nOur program places a strong emphasis on supporting our mentors. We believe that by taking good care of our experts, they in turn will be better equipped to nurture their mentees.\nMentors can specify the number of mentees they are willing to take on and their preferred mentoring strategy: some mentors may prefer a hands-on approach, actively guiding mentees through specific technical stacks. Others may opt for a more advisory role, providing strategic guidance and holding regular check-ins. This allows for a personalized mentorship experience that caters to the unique learning styles and objectives of each mentee.\nOptions for mentor involvement levels help prevent burnout and ensure a productive mentoring relationship.\n\n\n\nMentee Engagement\n\nMentees can identify their career stage and the type of help they need, allowing mentors to select mentees based on their expertise and interest areas.\nMentees play an active role in this mentorship program. They are expected to demonstrate a strong desire for learning and growth, as the value of the mentorship relationship is directly proportional to their commitment and engagement. Mentees are encouraged to take the initiative in following up, setting up meetings, and driving their learning journey. A balanced approach is expected, one that involves proactive engagement from the mentees while ensuring that the mentor’s time is respected.\n\n\n\nFeedback and Improvement\n\nRegular follow-ups and feedback collection from both mentors and mentees to assess the effectiveness of the mentorship and make necessary adjustments.\nConsider running polls within the group for additional feedback and suggestions.\n\n\n\nFAQs\nCan I participate if I’m not from the Philippines?\nYes.\nIs the mentoring program free?\nYes.\nIs there a certification or recognition at the end of the program?\nNone. [maybe in the future recognition for mentors, for mentees, the mentoring is the benefit :D]\nCan the mentor-mentee match be changed?\nYes, as long as both parties are both informed."
  },
  {
    "objectID": "about.html#our-mission",
    "href": "about.html#our-mission",
    "title": "About Data Engineering Pilipinas",
    "section": "Our Mission",
    "text": "Our Mission\nData Engineering Pilipinas is dedicated to democratizing data engineering education by offering open access to learning materials and promoting a collaborative environment for the exchange of tools and skills.\nThe community serves as a forum for both users and developers of data tools, facilitating the sharing of ideas and experiences. Data Engineering Pilipinas encourages discussions on best practices, innovative approaches, and emerging technologies in data management, processing, analytics, and visualization. It embraces a wide range of programming languages including Python, R, and SQL, reflecting the diverse methods used within data engineering and data science.\nWith a commitment to being an accessible, community-driven platform, Data Engineering Pilipinas caters to all levels of expertise, from novice to advanced practitioners. The conferences, tutorials, and talks provided by Data Engineering Pilipinas offer attendees insights into the latest project features as well as cutting-edge use cases."
  },
  {
    "objectID": "about.html#code-of-conduct-based-from-pydata",
    "href": "about.html#code-of-conduct-based-from-pydata",
    "title": "About Data Engineering Pilipinas",
    "section": "Code of Conduct (Based from PyData)",
    "text": "Code of Conduct (Based from PyData)\nBe kind to others. Do not insult or put down others. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes and language are not appropriate for Data Engineering Pilipinas.\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate.\nData Engineering Pilipinas is dedicated to providing a harassment-free event experience for everyone, regardless of gender, sexual orientation, gender identity, and expression, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of participants in any form.\nThank you for helping make this a welcoming, friendly community for all."
  },
  {
    "objectID": "about.html#official-community-guidelines",
    "href": "about.html#official-community-guidelines",
    "title": "About Data Engineering Pilipinas",
    "section": "Official Community Guidelines",
    "text": "Official Community Guidelines\nOur community thrives on respect, collaboration, and shared learning. Here are the guidelines we follow to ensure a positive and productive environment for all members.\n\nBe Respectful Treat all members with courtesy and respect, avoiding offensive language or personal attacks.\nShare Knowledge Encourage the sharing of valuable insights, best practices, and resources related to data engineering.\nNo Trolling or Harassment Engage constructively. Trolling or harassment of other members or administrators is not tolerated.\nAvoid Plagiarism Always attribute credit to the original source when sharing content or ideas.\nBe Mindful of Privacy Do not share personal information or sensitive data in our discussions.\nMinimal Self-Promotion Keep self-promotion or spamming of personal content or products to a minimum, especially if not related to data engineering.\nStay On Topic Ensure discussions are relevant to data engineering, its technologies, and related fields.\nNo Illegal Activities Do not engage in or promote discussions about activities that violate laws or ethical standards.\nLimit Recruitment Posts Job postings and collaboration requests are welcome but keep them moderate and relevant to data engineering roles or opportunities. Use appropriate hashtags like #hiring, #project, and #opportunity.\nKeep it Professional Maintain a professional tone in all interactions and adhere to Facebook’s guidelines and policies at all times."
  },
  {
    "objectID": "about.html#diversity-inclusion-based-from-pydata",
    "href": "about.html#diversity-inclusion-based-from-pydata",
    "title": "About Data Engineering Pilipinas",
    "section": "Diversity & Inclusion (Based from PyData)",
    "text": "Diversity & Inclusion (Based from PyData)\nData Engineering Pilipinas welcomes and encourages participation in our community by people of all backgrounds and identities. We are committed to promoting and sustaining a culture that values mutual respect, tolerance, and learning, and we work together as a community to help each other live out these values."
  },
  {
    "objectID": "content/Cloud-Free-Tier-Comparison/Articles.html",
    "href": "content/Cloud-Free-Tier-Comparison/Articles.html",
    "title": "Articles",
    "section": "",
    "text": "Articles\nhttps://www.cloudmanagementinsider.com/aws-azure-google-cloud-free-tier-comparison/\nhttps://n2ws.com/blog/amazon-aws-microsoft-azure-google-cloud-free-tier-cloud-computing-service-comparison\nhttps://www.infoworld.com/article/3179785/aws-vs-azure-vs-google-cloud-which-free-tier-is-best.html"
  },
  {
    "objectID": "content/data-engineering-101.html",
    "href": "content/data-engineering-101.html",
    "title": "Data Engineering 101",
    "section": "",
    "text": "Data Engineering Domain"
  },
  {
    "objectID": "content/data-engineering-101.html#what-is-data-engineering",
    "href": "content/data-engineering-101.html#what-is-data-engineering",
    "title": "Data Engineering 101",
    "section": "What is Data Engineering?",
    "text": "What is Data Engineering?\n\nPrimary Focus: Data engineering prepares data for analytical or operational use, emphasizing the practical application of data collection, transformation, and storage."
  },
  {
    "objectID": "content/data-engineering-101.html#roles-and-responsibilities",
    "href": "content/data-engineering-101.html#roles-and-responsibilities",
    "title": "Data Engineering 101",
    "section": "Roles and Responsibilities",
    "text": "Roles and Responsibilities\nAs a data engineer, you will be responsible for:\n\nBuilding and maintaining the infrastructure for data generation, collection, and distribution.\nDeveloping robust and scalable data pipelines that transform and transport data across systems.\nEnsuring data is readily available and in a usable format for analysts and data scientists to perform their tasks."
  },
  {
    "objectID": "content/data-engineering-101.html#skills-and-tools-for-data-engineering",
    "href": "content/data-engineering-101.html#skills-and-tools-for-data-engineering",
    "title": "Data Engineering 101",
    "section": "Skills and Tools for Data Engineering",
    "text": "Skills and Tools for Data Engineering\nTo thrive in data engineering, you will need to develop skills in:\n\nProgramming: Become proficient in languages like Python, Java, or Scala.\nData Management & Governance: Learn to manipulate databases using SQL.\nData Processing Frameworks: Gain expertise in tools such as Apache Hadoop and Apache Spark.\nData Storage and Warehousing: Understand how to implement and manage large-scale data storage solutions."
  },
  {
    "objectID": "content/data-engineering-101.html#related-disciplines",
    "href": "content/data-engineering-101.html#related-disciplines",
    "title": "Data Engineering 101",
    "section": "Related Disciplines",
    "text": "Related Disciplines\nData engineering intersects with several related fields:\n\nData Analysis\n\nDescription: Extracting insights and making sense of data.\nTools: Familiarize yourself with Excel, SQL, and BI tools like Tableau and Power BI.\n\n\n\nData Science\n\nDescription: Going beyond analysis to predict future trends and behaviors using data.\nTools: Learn Python, R, and machine learning libraries to build predictive models.\n\n\n\nMachine Learning Engineering\n\nDescription: Specializing in algorithms that can learn from and make decisions based on data.\nTools: Master Python and frameworks like TensorFlow.\n\n\n\nBusiness Intelligence (BI)\n\nDescription: Transforming data into actionable intelligence for business decisions.\nTools: Use SQL and BI platforms like Tableau, Power BI, or Looker.\n\n\n\nDatabase Administration\n\nDescription: Focusing on the technical management of database systems.\nTools: Understand database management systems like MySQL and PostgreSQL.\n\n\n\nBig Data\n\nDescription: Working with exceptionally large or complex data sets that require specialized approaches.\nTools: Learn to work with Hadoop, Spark, and NoSQL databases."
  },
  {
    "objectID": "content/data-engineering-101.html#the-data-engineering-lifecycle",
    "href": "content/data-engineering-101.html#the-data-engineering-lifecycle",
    "title": "Data Engineering 101",
    "section": "The Data Engineering Lifecycle",
    "text": "The Data Engineering Lifecycle\nUnderstanding the Data Engineering Lifecycle is crucial for managing data effectively:\n\nGeneration: Where and how data is produced.\nIngestion: Moving data to a place where it can be used.\nTransformation: Converting data to a useful format.\nServing: Making data accessible for use.\nStorage: Keeping data safe and retrievable."
  },
  {
    "objectID": "content/data-engineering-101.html#outcomes-of-the-data-engineering-process",
    "href": "content/data-engineering-101.html#outcomes-of-the-data-engineering-process",
    "title": "Data Engineering 101",
    "section": "Outcomes of the Data Engineering Process",
    "text": "Outcomes of the Data Engineering Process\nThe end goal of data engineering can be one of the following:\n\nAnalytics: Deriving insights that inform business strategies.\nMachine Learning: Training models to predict and act upon data.\nReverse ETL: Integrating processed data back into operational systems."
  },
  {
    "objectID": "content/data-engineering-101.html#supporting-practices-in-data-engineering",
    "href": "content/data-engineering-101.html#supporting-practices-in-data-engineering",
    "title": "Data Engineering 101",
    "section": "Supporting Practices in Data Engineering",
    "text": "Supporting Practices in Data Engineering\nThese are the undercurrents that ensure the data flows smoothly throughout the lifecycle:\n\nSecurity: Protecting data integrity and privacy.\nData Management: Ensuring that data is organized and maintained properly.\nDataOps: Streamlining the collaboration between teams working with data.\nData Architecture: Creating the blueprint for data collection and usage.\nOrchestration: Automating processes and workflows.\nSoftware Engineering: Developing the applications that handle data.\n\nBecoming a data engineer means you’ll be at the intersection of data, technology, and business, ensuring that data is a valuable asset that can be leveraged to its full potential."
  },
  {
    "objectID": "content/data-engineering-101.html#alternative-careers",
    "href": "content/data-engineering-101.html#alternative-careers",
    "title": "Data Engineering 101",
    "section": "Alternative Careers",
    "text": "Alternative Careers\n\nData Analyst: Analyzes data to help inform business decisions.\nMachine Learning Engineer: Creates algorithms to predict patterns and behaviors.\nDatabase Administrator: Manages and maintains database systems.\nBusiness Intelligence Analyst: Converts data into actionable business insights.\nData Architect: Designs and manages data solutions.\nData Science Generalist: Handles various data-related tasks in smaller companies.\nSystems Analyst: Improves IT systems through data analysis.\nProduct Manager: Integrates data insights into product strategy.\nOperations Analyst: Optimizes business operations using data.\nQuantitative Analyst: Applies data to financial analysis and risk assessment."
  },
  {
    "objectID": "content/data-engineering-101.html#advantages-of-data-engineering-skills-beyond-data-roles",
    "href": "content/data-engineering-101.html#advantages-of-data-engineering-skills-beyond-data-roles",
    "title": "Data Engineering 101",
    "section": "Advantages of Data Engineering Skills Beyond Data Roles",
    "text": "Advantages of Data Engineering Skills Beyond Data Roles\n\nEnhanced Problem-Solving: Develops structured approaches to solving complex issues.\nLogical Thinking: Fosters a logical mindset beneficial for strategic decision-making.\nTechnical Skills: Provides technical acumen applicable in many modern tech roles.\nData Literacy: Equips with the ability to understand and use data effectively.\nProject Management: Aligns with managing projects, resources, and workflows.\nEffective Communication: Improves the ability to communicate complex ideas clearly.\nAdaptability: Prepares for quick adaptation to industry changes.\nAutomation Knowledge: Offers insights into streamlining and automating processes.\nInterdisciplinary Collaboration: Encourages working across various teams and departments.\n\nLearning data engineering skills can significantly enhance your analytical and technical capabilities, useful in a wide array of professions, not limited to traditional data-centric roles."
  },
  {
    "objectID": "content/projects/pipeline_basic/readme.html",
    "href": "content/projects/pipeline_basic/readme.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "This is a basic ETL project"
  },
  {
    "objectID": "content/projects/pipeline_basic/readme.html#basic-etl-project",
    "href": "content/projects/pipeline_basic/readme.html#basic-etl-project",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "This is a basic ETL project"
  },
  {
    "objectID": "content/table_index/rmd/create_table.html",
    "href": "content/table_index/rmd/create_table.html",
    "title": "Create and populate table",
    "section": "",
    "text": "For our sample table, we will use a simple 4-column table that is a simulated result of an ETL process from some OLTP DB. We have the following values: - date value: You can think of this as an entry date, or a purchase date - numeric value: You can think of this as quantity of items, or a monetary value - descriptive value: You can think of this as a category code, or a branch code\nWe will then populate our table with 50 million rows randomly generated. These will be inserted in date order ascending. Why 50 million? So that we give the DB a little bit of a workout. A DB will not break a sweat with hundred-thousand-row tables."
  },
  {
    "objectID": "content/table_index/rmd/create_table.html#create-our-sample-table",
    "href": "content/table_index/rmd/create_table.html#create-our-sample-table",
    "title": "Create and populate table",
    "section": "Create our sample table",
    "text": "Create our sample table\nFirst, we create our table in our database with the following DDL:\nTake note that at this point, other than the primary key, we do not have any indexes defined.\n\n\n\nField\nData Type\nDescription\n\n\n\n\nid\nINT\nA simple unsigned primary key\n\n\nfk_id\nINT\nA simulated foreign key ID\n\n\narbitrary_value\nINT\nA value we can use aggregate functions on\n\n\nentry_date\nDATETIME\nA datetime value"
  },
  {
    "objectID": "content/table_index/rmd/create_table.html#populate-our-table",
    "href": "content/table_index/rmd/create_table.html#populate-our-table",
    "title": "Create and populate table",
    "section": "Populate our table",
    "text": "Populate our table\n\nGenerate data\nWe will now generate data for our table. To do this we use fixtuRes. We provide a YML configuration file:\n# sample_table.yml\nsample_table:\n  columns:\n    fk_id:\n      type: integer\n      min: 1\n      max: 100\n    arbitrary_value:\n      type: integer\n      min: 0\n      max: 50\n    entry_date:\n      type: date\n      min: 1973-01-01\n      max: 2023-12-31\n  arrange: entry_date\nWe use MockDataGenerator to create our data. This will produce a table ordered by entry_date. Then we add a sequential id column to have the id in the same order as entry_date.\nThe following is a sample output if we used size = 20:\n\n\nConnect to DB\nWe now establish a connection to our MySQL database server. The RMySQL library has been deprecated in favor of the RMariaDB library.\nIt is always good practice to keep your connection credentials like usernames, passwords, API tokens in your environment variables. Never hard-coded in source code. And never commit your environment variable file to the repo.\n\n\nWrite our data to the DB\nWhile there is a function we can use to write our mock_data to the DB table (dbWriteTable(conn, \"sample_table\", mock_data)), remember that we have 50 million rows. When mock_data is written to a CSV file, it results in a 1.3 GB file. Writing this to the DB will take some time. To avoid hitting the connection time-out constraint we will write the data by batches of date.\nLet’s take a look at what we have.\nDont’ leave any DB connections open!\nNow, we have a 50 million row DB table we can play around with."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html",
    "href": "content/table_index/rmd/indexed_table.html",
    "title": "Indexed table",
    "section": "",
    "text": "In the previous chapter, we saw that an SQL DB parses through the whole table to retrieve rows. Because it does not know where the rows that match the provided conditions are, it has to check every row. This is why it does not matter where in the table the rows are located. This is where INDEXes come in."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#duplicate-our-table",
    "href": "content/table_index/rmd/indexed_table.html#duplicate-our-table",
    "title": "Indexed table",
    "section": "Duplicate our table",
    "text": "Duplicate our table\nWe want to keep our non-indexed table so that we can still run non-indexed queries later.\nIt takes about 15 minutes to make a copy.\nThen we add the primary key and indexes for fk_id and entry_date.\nIt takes about 20 minutes to add these indices."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#database-connection",
    "href": "content/table_index/rmd/indexed_table.html#database-connection",
    "title": "Indexed table",
    "section": "Database Connection",
    "text": "Database Connection\nIn the background, we set up our environment, connect to the database, and turn on profiling.\nA quick check of our tables:\nOur tables are the same except for the indexes."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#show-profile-function",
    "href": "content/table_index/rmd/indexed_table.html#show-profile-function",
    "title": "Indexed table",
    "section": "Show Profile function",
    "text": "Show Profile function\nBecause we will run profiling repeatedly, it makes sense to write it into a function."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-6-run-the-first-day-query-with-the-benefit-of-an-index",
    "href": "content/table_index/rmd/indexed_table.html#query-6-run-the-first-day-query-with-the-benefit-of-an-index",
    "title": "Indexed table",
    "section": "Query 6: Run the “first day” query with the benefit of an index",
    "text": "Query 6: Run the “first day” query with the benefit of an index\nFrom 17 seconds, we are now down to below 0.007 seconds."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-7-run-the-last-day-query-with-the-benefit-of-an-index",
    "href": "content/table_index/rmd/indexed_table.html#query-7-run-the-last-day-query-with-the-benefit-of-an-index",
    "title": "Indexed table",
    "section": "Query 7: Run the “last day” query with the benefit of an index",
    "text": "Query 7: Run the “last day” query with the benefit of an index\nIt is the same for our “last day” query. Below 0.007 seconds. The DB is not parsing the entire table anymore.\nBut, this is a little bit of a cheat. Remember that entry_date is already sorted. Rows with the same entry_dates are together. How much difference is there if the needles are scattered all over the table?"
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-8-retrieve-all-fk_id-45",
    "href": "content/table_index/rmd/indexed_table.html#query-8-retrieve-all-fk_id-45",
    "title": "Indexed table",
    "section": "Query 8: Retrieve all fk_id = 45",
    "text": "Query 8: Retrieve all fk_id = 45\nWe have a different column we can filter on. fk_id is not sorted. It is randomly distributed across the entire table. Let’s run a baseline on the non-indexed table.\nAs expected, we get about 17 seconds."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-9-retrieve-all-fk_id-45-with-the-help-of-an-index",
    "href": "content/table_index/rmd/indexed_table.html#query-9-retrieve-all-fk_id-45-with-the-help-of-an-index",
    "title": "Indexed table",
    "section": "Query 9: Retrieve all fk_id = 45 with the help of an index",
    "text": "Query 9: Retrieve all fk_id = 45 with the help of an index\nThen the same query from the indexed table.\nWe have a substantial improvement from 17 seconds to less than 5 seconds."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#questions",
    "href": "content/table_index/rmd/indexed_table.html#questions",
    "title": "Indexed table",
    "section": "Questions",
    "text": "Questions\n\nWhat if we need to filter by year? Or by year-month?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "Data Engineering Pilipinas is a community for data engineers, data analysts, data scientists, developers, AI / ML engineers, and users of closed and open source data tools and methods / techniques in the Philippines. Data Engineering Pilipinas is a PyData group.\nThis page serves as a repository of notes, thoughts, ideas, plans, dreams, datasets, analyses, and whatever else we think of."
  },
  {
    "objectID": "index.html#getting-started-with-data-engineering",
    "href": "index.html#getting-started-with-data-engineering",
    "title": "Data Engineering Pilipinas",
    "section": "Getting Started with Data Engineering",
    "text": "Getting Started with Data Engineering\n\nRead more about Data Engineering 101"
  },
  {
    "objectID": "index.html#join-our-growing-community",
    "href": "index.html#join-our-growing-community",
    "title": "Data Engineering Pilipinas",
    "section": "Join Our Growing Community",
    "text": "Join Our Growing Community\nConnect with Data Engineering Pilipinas on various platforms. Like, follow, and join our groups and pages to stay updated and engage with our community:"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Community Contents",
    "section": "",
    "text": "Explore a rich collection of community-driven content, featuring insightful videos, blogs, and articles from data engineering experts. These resources offer valuable perspectives on various aspects of data engineering and analytics.\n\n\n\n\nA brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\n\n\n\nAn introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\n\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\n\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\n\n\n\nAn online event with key thought leaders and content creators in the Filipino tech community.\n\n\n\nA panel discussion hosted by R User’s Group Philippines on transitioning to a career in data.\n\n\n\n\n\n\n\nUnlocking the Future: Discussing Shifting into Tech with Kuya Dev\n\n\n\n\n\nUnlocking Data Engineering with Josh Valdeleon\n\n\n\n\n\nUnlocking Career Opportunities: Philippine Skills Framework for AI and Analytics with Sherwin Pelayo\n\n\n\n\n\nDiscussion on Data Engineering and Analytics with Gerard\n\n\n\n\n\nXavier Puspus talks about minting AI engineers through education\n\n\n\n\n\nDiscussing Tech and Data Careers with Noemi\n\n\n\n\n\nFrom Dropout to Tech Star: Aemy Obinguar’s Inspirational Tale\n\n\n\n\n\nExclusive Interview: Sandy Lauguico’s Data Engineering Transition\n\n\n\n\n\n\n\nKyle Escosia’s talk on Big Data and Analytics, recorded live at an AWS Siklab Pilipinas event.\n\n\n\nA session by Kyle Escosia on creating a serverless data lake in AWS.\n\n\n\n\n\nSnowflake in the Philippines - Kyle Escosia’s insights on Snowflake’s rise in the Philippines.\nUPSERTS and DELETS using AWS Glue and Delta Lake - A guide on using AWS Glue with Delta Lake for data operations.\nTable Indexes - An educational piece on the importance and implementation of table indexes.\n\n\n\n\n\nBasic ETL project\nETL Project with Azure Databricks - A walkthrough of an ETL project using Azure Databricks.\nContainerization - An exploration of containerization in data engineering.\n\n\n\n\n\nKyle Escosia - Explore the work of Kyle Escosia, a Data Engineer with a passion for all things data."
  },
  {
    "objectID": "readme.html#videos-and-presentations",
    "href": "readme.html#videos-and-presentations",
    "title": "Community Contents",
    "section": "",
    "text": "A brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\n\n\n\nAn introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\n\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\n\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\n\n\n\nAn online event with key thought leaders and content creators in the Filipino tech community.\n\n\n\nA panel discussion hosted by R User’s Group Philippines on transitioning to a career in data."
  },
  {
    "objectID": "readme.html#doc-ligot-interviews",
    "href": "readme.html#doc-ligot-interviews",
    "title": "Community Contents",
    "section": "",
    "text": "Unlocking the Future: Discussing Shifting into Tech with Kuya Dev\n\n\n\n\n\nUnlocking Data Engineering with Josh Valdeleon\n\n\n\n\n\nUnlocking Career Opportunities: Philippine Skills Framework for AI and Analytics with Sherwin Pelayo\n\n\n\n\n\nDiscussion on Data Engineering and Analytics with Gerard\n\n\n\n\n\nXavier Puspus talks about minting AI engineers through education\n\n\n\n\n\nDiscussing Tech and Data Careers with Noemi\n\n\n\n\n\nFrom Dropout to Tech Star: Aemy Obinguar’s Inspirational Tale\n\n\n\n\n\nExclusive Interview: Sandy Lauguico’s Data Engineering Transition"
  },
  {
    "objectID": "readme.html#recorded-events-and-talks",
    "href": "readme.html#recorded-events-and-talks",
    "title": "Community Contents",
    "section": "",
    "text": "Kyle Escosia’s talk on Big Data and Analytics, recorded live at an AWS Siklab Pilipinas event.\n\n\n\nA session by Kyle Escosia on creating a serverless data lake in AWS."
  },
  {
    "objectID": "readme.html#blogs-and-articles",
    "href": "readme.html#blogs-and-articles",
    "title": "Community Contents",
    "section": "",
    "text": "Snowflake in the Philippines - Kyle Escosia’s insights on Snowflake’s rise in the Philippines.\nUPSERTS and DELETS using AWS Glue and Delta Lake - A guide on using AWS Glue with Delta Lake for data operations.\nTable Indexes - An educational piece on the importance and implementation of table indexes."
  },
  {
    "objectID": "readme.html#projects",
    "href": "readme.html#projects",
    "title": "Community Contents",
    "section": "",
    "text": "Basic ETL project\nETL Project with Azure Databricks - A walkthrough of an ETL project using Azure Databricks.\nContainerization - An exploration of containerization in data engineering."
  },
  {
    "objectID": "readme.html#people-and-pages",
    "href": "readme.html#people-and-pages",
    "title": "Community Contents",
    "section": "",
    "text": "Kyle Escosia - Explore the work of Kyle Escosia, a Data Engineer with a passion for all things data."
  },
  {
    "objectID": "study-roadmap.html",
    "href": "study-roadmap.html",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "DataEngineerRoadmap_Notion - Data Engineering roadmap with a variety of course options from free to paid.\n\n\n\n\n\nSQL Roadmap - A guide for becoming proficient in SQL.\nPostgreSQL DBA Roadmap - A roadmap for those aspiring to become PostgreSQL Database Administrators.\nPython Roadmap - A detailed path for learning Python programming.\nBackend Development Roadmap - A guide for becoming a Backend Developer.\nAI & Data Scientist Roadmap - A comprehensive path for aspiring AI and Data Scientists.\n\n\n\n\n\nFundamentals Roadmap - A guide for understanding the fundamentals necessary in AI and machine learning.\nData Science Roadmap - A comprehensive guide to becoming a Data Scientist.\nMachine Learning Roadmap - A detailed pathway for learning Machine Learning.\nDeep Learning Roadmap - A structured guide for mastering Deep Learning.\nData Engineer Roadmap - A roadmap for becoming a Data Engineer.\nBig Data Engineer Roadmap - A guide for those looking to specialize in Big Data Engineering.\n\n\n\n\n\nUltimate Data Analytics Career Roadmap - From Data Analyst to Data Engineer as Individual Contributed (IC)\n\n\n\n\nThese are specific courses from Data Camp curated by Nicksy.\n\n\n\n\n\nData Engineering\n\n\n\nFoundational Data Engineering Skills\n\nUnderstanding Data Engineering (Beginner)\nIntroduction to Data Visualization (Beginner)\nUnderstanding Cloud Computing (Beginner)\nIntroduction to Git (Beginner)\nIntroduction to Shell (Beginner)\nProject: Designing a Bank Marketing Database (Project)\n\nSQL Data Management\n\nIntroduction to SQL (Beginner)\nIntermediate SQL (Intermediate)\nJoining Data in SQL (Intermediate)\nIntroduction to Relational Databases in SQL (Intermediate)\nDatabase Design (Advanced)\nStreamlined Data Ingestion with pandas (Intermediate)\n\nPython Programming and Data Handling\n\nIntroduction to Python (Beginner)\nIntermediate Python (Intermediate)\nIntroduction to Importing Data in Python (Intermediate)\nData Manipulation with pandas (Intermediate)\nJoining Data with pandas (Intermediate)\nPython Data Science Toolbox (Part 1) (Intermediate)\nPython Data Science Toolbox (Part 2) (Intermediate)\nSoftware Engineering Principles in Python (Intermediate)\nCleaning Data in Python (Intermediate)\nData Types for Data Science in Python (Intermediate)\nWriting Efficient Python Code (Advanced)\n\n\n\n\n\n\n\n\nData Analyst\n\n\n\nData Analyst in SQL Path\n\nIntroduction to SQL (Beginner)\nIntermediate SQL (Intermediate)\nJoining Data in SQL (Intermediate)\nData Manipulation in SQL (Intermediate)\nPostgreSQL Summary Stats and Window Functions (Advanced)\nFunctions for Manipulating Data in PostgreSQL (Advanced)\nData-Driven Decision Making in SQL (Advanced)\nExploratory Data Analysis in SQL (Advanced)\nProject: When Was the Golden Age of Video Games? (Project)\n\nData Analyst in Python Path\n\nIntroduction to Python (Beginner)\nIntermediate Python (Intermediate)\nData Manipulation with pandas (Intermediate)\nIntroduction to Data Science in Python (Intermediate)\nIntroduction to Data Visualization with Seaborn (Intermediate)\nIntroduction to Statistics in Python (Intermediate)\nJoining Data with pandas (Intermediate)\nSampling in Python (Advanced)\nHypothesis Testing in Python (Advanced)\nExploratory Data Analysis in Python (Advanced)\n\nData Analyst in R Path\n\nIntroduction to R (Beginner)\nIntermediate R (Intermediate)\nIntroduction to the Tidyverse (Intermediate)\nData Manipulation with dplyr (Intermediate)\nIntroduction to Data Visualization with ggplot2 (Intermediate)\nIntroduction to Statistics in R (Intermediate)\nJoining Data with dplyr (Intermediate)\nSampling in R (Advanced)\nHypothesis Testing in R (Advanced)\nExploratory Data Analysis in R (Advanced)\n\n\n\n\n\n\n\nYou can use this to generate your own custom study roadmap\n\n\n\n\n\nCheck this to Read more about Data Engineering 101\n\n\n\n\nBelow is an on-going project to propose a professional skills framework for data, analytics, and AI careers in the Philippines.\n\nProposed Professional Skills Framework ANALYTICS & ARTIFICIAL INTELLIGENCE"
  },
  {
    "objectID": "study-roadmap.html#data-engineering-by-sandy",
    "href": "study-roadmap.html#data-engineering-by-sandy",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "DataEngineerRoadmap_Notion - Data Engineering roadmap with a variety of course options from free to paid."
  },
  {
    "objectID": "study-roadmap.html#roadmap.sh",
    "href": "study-roadmap.html#roadmap.sh",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "SQL Roadmap - A guide for becoming proficient in SQL.\nPostgreSQL DBA Roadmap - A roadmap for those aspiring to become PostgreSQL Database Administrators.\nPython Roadmap - A detailed path for learning Python programming.\nBackend Development Roadmap - A guide for becoming a Backend Developer.\nAI & Data Scientist Roadmap - A comprehensive path for aspiring AI and Data Scientists."
  },
  {
    "objectID": "study-roadmap.html#i.am.ai",
    "href": "study-roadmap.html#i.am.ai",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "Fundamentals Roadmap - A guide for understanding the fundamentals necessary in AI and machine learning.\nData Science Roadmap - A comprehensive guide to becoming a Data Scientist.\nMachine Learning Roadmap - A detailed pathway for learning Machine Learning.\nDeep Learning Roadmap - A structured guide for mastering Deep Learning.\nData Engineer Roadmap - A roadmap for becoming a Data Engineer.\nBig Data Engineer Roadmap - A guide for those looking to specialize in Big Data Engineering."
  },
  {
    "objectID": "study-roadmap.html#surfalytics",
    "href": "study-roadmap.html#surfalytics",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "Ultimate Data Analytics Career Roadmap - From Data Analyst to Data Engineer as Individual Contributed (IC)"
  },
  {
    "objectID": "study-roadmap.html#data-camp-roadmaps-by-nicksy",
    "href": "study-roadmap.html#data-camp-roadmaps-by-nicksy",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "These are specific courses from Data Camp curated by Nicksy.\n\n\n\n\n\nData Engineering\n\n\n\nFoundational Data Engineering Skills\n\nUnderstanding Data Engineering (Beginner)\nIntroduction to Data Visualization (Beginner)\nUnderstanding Cloud Computing (Beginner)\nIntroduction to Git (Beginner)\nIntroduction to Shell (Beginner)\nProject: Designing a Bank Marketing Database (Project)\n\nSQL Data Management\n\nIntroduction to SQL (Beginner)\nIntermediate SQL (Intermediate)\nJoining Data in SQL (Intermediate)\nIntroduction to Relational Databases in SQL (Intermediate)\nDatabase Design (Advanced)\nStreamlined Data Ingestion with pandas (Intermediate)\n\nPython Programming and Data Handling\n\nIntroduction to Python (Beginner)\nIntermediate Python (Intermediate)\nIntroduction to Importing Data in Python (Intermediate)\nData Manipulation with pandas (Intermediate)\nJoining Data with pandas (Intermediate)\nPython Data Science Toolbox (Part 1) (Intermediate)\nPython Data Science Toolbox (Part 2) (Intermediate)\nSoftware Engineering Principles in Python (Intermediate)\nCleaning Data in Python (Intermediate)\nData Types for Data Science in Python (Intermediate)\nWriting Efficient Python Code (Advanced)\n\n\n\n\n\n\n\n\nData Analyst\n\n\n\nData Analyst in SQL Path\n\nIntroduction to SQL (Beginner)\nIntermediate SQL (Intermediate)\nJoining Data in SQL (Intermediate)\nData Manipulation in SQL (Intermediate)\nPostgreSQL Summary Stats and Window Functions (Advanced)\nFunctions for Manipulating Data in PostgreSQL (Advanced)\nData-Driven Decision Making in SQL (Advanced)\nExploratory Data Analysis in SQL (Advanced)\nProject: When Was the Golden Age of Video Games? (Project)\n\nData Analyst in Python Path\n\nIntroduction to Python (Beginner)\nIntermediate Python (Intermediate)\nData Manipulation with pandas (Intermediate)\nIntroduction to Data Science in Python (Intermediate)\nIntroduction to Data Visualization with Seaborn (Intermediate)\nIntroduction to Statistics in Python (Intermediate)\nJoining Data with pandas (Intermediate)\nSampling in Python (Advanced)\nHypothesis Testing in Python (Advanced)\nExploratory Data Analysis in Python (Advanced)\n\nData Analyst in R Path\n\nIntroduction to R (Beginner)\nIntermediate R (Intermediate)\nIntroduction to the Tidyverse (Intermediate)\nData Manipulation with dplyr (Intermediate)\nIntroduction to Data Visualization with ggplot2 (Intermediate)\nIntroduction to Statistics in R (Intermediate)\nJoining Data with dplyr (Intermediate)\nSampling in R (Advanced)\nHypothesis Testing in R (Advanced)\nExploratory Data Analysis in R (Advanced)"
  },
  {
    "objectID": "study-roadmap.html#globe.engineer",
    "href": "study-roadmap.html#globe.engineer",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "You can use this to generate your own custom study roadmap"
  },
  {
    "objectID": "study-roadmap.html#data-engineering-101",
    "href": "study-roadmap.html#data-engineering-101",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "Check this to Read more about Data Engineering 101"
  },
  {
    "objectID": "study-roadmap.html#philippines-skills-framework",
    "href": "study-roadmap.html#philippines-skills-framework",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "Below is an on-going project to propose a professional skills framework for data, analytics, and AI careers in the Philippines.\n\nProposed Professional Skills Framework ANALYTICS & ARTIFICIAL INTELLIGENCE"
  },
  {
    "objectID": "study-roadmap.html#graduate-programs-in-data-science-and-related-fields-in-the-philippines",
    "href": "study-roadmap.html#graduate-programs-in-data-science-and-related-fields-in-the-philippines",
    "title": "Study Roadmap for Beginners",
    "section": "Graduate Programs in Data Science and Related Fields in the Philippines",
    "text": "Graduate Programs in Data Science and Related Fields in the Philippines\nThese are the more traditional universities with data programs in the Philippines.\n\nUPD Department of Statistics\nUPD College of Science\nUPD College of Engineering\nTIP Graduate Programs\nDLSU Data Science Institute\nMSAI Program\nMapua Institute of Technology\nADMU Global\nAIM MSc in Data Science\nUA&P Graduate Programs"
  },
  {
    "objectID": "study-roadmap.html#open-universities",
    "href": "study-roadmap.html#open-universities",
    "title": "Study Roadmap for Beginners",
    "section": "Open Universities",
    "text": "Open Universities\nAn open university is a university with an open-door academic policy, with minimal or no entry requirements. Open universities may employ specific teaching methods, such as open supported learning or distance education.\n\nCap College E-Learning\nPUP Open University\nUP Open University"
  },
  {
    "objectID": "study-roadmap.html#expanded-tertiary-education-equivalency-and-accreditationeteeap",
    "href": "study-roadmap.html#expanded-tertiary-education-equivalency-and-accreditationeteeap",
    "title": "Study Roadmap for Beginners",
    "section": "Expanded Tertiary Education Equivalency and Accreditation(ETEEAP)",
    "text": "Expanded Tertiary Education Equivalency and Accreditation(ETEEAP)\nThe ETEEAP is a comprehensive educational assessment program at the tertiary level that recognizes, accredits and gives equivalencies to knowledge, skills, attitudes and values gained by individuals from relevant work.\n\nWhat is ETEEAP?\nETEEAP, panukalang gawing mas accessible - VIDEO\nETEEAP, Program Details - VIDEO"
  }
]