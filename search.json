[
  {
    "objectID": "technical-knowledgebase.html",
    "href": "technical-knowledgebase.html",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "This section provides a comprehensive overview of various technologies and tools in the field of data engineering.\n\n\n\nPostgreSQL: An open-source object-relational database system known for its reliability and robust features.\nMySQL: The most popular open-source SQL database management system, developed by Oracle Corporation.\nAmazon Relational Database System (RDS): A managed service for setting up and scaling databases in the cloud, supporting multiple database engines.\n\n\n\n\n\nAmazon Redshift: A cloud data warehouse known for its fast performance, available in both provisioned and serverless configurations.\nGoogle BigQuery: A serverless enterprise data warehouse that offers high scalability and cost-effectiveness.\n\n\n\n\n\nRedis: An open-source, in-memory key-value store known for its versatility and performance.\nAmazon DynamoDB: A fully managed, serverless NoSQL database designed for modern applications.\n\n\n\n\n\nAmazon S3: Offers scalable and secure object storage services.\nAzure Blob Storage: Ideal for storing large-scale cloud-native workloads, archives, and data lakes.\nGoogle Cloud Storage: A flexible service for storing and retrieving any amount of data.\n\n\n\n\n\nApache Kafka: A distributed event streaming platform with various implementations:\n\nConfluent’s Apache Kafka: A fully managed service with expert support.\nAmazon MSK: Amazon’s fully managed Kafka service.\n\nAWS SDK for pandas (AWS Wrangler): Extends pandas library to AWS, allowing seamless integration with AWS data services.\nAWS Kinesis: A cloud-based service for real-time data processing.\nAirbyte: An open-source data integration platform for ELT pipelines.\nPentaho Data Integration (Kettle): Includes both a core data integration engine and a graphical user interface for defining jobs and transformations.\n\n\n\n\n\nApache Avro: A serialization format ideal for streaming data pipelines.\nApache Parquet: An open-source columnar data file format.\nApache ORC: Optimized for Hadoop workloads.\n\n\n\n\n\nDelta Lake: An open-source storage framework by Databricks for building Lakehouse architecture.\nApache Iceberg: An open table format for huge analytical datasets, developed by Netflix.\nApache Hudi: A framework for managing large analytical datasets, created by Uber.\n\n\n\n\n\n\n\nApache Spark: A versatile engine for big data processing, available in various languages like Python (PySpark), Scala, Java, and R.\n\n\n\n\n\nPresto: A distributed SQL query engine for big data.\nApache Hive: Built on Hadoop, it facilitates reading, writing, and managing large datasets.\nApache Drill: An open-source SQL query engine.\nTrino: A SQL query engine designed for large data sets.\n\n\n\n\n\nAWS Elastic MapReduce (EMR): A cloud big data platform for processing large datasets.\nAWS Glue: A serverless data integration service.\n\n\n\n\n\n\nSpark Streaming: A part of Apache Spark for processing live data streams.\nSpark Structured Streaming: A stream processing engine built on Spark SQL.\nApache Flink: A framework for stateful computations over data streams.\nApache Storm: A system for processing streaming data in real-time.\n\n\n\n\nApache Druid: A high-performance real-time analytics database.\nApache Pinot: A real-time distributed OLAP datastore.\n\n\n\n\n\n\nApache Airflow: An open-source platform for managing complex computational workflows and data processing pipelines.\nMage: A modern replacement for Airflow for transforming and integrating data.\nDagster: An orchestration platform for data assets.\nPrefect: A workflow orchestration tool for data pipelines.\nKestra: An orchestrator for both scheduled and event-driven workflows.\nAWS Step Functions: Coordinates components of distributed applications.\n\n\n\n\n\n\n\nData Build Tool (dbt): A transformation workflow that follows software engineering best practices.\nSQLMesh: An open-source data transformation framework for SQL and Python.\n\n\n\n\n\n\n\n\nDataHub Project: An extensible metadata platform by LinkedIn.\nOpenMetadata: A platform for discovering, collaborating, and managing data.\nApache Atlas: An open-source metadata and governance framework.\nAmundsen: A data discovery and metadata engine by Lyft.\n\n\n\n\n\nGreat Expectations: A platform for data quality and observability.\n\n\n\n\n\n\nDatabricks: Offers a unified Data Lakehouse platform.\nSnowflake: A cloud-native data warehouse platform."
  },
  {
    "objectID": "technical-knowledgebase.html#relational-databases",
    "href": "technical-knowledgebase.html#relational-databases",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "PostgreSQL: An open-source object-relational database system known for its reliability and robust features.\nMySQL: The most popular open-source SQL database management system, developed by Oracle Corporation.\nAmazon Relational Database System (RDS): A managed service for setting up and scaling databases in the cloud, supporting multiple database engines."
  },
  {
    "objectID": "technical-knowledgebase.html#columnar-databases",
    "href": "technical-knowledgebase.html#columnar-databases",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Amazon Redshift: A cloud data warehouse known for its fast performance, available in both provisioned and serverless configurations.\nGoogle BigQuery: A serverless enterprise data warehouse that offers high scalability and cost-effectiveness."
  },
  {
    "objectID": "technical-knowledgebase.html#key-value-stores",
    "href": "technical-knowledgebase.html#key-value-stores",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Redis: An open-source, in-memory key-value store known for its versatility and performance.\nAmazon DynamoDB: A fully managed, serverless NoSQL database designed for modern applications."
  },
  {
    "objectID": "technical-knowledgebase.html#object-storage",
    "href": "technical-knowledgebase.html#object-storage",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Amazon S3: Offers scalable and secure object storage services.\nAzure Blob Storage: Ideal for storing large-scale cloud-native workloads, archives, and data lakes.\nGoogle Cloud Storage: A flexible service for storing and retrieving any amount of data."
  },
  {
    "objectID": "technical-knowledgebase.html#data-ingestion",
    "href": "technical-knowledgebase.html#data-ingestion",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Kafka: A distributed event streaming platform with various implementations:\n\nConfluent’s Apache Kafka: A fully managed service with expert support.\nAmazon MSK: Amazon’s fully managed Kafka service.\n\nAWS SDK for pandas (AWS Wrangler): Extends pandas library to AWS, allowing seamless integration with AWS data services.\nAWS Kinesis: A cloud-based service for real-time data processing.\nAirbyte: An open-source data integration platform for ELT pipelines.\nPentaho Data Integration (Kettle): Includes both a core data integration engine and a graphical user interface for defining jobs and transformations."
  },
  {
    "objectID": "technical-knowledgebase.html#data-formats",
    "href": "technical-knowledgebase.html#data-formats",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Avro: A serialization format ideal for streaming data pipelines.\nApache Parquet: An open-source columnar data file format.\nApache ORC: Optimized for Hadoop workloads."
  },
  {
    "objectID": "technical-knowledgebase.html#data-storage-framework",
    "href": "technical-knowledgebase.html#data-storage-framework",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Delta Lake: An open-source storage framework by Databricks for building Lakehouse architecture.\nApache Iceberg: An open table format for huge analytical datasets, developed by Netflix.\nApache Hudi: A framework for managing large analytical datasets, created by Uber."
  },
  {
    "objectID": "technical-knowledgebase.html#batch-processing",
    "href": "technical-knowledgebase.html#batch-processing",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Spark: A versatile engine for big data processing, available in various languages like Python (PySpark), Scala, Java, and R.\n\n\n\n\n\nPresto: A distributed SQL query engine for big data.\nApache Hive: Built on Hadoop, it facilitates reading, writing, and managing large datasets.\nApache Drill: An open-source SQL query engine.\nTrino: A SQL query engine designed for large data sets.\n\n\n\n\n\nAWS Elastic MapReduce (EMR): A cloud big data platform for processing large datasets.\nAWS Glue: A serverless data integration service."
  },
  {
    "objectID": "technical-knowledgebase.html#stream-processing",
    "href": "technical-knowledgebase.html#stream-processing",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Spark Streaming: A part of Apache Spark for processing live data streams.\nSpark Structured Streaming: A stream processing engine built on Spark SQL.\nApache Flink: A framework for stateful computations over data streams.\nApache Storm: A system for processing streaming data in real-time.\n\n\n\n\nApache Druid: A high-performance real-time analytics database.\nApache Pinot: A real-time distributed OLAP datastore."
  },
  {
    "objectID": "technical-knowledgebase.html#workflow-orchestration",
    "href": "technical-knowledgebase.html#workflow-orchestration",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Airflow: An open-source platform for managing complex computational workflows and data processing pipelines.\nMage: A modern replacement for Airflow for transforming and integrating data.\nDagster: An orchestration platform for data assets.\nPrefect: A workflow orchestration tool for data pipelines.\nKestra: An orchestrator for both scheduled and event-driven workflows.\nAWS Step Functions: Coordinates components of distributed applications."
  },
  {
    "objectID": "technical-knowledgebase.html#data-transformation",
    "href": "technical-knowledgebase.html#data-transformation",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Data Build Tool (dbt): A transformation workflow that follows software engineering best practices.\nSQLMesh: An open-source data transformation framework for SQL and Python."
  },
  {
    "objectID": "technical-knowledgebase.html#data-governance",
    "href": "technical-knowledgebase.html#data-governance",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "DataHub Project: An extensible metadata platform by LinkedIn.\nOpenMetadata: A platform for discovering, collaborating, and managing data.\nApache Atlas: An open-source metadata and governance framework.\nAmundsen: A data discovery and metadata engine by Lyft.\n\n\n\n\n\nGreat Expectations: A platform for data quality and observability."
  },
  {
    "objectID": "technical-knowledgebase.html#data-platforms",
    "href": "technical-knowledgebase.html#data-platforms",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Databricks: Offers a unified Data Lakehouse platform.\nSnowflake: A cloud-native data warehouse platform."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Free Resources",
    "section": "",
    "text": "IBM Data Engineering Basics for Everyone on edX\nGoogle Cloud Data Engineering Path\nMeta Database Engineer Professional Certificate on Coursera\nBig Data Specialization by UC San Diego on Coursera\nData Engineering Zoomcamp on GitHub"
  },
  {
    "objectID": "resources.html#data-engineering-training",
    "href": "resources.html#data-engineering-training",
    "title": "Free Resources",
    "section": "",
    "text": "IBM Data Engineering Basics for Everyone on edX\nGoogle Cloud Data Engineering Path\nMeta Database Engineer Professional Certificate on Coursera\nBig Data Specialization by UC San Diego on Coursera\nData Engineering Zoomcamp on GitHub"
  },
  {
    "objectID": "resources.html#sql-tutorials-and-courses",
    "href": "resources.html#sql-tutorials-and-courses",
    "title": "Free Resources",
    "section": "SQL Tutorials and Courses",
    "text": "SQL Tutorials and Courses\n\nW3 Schools SQL Tutorial - A comprehensive tutorial for SQL beginners.\nSQLBolt - Practice with real-world datasets and challenging SQL problems to deepen your skills."
  },
  {
    "objectID": "resources.html#interactive-sql-learning-games",
    "href": "resources.html#interactive-sql-learning-games",
    "title": "Free Resources",
    "section": "Interactive SQL Learning Games",
    "text": "Interactive SQL Learning Games\n\nSQL Murder Mystery - Solve a captivating murder case using your SQL skills in this immersive game-like environment. Suitable for beginners.\nSQL Island - Navigate through an adventure on SQL Island to learn SQL commands. Remember to change the language to English via the hamburger icon.\nSchemaVerse - A space-based strategy game where you use SQL commands to control your fleet and conquer the universe.\nLost at SQL: The SQL Learning Game - Enhance your SQL skills through this engaging learning game."
  },
  {
    "objectID": "resources.html#sql-practice-and-challenges",
    "href": "resources.html#sql-practice-and-challenges",
    "title": "Free Resources",
    "section": "SQL Practice and Challenges",
    "text": "SQL Practice and Challenges\n\n8 Week SQL Challenge - Intermediate/Advanced SQL challenges through interactive projects.\nHackerRank SQL Challenges - Test your mettle against others in coding challenges that push your SQL boundaries. Suitable for Intermediate-Advanced learners.\nCodewars SQL Kata - Hone your understanding with kata-style SQL exercises, practicing diverse concepts.\nSQLZoo - Practice with a wide range of exercises at different difficulty levels, mastering SQL fundamentals."
  },
  {
    "objectID": "resources.html#learn-python",
    "href": "resources.html#learn-python",
    "title": "Free Resources",
    "section": "Learn Python",
    "text": "Learn Python\nPython is a versatile and powerful programming language that’s great for beginners and professionals alike. Here are some resources to get started or enhance your Python skills:\n\nPython Official Getting Started Guide - Official Python documentation and guide on getting started with Python.\nLearnpython.org - A free interactive Python tutorial for people who want to learn Python, starting from the basics.\nOpenClassrooms - Learn Programming with Python - A course designed to introduce you to programming using Python.\nPrinciples of Computation with Python - Carnegie Mellon University - An open & free course by CMU focusing on computational principles using Python.\nCodecademy Python Course - Interactive Python programming courses for all levels.\nMIT OpenCourseWare - Introduction to Computer Science and Programming in Python - A course by MIT that introduces the fundamental ideas of computing using Python.\nHarvard University - Python Programming - Various Python programming courses offered by Harvard University.\nfreeCodeCamp - Learn Python - A collection of free Python courses for beginners to help learn Python programming.\nGoogle Cloud - Python on Google Cloud - Learn how to use Python with Google Cloud services for building and deploying applications."
  },
  {
    "objectID": "resources.html#learn-r",
    "href": "resources.html#learn-r",
    "title": "Free Resources",
    "section": "Learn R",
    "text": "Learn R\nR is a programming language and environment commonly used for statistical computing and graphics. The following resources provide comprehensive guides, tutorials, and courses for beginners to advanced users interested in learning R:\n\nCodecademy - Learn R - An interactive platform offering a course designed to get you started with R programming.\nW3Schools R Tutorial - Provides a quick and easy understanding of R, covering basics to advanced topics.\nRStudio Education - Learn R - Beginner resources compiled by RStudio, aiming to make learning R easier and more effective.\nProgramiz - Learn R Programming - Offers R tutorials for beginners to learn R programming online.\nCodecademy - R Catalog - Discover more R courses offered by Codecademy to deepen your understanding and skills.\nSwirl - Learn R, in R - A platform that offers interactive R programming lessons directly within the R console.\nHands-On Programming with R - Teaches how to perform data analysis with R through practical examples, covering basics to more advanced topics."
  },
  {
    "objectID": "resources.html#statistics-lessons-on-youtube",
    "href": "resources.html#statistics-lessons-on-youtube",
    "title": "Free Resources",
    "section": "Statistics Lessons on YouTube",
    "text": "Statistics Lessons on YouTube\n\nCrashCourse Statistics: Quick statistics overview.\nStatQuest - Statistics Fundamentals: Fundamental statistics concepts."
  },
  {
    "objectID": "resources.html#data-science-tutorials",
    "href": "resources.html#data-science-tutorials",
    "title": "Free Resources",
    "section": "Data Science Tutorials",
    "text": "Data Science Tutorials\n\nData Science for Beginners by Microsoft: Beginner-friendly data science materials on GitHub.\nHarvard CS50’s Introduction to Programming with Python: Full university course available on YouTube.\nR for Data Science - This online book provides an introduction to R specifically for data science, covering data visualization, transformation, and modeling."
  },
  {
    "objectID": "resources.html#web-scraping-with-python",
    "href": "resources.html#web-scraping-with-python",
    "title": "Free Resources",
    "section": "Web Scraping with Python",
    "text": "Web Scraping with Python\nWeb scraping is a method used to extract data from websites. Python offers several libraries and tools for web scraping. Here are some essential resources to get started or enhance your web scraping skills:\n\nBeautiful Soup Documentation - Official documentation for Beautiful Soup, a Python library designed for quick turnaround projects like screen-scraping.\nPython Requests Library - Official documentation for Requests, a simple HTTP library for Python, used to send HTTP requests easily.\nAutomate the Boring Stuff with Python - A book that teaches Python programming with practical examples, including a chapter on web scraping.\nScrape Quotes - A practice website designed for scraping quotes from famous authors.\nScrape This Site - A website that offers lessons and challenges for web scraping practices.\nfreeCodeCamp Web Scraping Python Tutorial - A comprehensive guide on how to scrape data from a website using Python.\nGeeksforGeeks Python Web Scraping Tutorial - Offers a tutorial on web scraping using Python, covering basics to advanced topics.\nBeautiful Soup Web Scraper Python - Real Python - A tutorial that explains how to use Beautiful Soup for web scraping effectively.\nPython Web Scraping: A Practical Introduction - Real Python - Provides a practical introduction to web scraping using Python, including setting up your environment and parsing HTML."
  },
  {
    "objectID": "resources.html#portfolio-building-resources",
    "href": "resources.html#portfolio-building-resources",
    "title": "Free Resources",
    "section": "Portfolio Building Resources",
    "text": "Portfolio Building Resources\nBuilding a professional portfolio is crucial for showcasing your skills and projects to potential employers or clients. Here are some valuable resources to help you create and enhance your portfolio:\n\nCreate a Portfolio Website with AlexTheAnalyst - A comprehensive tutorial on creating and hosting a portfolio using GitHub Pages. Perfect for beginners looking to establish an online presence.\nGoogle Analytics Tutorials:\n\nGoogle Analytics for Beginners - Learn how to track your portfolio or website visitors, understand your audience, and improve your site based on analytics data.\nAdvanced Google Analytics - Dive deeper into Google Analytics to leverage more complex tracking and analysis techniques for your site.\n\nYouTube Playlist on Data Engineering Projects - A curated list of video tutorials and project ideas for aspiring data engineers, covering various tools and technologies.\nData Engineer Portfolio Project Ideas - Provides a range of project ideas and guidance on building a portfolio that stands out for data engineering roles.\nData Engineering Projects for Beginners - Simplilearn - Offers tutorials and project ideas for beginners in data engineering, helping you to start building your portfolio with practical experience."
  },
  {
    "objectID": "resources.html#foundational-data-science-courses",
    "href": "resources.html#foundational-data-science-courses",
    "title": "Free Resources",
    "section": "Foundational Data Science Courses",
    "text": "Foundational Data Science Courses\n\nThe Open Source Data Science Masters - A comprehensive curriculum for self-study in data science.\nData Science Essentials- Microsoft - Foundation course on edX covering essential data science concepts.\nHarvard CS109 Data Science - In-depth course covering data science methodologies and Python.\nData Science Fundamentals – IBM - Covers fundamentals of data science through hands-on practice on the Cognitive Class platform."
  },
  {
    "objectID": "resources.html#introduction-to-data-science",
    "href": "resources.html#introduction-to-data-science",
    "title": "Free Resources",
    "section": "Introduction to Data Science",
    "text": "Introduction to Data Science\n\nIntroduction to Data Science by Jeff Hammerbacher at UC, Berkeley - Lectures and materials on the introduction to data science.\nIntroduction to Data Science @coursera - Offers a broad introduction to data science through Coursera.\nIntroduction to Data Science @UofWashington - Another Coursera course providing foundational knowledge in data science."
  },
  {
    "objectID": "resources.html#specialized-topics-in-data-science",
    "href": "resources.html#specialized-topics-in-data-science",
    "title": "Free Resources",
    "section": "Specialized Topics in Data Science",
    "text": "Specialized Topics in Data Science\n\nLearning from Data – California Institute of Technology & UCBerkeley - Focus on machine learning and data analysis.\nProcess Mining: Data Science in Action @TUEindhoven - Specialized course on process mining techniques.\nPattern Discovery in Data Mining @UoIllinois - Course on discovering patterns in datasets.\nIntroduction to Data Mining @MIT - Lecture notes and materials on data mining concepts.\nMining Massive Datasets @Stanford - Advanced course on dealing with massive datasets."
  },
  {
    "objectID": "resources.html#statistical-analysis-and-data-wrangling",
    "href": "resources.html#statistical-analysis-and-data-wrangling",
    "title": "Free Resources",
    "section": "Statistical Analysis and Data Wrangling",
    "text": "Statistical Analysis and Data Wrangling\n\nStatistical Thinking and Data Analysis @MIT & DukeUni - Courses focused on statistical thinking and data analysis techniques.\nOpen intro to Statistics - Provides a solid introduction to statistics.\nIntroduction to Data Wrangling at the School of Data - Focuses on data cleaning and preparation techniques."
  },
  {
    "objectID": "resources.html#programming-for-data-science",
    "href": "resources.html#programming-for-data-science",
    "title": "Free Resources",
    "section": "Programming for Data Science",
    "text": "Programming for Data Science\n\nPython for Data Science - Course by Cognitive Class on using Python for data science tasks.\nData Science: R Basics - Harvard University - Introduction to using R for data science."
  },
  {
    "objectID": "resources.html#online-learning-platforms-and-resources",
    "href": "resources.html#online-learning-platforms-and-resources",
    "title": "Free Resources",
    "section": "Online Learning Platforms and Resources",
    "text": "Online Learning Platforms and Resources\n\nCognitive Class - Interactive platform offering courses on data science and AI.\nJovian - For sharing and collaborating on Jupyter notebooks.\nDataPen - Interactive tutorials on data science and machine learning.\nData Analytics Educational Resources - Collection of learning materials for data analytics.\nExecutive Levels Data Software Training - Offers training on various data software tools.\nSQLZOO - Interactive SQL tutorials for hands-on learning."
  },
  {
    "objectID": "resources.html#additional-learning-resources",
    "href": "resources.html#additional-learning-resources",
    "title": "Free Resources",
    "section": "Additional Learning Resources",
    "text": "Additional Learning Resources\n\nGitHub - Platform for code sharing and collaboration.\nPLURALSIGHT - Offers video courses on a wide range of tech topics, including data science.\nhackerRank - Platform for practicing coding and data science skills.\nWorld Quant - Provides resources for quantitative analysis.\nOpen Books - Free access to educational textbooks, including topics on data science and statistics."
  },
  {
    "objectID": "resources.html#training-and-internships",
    "href": "resources.html#training-and-internships",
    "title": "Free Resources",
    "section": "Training and Internships",
    "text": "Training and Internships\n\nThe Sparks Foundation: A remote one-month internship in Data Science and Business Analytics.\nFor The Women Foundation: FTW is a nonprofit organization providing free data science and technology training for women"
  },
  {
    "objectID": "resources.html#online-learning-platforms",
    "href": "resources.html#online-learning-platforms",
    "title": "Free Resources",
    "section": "Online Learning Platforms",
    "text": "Online Learning Platforms\n\nDataquest\nDatacamp\nCodecademy\nSoloLearn\nW3Schools\nKhan Academy\nCoursera\nedX\nfreeCodeCamp\nUdacity\nThe Odin Project\nMozilla Developer Network (MDN)\nKaggle"
  },
  {
    "objectID": "resources.html#cloud-resources",
    "href": "resources.html#cloud-resources",
    "title": "Free Resources",
    "section": "Cloud Resources",
    "text": "Cloud Resources\n\nAWS Training and Certification: https://aws.amazon.com/training/\nGoogle Cloud Training: https://cloud.google.com/training\nMicrosoft Learn: https://docs.microsoft.com/en-us/learn/\nCloud Free Tier Comparison Cloud Free Tier Comparison: Articles comparing free tier offers from AWS, Azure, GCP, and Oracle Cloud."
  },
  {
    "objectID": "resources.html#open-courseware",
    "href": "resources.html#open-courseware",
    "title": "Free Resources",
    "section": "Open Courseware",
    "text": "Open Courseware\n\nData Analysis with R\nData Engineering Zoomcamp\nThe Open Source Data Science Masters\nFast.ai Courses\nGIS Programming Roadmap on GitHub"
  },
  {
    "objectID": "resources.html#formal-education",
    "href": "resources.html#formal-education",
    "title": "Free Resources",
    "section": "Formal Education",
    "text": "Formal Education\n\nMaster’s Programs in Data Science and Related Fields in the Philippines\n\nUniversity of the Philippines (UP)\n\nMaster’s in Data Science and Engineering:\n\nUPD Department of Statistics\nUPD College of Science\nUPD College of Engineering\n\n\n\n\nTechnological Institute of the Philippines (TIP)\n\nProfessional Science Master’s Degree in Data Science:\n\nTIP Graduate Programs\n\n\n\n\nDe La Salle University (DLSU)\n\nMaster’s in Data Science:\n\nDLSU Data Science Institute\n\n\n\n\nBatangas State University\n\nMaster of Science in Artificial Intelligence:\n\nMSAI Program\n\n\n\n\nMapua University\n\nMaster in Business Analytics:\n\nMapua Institute of Technology\n\n\n\n\nAteneo de Manila University (ADMU)\n\nMaster of Science in Data Science:\n\nADMU Global\n\n\n\n\nAsian Institute of Management (AIM)\n\nMaster of Science in Data Science:\n\nAIM MSc in Data Science\n\n\n\n\nUniversity of Asia and the Pacific (UA&P)\n\nMaster in Applied Business Analytics:\n\nUA&P Graduate Programs\n\n\nThis list provides a variety of options for advanced education in data science and related fields across the Philippines. Each program offers a unique focus, from artificial intelligence to business analytics, catering to diverse interests and career paths in the burgeoning field of data science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "Data Engineering Pilipinas is a community for data engineers, data analysts, data scientists, developers, AI / ML engineers, and users of closed and open source data tools and methods / techniques in the Philippines. Data Engineering Pilipinas is a PyData group.\nThis page serves as a repository of notes, thoughts, ideas, plans, dreams, datasets, analyses, and whatever else we think of."
  },
  {
    "objectID": "index.html#getting-started-with-data-engineering",
    "href": "index.html#getting-started-with-data-engineering",
    "title": "Data Engineering Pilipinas",
    "section": "Getting Started with Data Engineering",
    "text": "Getting Started with Data Engineering\n\nRead more about Data Engineering 101"
  },
  {
    "objectID": "index.html#join-our-growing-community",
    "href": "index.html#join-our-growing-community",
    "title": "Data Engineering Pilipinas",
    "section": "Join Our Growing Community",
    "text": "Join Our Growing Community\nConnect with Data Engineering Pilipinas on various platforms. Like, follow, and join our groups and pages to stay updated and engage with our community:"
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html",
    "href": "content/table_index/rmd/indexed_table.html",
    "title": "Indexed table",
    "section": "",
    "text": "In the previous chapter, we saw that an SQL DB parses through the whole table to retrieve rows. Because it does not know where the rows that match the provided conditions are, it has to check every row. This is why it does not matter where in the table the rows are located. This is where INDEXes come in."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#duplicate-our-table",
    "href": "content/table_index/rmd/indexed_table.html#duplicate-our-table",
    "title": "Indexed table",
    "section": "Duplicate our table",
    "text": "Duplicate our table\nWe want to keep our non-indexed table so that we can still run non-indexed queries later.\nIt takes about 15 minutes to make a copy.\nThen we add the primary key and indexes for fk_id and entry_date.\nIt takes about 20 minutes to add these indices."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#database-connection",
    "href": "content/table_index/rmd/indexed_table.html#database-connection",
    "title": "Indexed table",
    "section": "Database Connection",
    "text": "Database Connection\nIn the background, we set up our environment, connect to the database, and turn on profiling.\nA quick check of our tables:\nOur tables are the same except for the indexes."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#show-profile-function",
    "href": "content/table_index/rmd/indexed_table.html#show-profile-function",
    "title": "Indexed table",
    "section": "Show Profile function",
    "text": "Show Profile function\nBecause we will run profiling repeatedly, it makes sense to write it into a function."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-6-run-the-first-day-query-with-the-benefit-of-an-index",
    "href": "content/table_index/rmd/indexed_table.html#query-6-run-the-first-day-query-with-the-benefit-of-an-index",
    "title": "Indexed table",
    "section": "Query 6: Run the “first day” query with the benefit of an index",
    "text": "Query 6: Run the “first day” query with the benefit of an index\nFrom 17 seconds, we are now down to below 0.007 seconds."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-7-run-the-last-day-query-with-the-benefit-of-an-index",
    "href": "content/table_index/rmd/indexed_table.html#query-7-run-the-last-day-query-with-the-benefit-of-an-index",
    "title": "Indexed table",
    "section": "Query 7: Run the “last day” query with the benefit of an index",
    "text": "Query 7: Run the “last day” query with the benefit of an index\nIt is the same for our “last day” query. Below 0.007 seconds. The DB is not parsing the entire table anymore.\nBut, this is a little bit of a cheat. Remember that entry_date is already sorted. Rows with the same entry_dates are together. How much difference is there if the needles are scattered all over the table?"
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-8-retrieve-all-fk_id-45",
    "href": "content/table_index/rmd/indexed_table.html#query-8-retrieve-all-fk_id-45",
    "title": "Indexed table",
    "section": "Query 8: Retrieve all fk_id = 45",
    "text": "Query 8: Retrieve all fk_id = 45\nWe have a different column we can filter on. fk_id is not sorted. It is randomly distributed across the entire table. Let’s run a baseline on the non-indexed table.\nAs expected, we get about 17 seconds."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-9-retrieve-all-fk_id-45-with-the-help-of-an-index",
    "href": "content/table_index/rmd/indexed_table.html#query-9-retrieve-all-fk_id-45-with-the-help-of-an-index",
    "title": "Indexed table",
    "section": "Query 9: Retrieve all fk_id = 45 with the help of an index",
    "text": "Query 9: Retrieve all fk_id = 45 with the help of an index\nThen the same query from the indexed table.\nWe have a substantial improvement from 17 seconds to less than 5 seconds."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#questions",
    "href": "content/table_index/rmd/indexed_table.html#questions",
    "title": "Indexed table",
    "section": "Questions",
    "text": "Questions\n\nWhat if we need to filter by year? Or by year-month?"
  },
  {
    "objectID": "content/table_index/rmd/create_table.html",
    "href": "content/table_index/rmd/create_table.html",
    "title": "Create and populate table",
    "section": "",
    "text": "For our sample table, we will use a simple 4-column table that is a simulated result of an ETL process from some OLTP DB. We have the following values: - date value: You can think of this as an entry date, or a purchase date - numeric value: You can think of this as quantity of items, or a monetary value - descriptive value: You can think of this as a category code, or a branch code\nWe will then populate our table with 50 million rows randomly generated. These will be inserted in date order ascending. Why 50 million? So that we give the DB a little bit of a workout. A DB will not break a sweat with hundred-thousand-row tables."
  },
  {
    "objectID": "content/table_index/rmd/create_table.html#create-our-sample-table",
    "href": "content/table_index/rmd/create_table.html#create-our-sample-table",
    "title": "Create and populate table",
    "section": "Create our sample table",
    "text": "Create our sample table\nFirst, we create our table in our database with the following DDL:\nTake note that at this point, other than the primary key, we do not have any indexes defined.\n\n\n\nField\nData Type\nDescription\n\n\n\n\nid\nINT\nA simple unsigned primary key\n\n\nfk_id\nINT\nA simulated foreign key ID\n\n\narbitrary_value\nINT\nA value we can use aggregate functions on\n\n\nentry_date\nDATETIME\nA datetime value"
  },
  {
    "objectID": "content/table_index/rmd/create_table.html#populate-our-table",
    "href": "content/table_index/rmd/create_table.html#populate-our-table",
    "title": "Create and populate table",
    "section": "Populate our table",
    "text": "Populate our table\n\nGenerate data\nWe will now generate data for our table. To do this we use fixtuRes. We provide a YML configuration file:\n# sample_table.yml\nsample_table:\n  columns:\n    fk_id:\n      type: integer\n      min: 1\n      max: 100\n    arbitrary_value:\n      type: integer\n      min: 0\n      max: 50\n    entry_date:\n      type: date\n      min: 1973-01-01\n      max: 2023-12-31\n  arrange: entry_date\nWe use MockDataGenerator to create our data. This will produce a table ordered by entry_date. Then we add a sequential id column to have the id in the same order as entry_date.\nThe following is a sample output if we used size = 20:\n\n\nConnect to DB\nWe now establish a connection to our MySQL database server. The RMySQL library has been deprecated in favor of the RMariaDB library.\nIt is always good practice to keep your connection credentials like usernames, passwords, API tokens in your environment variables. Never hard-coded in source code. And never commit your environment variable file to the repo.\n\n\nWrite our data to the DB\nWhile there is a function we can use to write our mock_data to the DB table (dbWriteTable(conn, \"sample_table\", mock_data)), remember that we have 50 million rows. When mock_data is written to a CSV file, it results in a 1.3 GB file. Writing this to the DB will take some time. To avoid hitting the connection time-out constraint we will write the data by batches of date.\nLet’s take a look at what we have.\nDont’ leave any DB connections open!\nNow, we have a 50 million row DB table we can play around with."
  },
  {
    "objectID": "content/projects/pipeline_basic/readme.html",
    "href": "content/projects/pipeline_basic/readme.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "This is a basic ETL project"
  },
  {
    "objectID": "content/projects/pipeline_basic/readme.html#basic-etl-project",
    "href": "content/projects/pipeline_basic/readme.html#basic-etl-project",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "This is a basic ETL project"
  },
  {
    "objectID": "content/data-engineering-101.html",
    "href": "content/data-engineering-101.html",
    "title": "Data Engineering 101",
    "section": "",
    "text": "Data Engineering Domain"
  },
  {
    "objectID": "content/data-engineering-101.html#data-engineering",
    "href": "content/data-engineering-101.html#data-engineering",
    "title": "Data Engineering 101",
    "section": "Data Engineering",
    "text": "Data Engineering\n\nPrimary Focus: Data engineering focuses on the practical aspects of data collection, data transformation, and data storage, preparing data for analytical or operational use.\nKey Responsibilities:\n\nBuilding and maintaining data architecture (databases, large-scale processing systems).\nDeveloping and managing data pipelines.\nEnsuring data availability and usability for data scientists and analysts.\n\nSkills and Tools:\n\nProgramming languages (Python, Java, Scala).\nDatabase languages (SQL).\nTools and frameworks (Apache Hadoop, Apache Spark, ETL tools, data warehousing solutions)."
  },
  {
    "objectID": "content/data-engineering-101.html#related-fields",
    "href": "content/data-engineering-101.html#related-fields",
    "title": "Data Engineering 101",
    "section": "Related Fields",
    "text": "Related Fields\n\n1. Data Analysis\n\nInvolves extracting insights from data.\nTools: Excel, SQL, R, Python, BI tools (like Tableau, Power BI).\n\n\n\n2. Data Science\n\nEncompasses data analysis, predictive modeling, and machine learning.\nTools: Python, R, TensorFlow, machine learning libraries.\n\n\n\n3. Machine Learning Engineering\n\nFocuses on building systems that learn from data.\nTools: Python, machine learning frameworks, cloud computing platforms.\n\n\n\n4. Business Intelligence (BI)\n\nAnalyzing data to aid business decision-making.\nTools: SQL, BI platforms (Tableau, Power BI, Looker).\n\n\n\n5. Database Administration\n\nManaging and maintaining databases.\nTools: SQL, database management systems (MySQL, PostgreSQL).\n\n\n\n6. Big Data\n\nHandling large and complex data sets.\nTools: Hadoop, Spark, NoSQL databases.\n\nEach field plays a unique role in the data ecosystem, often collaborating to turn data into actionable insights. As the name suggests, our community focuses on all data career paths with emphasis on data engineering."
  },
  {
    "objectID": "content/Cloud-Free-Tier-Comparison/Articles.html",
    "href": "content/Cloud-Free-Tier-Comparison/Articles.html",
    "title": "Articles",
    "section": "",
    "text": "Articles\nhttps://www.cloudmanagementinsider.com/aws-azure-google-cloud-free-tier-comparison/\nhttps://n2ws.com/blog/amazon-aws-microsoft-azure-google-cloud-free-tier-cloud-computing-service-comparison\nhttps://www.infoworld.com/article/3179785/aws-vs-azure-vs-google-cloud-which-free-tier-is-best.html"
  },
  {
    "objectID": "about.html#our-mission",
    "href": "about.html#our-mission",
    "title": "About Data Engineering Pilipinas",
    "section": "Our Mission",
    "text": "Our Mission\nData Engineering Pilipinas is dedicated to democratizing data engineering education by offering open access to learning materials and promoting a collaborative environment for the exchange of tools and skills.\nThe community serves as a forum for both users and developers of data tools, facilitating the sharing of ideas and experiences. Data Engineering Pilipinas encourages discussions on best practices, innovative approaches, and emerging technologies in data management, processing, analytics, and visualization. It embraces a wide range of programming languages including Python, R, and SQL, reflecting the diverse methods used within data engineering and data science.\nWith a commitment to being an accessible, community-driven platform, Data Engineering Pilipinas caters to all levels of expertise, from novice to advanced practitioners. The conferences, tutorials, and talks provided by Data Engineering Pilipinas offer attendees insights into the latest project features as well as cutting-edge use cases."
  },
  {
    "objectID": "about.html#code-of-conduct-based-from-pydata",
    "href": "about.html#code-of-conduct-based-from-pydata",
    "title": "About Data Engineering Pilipinas",
    "section": "Code of Conduct (Based from PyData)",
    "text": "Code of Conduct (Based from PyData)\nBe kind to others. Do not insult or put down others. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes and language are not appropriate for Data Engineering Pilipinas.\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate.\nData Engineering Pilipinas is dedicated to providing a harassment-free event experience for everyone, regardless of gender, sexual orientation, gender identity, and expression, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of participants in any form.\nThank you for helping make this a welcoming, friendly community for all."
  },
  {
    "objectID": "about.html#official-community-guidelines",
    "href": "about.html#official-community-guidelines",
    "title": "About Data Engineering Pilipinas",
    "section": "Official Community Guidelines",
    "text": "Official Community Guidelines\nOur community thrives on respect, collaboration, and shared learning. Here are the guidelines we follow to ensure a positive and productive environment for all members.\n\nBe Respectful Treat all members with courtesy and respect, avoiding offensive language or personal attacks.\nShare Knowledge Encourage the sharing of valuable insights, best practices, and resources related to data engineering.\nNo Trolling or Harassment Engage constructively. Trolling or harassment of other members or administrators is not tolerated.\nAvoid Plagiarism Always attribute credit to the original source when sharing content or ideas.\nBe Mindful of Privacy Do not share personal information or sensitive data in our discussions.\nMinimal Self-Promotion Keep self-promotion or spamming of personal content or products to a minimum, especially if not related to data engineering.\nStay On Topic Ensure discussions are relevant to data engineering, its technologies, and related fields.\nNo Illegal Activities Do not engage in or promote discussions about activities that violate laws or ethical standards.\nLimit Recruitment Posts Job postings and collaboration requests are welcome but keep them moderate and relevant to data engineering roles or opportunities. Use appropriate hashtags like #hiring, #project, and #opportunity.\nKeep it Professional Maintain a professional tone in all interactions and adhere to Facebook’s guidelines and policies at all times."
  },
  {
    "objectID": "about.html#diversity-inclusion-based-from-pydata",
    "href": "about.html#diversity-inclusion-based-from-pydata",
    "title": "About Data Engineering Pilipinas",
    "section": "Diversity & Inclusion (Based from PyData)",
    "text": "Diversity & Inclusion (Based from PyData)\nData Engineering Pilipinas welcomes and encourages participation in our community by people of all backgrounds and identities. We are committed to promoting and sustaining a culture that values mutual respect, tolerance, and learning, and we work together as a community to help each other live out these values."
  },
  {
    "objectID": "community.html",
    "href": "community.html",
    "title": "Community Content",
    "section": "",
    "text": "A brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\n An introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\n An online event with key thought leaders and content creators in the Filipino tech community.\n\nA panel discussion hosted by R User’s Group Philippines on transitioning to a career in data."
  },
  {
    "objectID": "community.html#videos-and-presentations",
    "href": "community.html#videos-and-presentations",
    "title": "Community Content",
    "section": "",
    "text": "A brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\n An introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\n An online event with key thought leaders and content creators in the Filipino tech community.\n\nA panel discussion hosted by R User’s Group Philippines on transitioning to a career in data."
  },
  {
    "objectID": "community.html#doc-ligot-interviews",
    "href": "community.html#doc-ligot-interviews",
    "title": "Community Content",
    "section": "Doc Ligot Interviews",
    "text": "Doc Ligot Interviews\n\nKuya Dev\n\n\n\nJosh V.\n\n\n\nSherwin Pelayo\n\n\n\nGerard\n\n\n\nXavier Puspus\n\n\n\nNoemi\n\n\n\nAemy Obinguar\n\n\n\nSandy Lauguico"
  },
  {
    "objectID": "community.html#recorded-events-and-talks",
    "href": "community.html#recorded-events-and-talks",
    "title": "Community Content",
    "section": "Recorded Events and Talks",
    "text": "Recorded Events and Talks\n Kyle Escosia’s talk on Big Data and Analytics, recorded live at an AWS Siklab Pilipinas event.\n A session by Kyle Escosia on creating a serverless data lake in AWS."
  },
  {
    "objectID": "community.html#blogs-and-articles",
    "href": "community.html#blogs-and-articles",
    "title": "Community Content",
    "section": "Blogs and Articles",
    "text": "Blogs and Articles\n\nSnowflake in the Philippines - Kyle Escosia’s insights on Snowflake’s rise in the Philippines.\nUPSERTS and DELETS using AWS Glue and Delta Lake - A guide on using AWS Glue with Delta Lake for data operations.\nTable Indexes - An educational piece on the importance and implementation of table indexes."
  },
  {
    "objectID": "community.html#projects",
    "href": "community.html#projects",
    "title": "Community Content",
    "section": "Projects",
    "text": "Projects\n\nBasic ETL project\nETL Sales\nETL Project with Azure Databricks - A walkthrough of an ETL project using Azure Databricks.\nContainerization - An exploration of containerization in data engineering."
  },
  {
    "objectID": "community.html#people-and-pages",
    "href": "community.html#people-and-pages",
    "title": "Community Content",
    "section": "People and Pages",
    "text": "People and Pages\n\nKyle Escosia - Explore the work of Kyle Escosia, a Data Engineer with a passion for all things data."
  },
  {
    "objectID": "content/Cloud-Free-Tier-Comparison/LICENSE.html",
    "href": "content/Cloud-Free-Tier-Comparison/LICENSE.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "MIT License\nCopyright (c) 2020 Cloud Study Network.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html",
    "href": "content/projects/ETL-SALES/readme.html",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "This project is an Extract, Transform, Load (ETL) package designed to merge data from various sources, including CSV files, Excel spreadsheets, and a PostgreSQL database, into a single destination database. The ETL process involves cleaning, transforming, and loading data to facilitate efficient analysis and reporting.\n\n\n\n\nExtracts data from CSV files, Excel spreadsheets, and a PostgreSQL database.\nApplies customizable transformations to clean and format the data.\nLoads the transformed data into a single destination database.\n\n\n\n\nRun the ETL pipeline: python PIPELINE_EXEC.py\n\n\n\n420171277_408831038167542_8129673976330505678_n\n\n\ndata set from link : DATA_SOURCE\nI separate csv files into different file sources to demonstrate extracting from different file sources.\nplease see below:\n\n\njanuary to april sales as excel (Sales_January_April_2019)\n\n\n\nmay to august sales as csv(sales_csv folder)\n\n\n\nseptember to december as database (sales_db)"
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html#overview",
    "href": "content/projects/ETL-SALES/readme.html#overview",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "This project is an Extract, Transform, Load (ETL) package designed to merge data from various sources, including CSV files, Excel spreadsheets, and a PostgreSQL database, into a single destination database. The ETL process involves cleaning, transforming, and loading data to facilitate efficient analysis and reporting."
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html#features",
    "href": "content/projects/ETL-SALES/readme.html#features",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "Extracts data from CSV files, Excel spreadsheets, and a PostgreSQL database.\nApplies customizable transformations to clean and format the data.\nLoads the transformed data into a single destination database."
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html#usage",
    "href": "content/projects/ETL-SALES/readme.html#usage",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "Run the ETL pipeline: python PIPELINE_EXEC.py\n\n\n\n420171277_408831038167542_8129673976330505678_n\n\n\ndata set from link : DATA_SOURCE\nI separate csv files into different file sources to demonstrate extracting from different file sources.\nplease see below:\n\n\njanuary to april sales as excel (Sales_January_April_2019)\n\n\n\nmay to august sales as csv(sales_csv folder)\n\n\n\nseptember to december as database (sales_db)"
  },
  {
    "objectID": "content/table_index/readme.html",
    "href": "content/table_index/readme.html",
    "title": "Table Indexes",
    "section": "",
    "text": "Create and populate table\nHow an RDBMS retrieves data\nIndexed table\n[TODO] Filter by year\n[TODO] Group by year\n\n\n\n\n\nUnraid 6.11.5\nDocker 20.10.21\ni7-4770S 4C/8T\n16GB DDR3\n500GB Crucial MX500 SSD\nOther hardware exist but are not used in this demonstration\n\n\n\n\nThe RDBMS used for this demonstration is MySQL 8.1.0. It is running as a Docker container on top of an Unraid 6.11.5 server. The container instance is running un-constrained. It has full access to the CPU and memory resources. The MySQL data files reside on the SSD. Only the MySQL container will be running during the query executions.\nThe principles and (relative) results will be largely the same when other RDBMS are used. The syntax may be different, however.\n\n\nQuickest RDBMS for me to set up and tear down.\n\n\n\n\nThe raw source documents are written in R Markdown using R 4.3.2. R Markdown is similar in structure and function to a .ipynb Python notebook you may be familiar with.\n\n\nAs data engineers, you will have to support a variety of languages/environments/platforms used by different data-end-users. This is a way to expose you to a language other than Python.\n\n\n\n\n\nNone of the code presented are production-ready. It is advisable to use error-tolerant practices such as tryCatch() and DB transactions when working with DBs.\nAll queries are run only once. Ideally, each should be run at least 5 times, then the mean of each run is taken."
  },
  {
    "objectID": "content/table_index/readme.html#toc",
    "href": "content/table_index/readme.html#toc",
    "title": "Table Indexes",
    "section": "",
    "text": "Create and populate table\nHow an RDBMS retrieves data\nIndexed table\n[TODO] Filter by year\n[TODO] Group by year"
  },
  {
    "objectID": "content/table_index/readme.html#server-platform",
    "href": "content/table_index/readme.html#server-platform",
    "title": "Table Indexes",
    "section": "",
    "text": "Unraid 6.11.5\nDocker 20.10.21\ni7-4770S 4C/8T\n16GB DDR3\n500GB Crucial MX500 SSD\nOther hardware exist but are not used in this demonstration"
  },
  {
    "objectID": "content/table_index/readme.html#mysql-rdbms",
    "href": "content/table_index/readme.html#mysql-rdbms",
    "title": "Table Indexes",
    "section": "",
    "text": "The RDBMS used for this demonstration is MySQL 8.1.0. It is running as a Docker container on top of an Unraid 6.11.5 server. The container instance is running un-constrained. It has full access to the CPU and memory resources. The MySQL data files reside on the SSD. Only the MySQL container will be running during the query executions.\nThe principles and (relative) results will be largely the same when other RDBMS are used. The syntax may be different, however.\n\n\nQuickest RDBMS for me to set up and tear down."
  },
  {
    "objectID": "content/table_index/readme.html#r-markdown",
    "href": "content/table_index/readme.html#r-markdown",
    "title": "Table Indexes",
    "section": "",
    "text": "The raw source documents are written in R Markdown using R 4.3.2. R Markdown is similar in structure and function to a .ipynb Python notebook you may be familiar with.\n\n\nAs data engineers, you will have to support a variety of languages/environments/platforms used by different data-end-users. This is a way to expose you to a language other than Python."
  },
  {
    "objectID": "content/table_index/readme.html#disclaimers",
    "href": "content/table_index/readme.html#disclaimers",
    "title": "Table Indexes",
    "section": "",
    "text": "None of the code presented are production-ready. It is advisable to use error-tolerant practices such as tryCatch() and DB transactions when working with DBs.\nAll queries are run only once. Ideally, each should be run at least 5 times, then the mean of each run is taken."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html",
    "href": "content/table_index/rmd/how_db_works.html",
    "title": "How SQL DB server works",
    "section": "",
    "text": "In the previous chapter, we set up our sandbox table sample_table with 4 columns and 50 million rows. Now, we will execute a few queries to understand how SQL databases retrieve the rows we request."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#connect-to-the-db",
    "href": "content/table_index/rmd/how_db_works.html#connect-to-the-db",
    "title": "How SQL DB server works",
    "section": "Connect to the DB",
    "text": "Connect to the DB"
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#initial-exploration",
    "href": "content/table_index/rmd/how_db_works.html#initial-exploration",
    "title": "How SQL DB server works",
    "section": "Initial exploration",
    "text": "Initial exploration\nLet’s take a look at what we are dealing with. We know we have entry_date in the table sorted in ascending order."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#start-mysql-profiling",
    "href": "content/table_index/rmd/how_db_works.html#start-mysql-profiling",
    "title": "How SQL DB server works",
    "section": "Start MySQL profiling",
    "text": "Start MySQL profiling"
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#query-1-retrieve-just-the-first-day",
    "href": "content/table_index/rmd/how_db_works.html#query-1-retrieve-just-the-first-day",
    "title": "How SQL DB server works",
    "section": "Query 1: Retrieve just the first day",
    "text": "Query 1: Retrieve just the first day\nWe know that the first day we have data for is at the top of the table.\nIt takes around 17 seconds.\nThe DBI library performed commit DB commands after the query, and again, after the SHOW PROFILES command.\nIf you are using a GUI client to connect to your DB, it may show a time for the query to execute. In MySQL Workbench, this would be shown as Duration/Fetch. And for this query, MySQL Workbench shows 0.029 sec / 17.224 sec. The sum of these is roughly the same as the duration shown in the profiles result.\nThe majority of the time is spent in executing."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#query-2-retrieve-just-the-last-day",
    "href": "content/table_index/rmd/how_db_works.html#query-2-retrieve-just-the-last-day",
    "title": "How SQL DB server works",
    "section": "Query 2: Retrieve just the last day",
    "text": "Query 2: Retrieve just the last day\nLet’s do the same for the last day we have, 2023-12-31. We know that the last day of the data we have is at the end of the table."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Open Data Sources",
    "section": "",
    "text": "Explore various Philippine data sources for diverse data projects:\n\n\n\nPhilippine Standard Geographic Code (PSGC): A comprehensive list of administrative divisions in the Philippines.\nCOVID Data via DOH Data Drop: Official Department of Health COVID-19 data repository.\nPhilippine Statistics Authority (PSA): Various statistical datasets available for public use.\nPSA Annual Poverty Indicators Survey (API): Data on annual poverty indicators in the Philippines.\nPSA Family Income and Expenditure Survey (FIES): Information on family incomes and expenditures.\nCities and Municipalities Competitiveness Index (CMCI): Data assessing the competitiveness of cities and municipalities.\nBSP Banks Directory: Directory of banks and financial institutions in the Philippines.\nBSP Financial Service Access Points: A listing of financial service access points in the Philippines.\nUnified Accounts Code Structure (UACS): Standard for government accounting in the Philippines.\nOpen Data Philippines (beta): Portal for various government datasets.\nWorld Bank Philippines Data: Country-specific economic data and indicators.\nAwesome Data Philippines by BNHR: A curated collection of Philippine data resources.\n\n\n\n\n\nHydroSheds Data: Hydrological data and high-resolution mapping.\nEarthquake Data: Seismic activity data from around the world.\nHazard Data: Data from the World Environment Situation Room.\nNatural Resources Data: Datasets related to the Earth’s natural resources.\nTyphoon Data: Tracking and information on typhoons affecting the Philippines.\nClimate Data: Climate datasets provided by NOAA.\nLand Cover by ESRI: Worldwide data on land cover.\nMarine Boundaries: Detailed information on global marine regions.\nCoral Reefs: Data and mapping of coral reefs.\nPhilippines - Subnational Administrative Boundaries: Data on the administrative boundaries within the Philippines.\n\n\n\n\n\nEconomic and Social Database: Data provided by the Philippine Institute for Development Studies.\n\n\n\n\n\nGoogle Datasets: Explore datasets offered by Google Cloud.\nGoogle Research Datasets: Find datasets for research projects.\nBigQuery Public Datasets: Access a variety of public datasets through Google BigQuery.\n\n\n\n\n\nWorld Health Organization (WHO) Collections: Global health data: https://www.who.int/data/collections\nHealthcare Images: Public datasets of healthcare images: https://cloud.google.com/healthcare-api/docs/resources/public-datasets/idc\nGenome Datasets: Data from the International Genome project: https://www.internationalgenome.org/data\nNOAA Data Products: Datasets from the National Oceanic and Atmospheric Administration: https://www.ncei.noaa.gov/products\nClimate Data: Comprehensive climate datasets: https://www.climate.gov/maps-data/all\n\n\n\n\n\nUnicef Data Collections: Data on global child welfare: https://data.unicef.org/resources/dataset/sowc-2019-statistical-tables/\nUS Population Data: Population statistics from the US Bureau of Labor Statistics: https://www.bls.gov/cps/tables.htm\nStanford Open Policing Project: Data on police traffic stops: https://openpolicing.stanford.edu/\nScikit-Learn Datasets: Datasets available in the Scikit-Learn library: https://scikit-learn.org/stable/datasets/toy_dataset.html\nPyTorch Datasets: Datasets available in PyTorch: https://pytorch.org/vision/stable/datasets.html\nHuggingface Datasets: A hub for various ML datasets: https://huggingface.co/datasets\nWikipedia ML Research Datasets: A list of datasets for machine learning research: https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research\n\n\n\n\n\nWorld Health Organization (WHO) Collections: Access global health data.\nHealthcare Images: Explore public datasets of healthcare images.\nGenome Datasets: Utilize data from the International Genome project.\nNOAA Data Products: Find datasets from the National Oceanic and Atmospheric Administration.\nClimate Data: Research comprehensive climate datasets.\n\n\n\n\n\nUnicef Data Collections: Data on global child welfare.\nUS Population Data: Statistics from the US Bureau of Labor Statistics.\nStanford Open Policing Project: Information on police traffic stops.\nScikit-Learn Datasets: Datasets within the Scikit-Learn library.\nPyTorch Datasets: Datasets available through PyTorch.\nHuggingface Datasets: A platform offering various ML datasets.\nWikipedia ML Research Datasets: A comprehensive list of datasets for machine learning research.\nOECD"
  },
  {
    "objectID": "datasets.html#philippine-data-sources",
    "href": "datasets.html#philippine-data-sources",
    "title": "Open Data Sources",
    "section": "",
    "text": "Explore various Philippine data sources for diverse data projects:\n\n\n\nPhilippine Standard Geographic Code (PSGC): A comprehensive list of administrative divisions in the Philippines.\nCOVID Data via DOH Data Drop: Official Department of Health COVID-19 data repository.\nPhilippine Statistics Authority (PSA): Various statistical datasets available for public use.\nPSA Annual Poverty Indicators Survey (API): Data on annual poverty indicators in the Philippines.\nPSA Family Income and Expenditure Survey (FIES): Information on family incomes and expenditures.\nCities and Municipalities Competitiveness Index (CMCI): Data assessing the competitiveness of cities and municipalities.\nBSP Banks Directory: Directory of banks and financial institutions in the Philippines.\nBSP Financial Service Access Points: A listing of financial service access points in the Philippines.\nUnified Accounts Code Structure (UACS): Standard for government accounting in the Philippines.\nOpen Data Philippines (beta): Portal for various government datasets.\nWorld Bank Philippines Data: Country-specific economic data and indicators.\nAwesome Data Philippines by BNHR: A curated collection of Philippine data resources.\n\n\n\n\n\nHydroSheds Data: Hydrological data and high-resolution mapping.\nEarthquake Data: Seismic activity data from around the world.\nHazard Data: Data from the World Environment Situation Room.\nNatural Resources Data: Datasets related to the Earth’s natural resources.\nTyphoon Data: Tracking and information on typhoons affecting the Philippines.\nClimate Data: Climate datasets provided by NOAA.\nLand Cover by ESRI: Worldwide data on land cover.\nMarine Boundaries: Detailed information on global marine regions.\nCoral Reefs: Data and mapping of coral reefs.\nPhilippines - Subnational Administrative Boundaries: Data on the administrative boundaries within the Philippines.\n\n\n\n\n\nEconomic and Social Database: Data provided by the Philippine Institute for Development Studies.\n\n\n\n\n\nGoogle Datasets: Explore datasets offered by Google Cloud.\nGoogle Research Datasets: Find datasets for research projects.\nBigQuery Public Datasets: Access a variety of public datasets through Google BigQuery.\n\n\n\n\n\nWorld Health Organization (WHO) Collections: Global health data: https://www.who.int/data/collections\nHealthcare Images: Public datasets of healthcare images: https://cloud.google.com/healthcare-api/docs/resources/public-datasets/idc\nGenome Datasets: Data from the International Genome project: https://www.internationalgenome.org/data\nNOAA Data Products: Datasets from the National Oceanic and Atmospheric Administration: https://www.ncei.noaa.gov/products\nClimate Data: Comprehensive climate datasets: https://www.climate.gov/maps-data/all\n\n\n\n\n\nUnicef Data Collections: Data on global child welfare: https://data.unicef.org/resources/dataset/sowc-2019-statistical-tables/\nUS Population Data: Population statistics from the US Bureau of Labor Statistics: https://www.bls.gov/cps/tables.htm\nStanford Open Policing Project: Data on police traffic stops: https://openpolicing.stanford.edu/\nScikit-Learn Datasets: Datasets available in the Scikit-Learn library: https://scikit-learn.org/stable/datasets/toy_dataset.html\nPyTorch Datasets: Datasets available in PyTorch: https://pytorch.org/vision/stable/datasets.html\nHuggingface Datasets: A hub for various ML datasets: https://huggingface.co/datasets\nWikipedia ML Research Datasets: A list of datasets for machine learning research: https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research\n\n\n\n\n\nWorld Health Organization (WHO) Collections: Access global health data.\nHealthcare Images: Explore public datasets of healthcare images.\nGenome Datasets: Utilize data from the International Genome project.\nNOAA Data Products: Find datasets from the National Oceanic and Atmospheric Administration.\nClimate Data: Research comprehensive climate datasets.\n\n\n\n\n\nUnicef Data Collections: Data on global child welfare.\nUS Population Data: Statistics from the US Bureau of Labor Statistics.\nStanford Open Policing Project: Information on police traffic stops.\nScikit-Learn Datasets: Datasets within the Scikit-Learn library.\nPyTorch Datasets: Datasets available through PyTorch.\nHuggingface Datasets: A platform offering various ML datasets.\nWikipedia ML Research Datasets: A comprehensive list of datasets for machine learning research.\nOECD"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Community Contents",
    "section": "",
    "text": "Explore a rich collection of community-driven content, featuring insightful videos, blogs, and articles from data engineering experts. These resources offer valuable perspectives on various aspects of data engineering and analytics.\n\n\n\n\nA brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\n\n\n\nAn introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\n\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\n\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\n\n\n\nAn online event with key thought leaders and content creators in the Filipino tech community.\n\n\n\nA panel discussion hosted by R User’s Group Philippines on transitioning to a career in data.\n\n\n\n\n\n\n\nUnlocking the Future: Discussing Shifting into Tech with Kuya Dev\n\n\n\n\n\nUnlocking Data Engineering with Josh Valdeleon\n\n\n\n\n\nUnlocking Career Opportunities: Philippine Skills Framework for AI and Analytics with Sherwin Pelayo\n\n\n\n\n\nDiscussion on Data Engineering and Analytics with Gerard\n\n\n\n\n\nXavier Puspus talks about minting AI engineers through education\n\n\n\n\n\nDiscussing Tech and Data Careers with Noemi\n\n\n\n\n\nFrom Dropout to Tech Star: Aemy Obinguar’s Inspirational Tale\n\n\n\n\n\nExclusive Interview: Sandy Lauguico’s Data Engineering Transition\n\n\n\n\n\n\n\nKyle Escosia’s talk on Big Data and Analytics, recorded live at an AWS Siklab Pilipinas event.\n\n\n\nA session by Kyle Escosia on creating a serverless data lake in AWS.\n\n\n\n\n\nSnowflake in the Philippines - Kyle Escosia’s insights on Snowflake’s rise in the Philippines.\nUPSERTS and DELETS using AWS Glue and Delta Lake - A guide on using AWS Glue with Delta Lake for data operations.\nTable Indexes - An educational piece on the importance and implementation of table indexes.\n\n\n\n\n\nBasic ETL project\nETL Project with Azure Databricks - A walkthrough of an ETL project using Azure Databricks.\nContainerization - An exploration of containerization in data engineering.\n\n\n\n\n\nKyle Escosia - Explore the work of Kyle Escosia, a Data Engineer with a passion for all things data."
  },
  {
    "objectID": "readme.html#videos-and-presentations",
    "href": "readme.html#videos-and-presentations",
    "title": "Community Contents",
    "section": "",
    "text": "A brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\n\n\n\nAn introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\n\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\n\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\n\n\n\nAn online event with key thought leaders and content creators in the Filipino tech community.\n\n\n\nA panel discussion hosted by R User’s Group Philippines on transitioning to a career in data."
  },
  {
    "objectID": "readme.html#doc-ligot-interviews",
    "href": "readme.html#doc-ligot-interviews",
    "title": "Community Contents",
    "section": "",
    "text": "Unlocking the Future: Discussing Shifting into Tech with Kuya Dev\n\n\n\n\n\nUnlocking Data Engineering with Josh Valdeleon\n\n\n\n\n\nUnlocking Career Opportunities: Philippine Skills Framework for AI and Analytics with Sherwin Pelayo\n\n\n\n\n\nDiscussion on Data Engineering and Analytics with Gerard\n\n\n\n\n\nXavier Puspus talks about minting AI engineers through education\n\n\n\n\n\nDiscussing Tech and Data Careers with Noemi\n\n\n\n\n\nFrom Dropout to Tech Star: Aemy Obinguar’s Inspirational Tale\n\n\n\n\n\nExclusive Interview: Sandy Lauguico’s Data Engineering Transition"
  },
  {
    "objectID": "readme.html#recorded-events-and-talks",
    "href": "readme.html#recorded-events-and-talks",
    "title": "Community Contents",
    "section": "",
    "text": "Kyle Escosia’s talk on Big Data and Analytics, recorded live at an AWS Siklab Pilipinas event.\n\n\n\nA session by Kyle Escosia on creating a serverless data lake in AWS."
  },
  {
    "objectID": "readme.html#blogs-and-articles",
    "href": "readme.html#blogs-and-articles",
    "title": "Community Contents",
    "section": "",
    "text": "Snowflake in the Philippines - Kyle Escosia’s insights on Snowflake’s rise in the Philippines.\nUPSERTS and DELETS using AWS Glue and Delta Lake - A guide on using AWS Glue with Delta Lake for data operations.\nTable Indexes - An educational piece on the importance and implementation of table indexes."
  },
  {
    "objectID": "readme.html#projects",
    "href": "readme.html#projects",
    "title": "Community Contents",
    "section": "",
    "text": "Basic ETL project\nETL Project with Azure Databricks - A walkthrough of an ETL project using Azure Databricks.\nContainerization - An exploration of containerization in data engineering."
  },
  {
    "objectID": "readme.html#people-and-pages",
    "href": "readme.html#people-and-pages",
    "title": "Community Contents",
    "section": "",
    "text": "Kyle Escosia - Explore the work of Kyle Escosia, a Data Engineer with a passion for all things data."
  },
  {
    "objectID": "study-roadmap.html",
    "href": "study-roadmap.html",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "Data Engineering - by Sandy\n\nDataEngineerRoadmap_Notion - Data Engineering roadmap with a variety of course options from free to paid.\n\n\n\nBy Nicksy via Data Camp\n\n\nData Engineering\n\n\n\nData Engineering\n\n\n\n\nData Analyst\n\n\n\nData Analyst\n\n\n\n\nRoadmap.sh\n\nRoadmap.sh: Roadmaps and study guides for Python, Data Scientist, SQL, and more."
  }
]