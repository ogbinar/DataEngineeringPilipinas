[
  {
    "objectID": "technical-knowledgebase.html",
    "href": "technical-knowledgebase.html",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "This section provides a comprehensive overview of various technologies and tools in the field of data engineering.\n\n\n\nPostgreSQL: An open-source object-relational database system known for its reliability and robust features.\nMySQL: The most popular open-source SQL database management system, developed by Oracle Corporation.\nAmazon Relational Database System (RDS): A managed service for setting up and scaling databases in the cloud, supporting multiple database engines.\n\n\n\n\n\nAmazon Redshift: A cloud data warehouse known for its fast performance, available in both provisioned and serverless configurations.\nGoogle BigQuery: A serverless enterprise data warehouse that offers high scalability and cost-effectiveness.\n\n\n\n\n\nRedis: An open-source, in-memory key-value store known for its versatility and performance.\nAmazon DynamoDB: A fully managed, serverless NoSQL database designed for modern applications.\n\n\n\n\n\nAmazon S3: Offers scalable and secure object storage services.\nAzure Blob Storage: Ideal for storing large-scale cloud-native workloads, archives, and data lakes.\nGoogle Cloud Storage: A flexible service for storing and retrieving any amount of data.\n\n\n\n\n\nApache Kafka: A distributed event streaming platform with various implementations:\n\nConfluent‚Äôs Apache Kafka: A fully managed service with expert support.\nAmazon MSK: Amazon‚Äôs fully managed Kafka service.\n\nAWS SDK for pandas (AWS Wrangler): Extends pandas library to AWS, allowing seamless integration with AWS data services.\nAWS Kinesis: A cloud-based service for real-time data processing.\nAirbyte: An open-source data integration platform for ELT pipelines.\nPentaho Data Integration (Kettle): Includes both a core data integration engine and a graphical user interface for defining jobs and transformations.\n\n\n\n\n\nApache Avro: A serialization format ideal for streaming data pipelines.\nApache Parquet: An open-source columnar data file format.\nApache ORC: Optimized for Hadoop workloads.\n\n\n\n\n\nDelta Lake: An open-source storage framework by Databricks for building Lakehouse architecture.\nApache Iceberg: An open table format for huge analytical datasets, developed by Netflix.\nApache Hudi: A framework for managing large analytical datasets, created by Uber.\n\n\n\n\n\n\n\nApache Spark: A versatile engine for big data processing, available in various languages like Python (PySpark), Scala, Java, and R.\n\n\n\n\n\nPresto: A distributed SQL query engine for big data.\nApache Hive: Built on Hadoop, it facilitates reading, writing, and managing large datasets.\nApache Drill: An open-source SQL query engine.\nTrino: A SQL query engine designed for large data sets.\n\n\n\n\n\nAWS Elastic MapReduce (EMR): A cloud big data platform for processing large datasets.\nAWS Glue: A serverless data integration service.\n\n\n\n\n\n\nSpark Streaming: A part of Apache Spark for processing live data streams.\nSpark Structured Streaming: A stream processing engine built on Spark SQL.\nApache Flink: A framework for stateful computations over data streams.\nApache Storm: A system for processing streaming data in real-time.\n\n\n\n\nApache Druid: A high-performance real-time analytics database.\nApache Pinot: A real-time distributed OLAP datastore.\n\n\n\n\n\n\nApache Airflow: An open-source platform for managing complex computational workflows and data processing pipelines.\nMage: A modern replacement for Airflow for transforming and integrating data.\nDagster: An orchestration platform for data assets.\nPrefect: A workflow orchestration tool for data pipelines.\nKestra: An orchestrator for both scheduled and event-driven workflows.\nAWS Step Functions: Coordinates components of distributed applications.\n\n\n\n\n\n\n\nData Build Tool (dbt): A transformation workflow that follows software engineering best practices.\nSQLMesh: An open-source data transformation framework for SQL and Python.\n\n\n\n\n\n\n\n\nDataHub Project: An extensible metadata platform by LinkedIn.\nOpenMetadata: A platform for discovering, collaborating, and managing data.\nApache Atlas: An open-source metadata and governance framework.\nAmundsen: A data discovery and metadata engine by Lyft.\n\n\n\n\n\nGreat Expectations: A platform for data quality and observability.\n\n\n\n\n\n\nDatabricks: Offers a unified Data Lakehouse platform.\nSnowflake: A cloud-native data warehouse platform."
  },
  {
    "objectID": "technical-knowledgebase.html#relational-databases",
    "href": "technical-knowledgebase.html#relational-databases",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "PostgreSQL: An open-source object-relational database system known for its reliability and robust features.\nMySQL: The most popular open-source SQL database management system, developed by Oracle Corporation.\nAmazon Relational Database System (RDS): A managed service for setting up and scaling databases in the cloud, supporting multiple database engines."
  },
  {
    "objectID": "technical-knowledgebase.html#columnar-databases",
    "href": "technical-knowledgebase.html#columnar-databases",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Amazon Redshift: A cloud data warehouse known for its fast performance, available in both provisioned and serverless configurations.\nGoogle BigQuery: A serverless enterprise data warehouse that offers high scalability and cost-effectiveness."
  },
  {
    "objectID": "technical-knowledgebase.html#key-value-stores",
    "href": "technical-knowledgebase.html#key-value-stores",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Redis: An open-source, in-memory key-value store known for its versatility and performance.\nAmazon DynamoDB: A fully managed, serverless NoSQL database designed for modern applications."
  },
  {
    "objectID": "technical-knowledgebase.html#object-storage",
    "href": "technical-knowledgebase.html#object-storage",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Amazon S3: Offers scalable and secure object storage services.\nAzure Blob Storage: Ideal for storing large-scale cloud-native workloads, archives, and data lakes.\nGoogle Cloud Storage: A flexible service for storing and retrieving any amount of data."
  },
  {
    "objectID": "technical-knowledgebase.html#data-ingestion",
    "href": "technical-knowledgebase.html#data-ingestion",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Kafka: A distributed event streaming platform with various implementations:\n\nConfluent‚Äôs Apache Kafka: A fully managed service with expert support.\nAmazon MSK: Amazon‚Äôs fully managed Kafka service.\n\nAWS SDK for pandas (AWS Wrangler): Extends pandas library to AWS, allowing seamless integration with AWS data services.\nAWS Kinesis: A cloud-based service for real-time data processing.\nAirbyte: An open-source data integration platform for ELT pipelines.\nPentaho Data Integration (Kettle): Includes both a core data integration engine and a graphical user interface for defining jobs and transformations."
  },
  {
    "objectID": "technical-knowledgebase.html#data-formats",
    "href": "technical-knowledgebase.html#data-formats",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Avro: A serialization format ideal for streaming data pipelines.\nApache Parquet: An open-source columnar data file format.\nApache ORC: Optimized for Hadoop workloads."
  },
  {
    "objectID": "technical-knowledgebase.html#data-storage-framework",
    "href": "technical-knowledgebase.html#data-storage-framework",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Delta Lake: An open-source storage framework by Databricks for building Lakehouse architecture.\nApache Iceberg: An open table format for huge analytical datasets, developed by Netflix.\nApache Hudi: A framework for managing large analytical datasets, created by Uber."
  },
  {
    "objectID": "technical-knowledgebase.html#batch-processing",
    "href": "technical-knowledgebase.html#batch-processing",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Spark: A versatile engine for big data processing, available in various languages like Python (PySpark), Scala, Java, and R.\n\n\n\n\n\nPresto: A distributed SQL query engine for big data.\nApache Hive: Built on Hadoop, it facilitates reading, writing, and managing large datasets.\nApache Drill: An open-source SQL query engine.\nTrino: A SQL query engine designed for large data sets.\n\n\n\n\n\nAWS Elastic MapReduce (EMR): A cloud big data platform for processing large datasets.\nAWS Glue: A serverless data integration service."
  },
  {
    "objectID": "technical-knowledgebase.html#stream-processing",
    "href": "technical-knowledgebase.html#stream-processing",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Spark Streaming: A part of Apache Spark for processing live data streams.\nSpark Structured Streaming: A stream processing engine built on Spark SQL.\nApache Flink: A framework for stateful computations over data streams.\nApache Storm: A system for processing streaming data in real-time.\n\n\n\n\nApache Druid: A high-performance real-time analytics database.\nApache Pinot: A real-time distributed OLAP datastore."
  },
  {
    "objectID": "technical-knowledgebase.html#workflow-orchestration",
    "href": "technical-knowledgebase.html#workflow-orchestration",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Apache Airflow: An open-source platform for managing complex computational workflows and data processing pipelines.\nMage: A modern replacement for Airflow for transforming and integrating data.\nDagster: An orchestration platform for data assets.\nPrefect: A workflow orchestration tool for data pipelines.\nKestra: An orchestrator for both scheduled and event-driven workflows.\nAWS Step Functions: Coordinates components of distributed applications."
  },
  {
    "objectID": "technical-knowledgebase.html#data-transformation",
    "href": "technical-knowledgebase.html#data-transformation",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Data Build Tool (dbt): A transformation workflow that follows software engineering best practices.\nSQLMesh: An open-source data transformation framework for SQL and Python."
  },
  {
    "objectID": "technical-knowledgebase.html#data-governance",
    "href": "technical-knowledgebase.html#data-governance",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "DataHub Project: An extensible metadata platform by LinkedIn.\nOpenMetadata: A platform for discovering, collaborating, and managing data.\nApache Atlas: An open-source metadata and governance framework.\nAmundsen: A data discovery and metadata engine by Lyft.\n\n\n\n\n\nGreat Expectations: A platform for data quality and observability."
  },
  {
    "objectID": "technical-knowledgebase.html#data-platforms",
    "href": "technical-knowledgebase.html#data-platforms",
    "title": "Technical Knowledge Base",
    "section": "",
    "text": "Databricks: Offers a unified Data Lakehouse platform.\nSnowflake: A cloud-native data warehouse platform."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Free Resources",
    "section": "",
    "text": "IBM Data Engineering Basics for Everyone on edX - An introductory course to data engineering principles offered by IBM on edX.\nGoogle Cloud Data Engineering Path - A learning path designed to master data engineering on the Google Cloud Platform.\nMeta Database Engineer Professional Certificate on Coursera - A professional certification program by Meta on Coursera, focusing on database engineering.\nBig Data Specialization by UC San Diego on Coursera - A series of courses by UC San Diego on Coursera, covering big data analysis techniques and tools.\nData Engineering Zoomcamp on GitHub - A free, self-paced data engineering bootcamp hosted on GitHub, covering a range of tools and practices."
  },
  {
    "objectID": "resources.html#data-engineering-training",
    "href": "resources.html#data-engineering-training",
    "title": "Free Resources",
    "section": "",
    "text": "IBM Data Engineering Basics for Everyone on edX - An introductory course to data engineering principles offered by IBM on edX.\nGoogle Cloud Data Engineering Path - A learning path designed to master data engineering on the Google Cloud Platform.\nMeta Database Engineer Professional Certificate on Coursera - A professional certification program by Meta on Coursera, focusing on database engineering.\nBig Data Specialization by UC San Diego on Coursera - A series of courses by UC San Diego on Coursera, covering big data analysis techniques and tools.\nData Engineering Zoomcamp on GitHub - A free, self-paced data engineering bootcamp hosted on GitHub, covering a range of tools and practices."
  },
  {
    "objectID": "resources.html#data-engineering-books",
    "href": "resources.html#data-engineering-books",
    "title": "Free Resources",
    "section": "Data Engineering Books",
    "text": "Data Engineering Books\n\nFundamentals of Data Engineering - A guide by Redpanda to the basics of data engineering, covering essential concepts and practices.\nDatabase Technology Overview - An overview of database technology fundamentals.\nErwin in Database Design - Insights into using Erwin for database design.\nModern Data Engineering Playbook - ThoughtWorks presents strategies for modern data engineering, focusing on scalable, efficient solutions.\nThe Data Engineering Cookbook - A comprehensive guide for data engineering practices.\nData Engineering Design Patterns (DEDP) - An exploration of convergent evolution in data engineering, emphasizing design patterns for data systems."
  },
  {
    "objectID": "resources.html#sql-tutorials-and-courses",
    "href": "resources.html#sql-tutorials-and-courses",
    "title": "Free Resources",
    "section": "SQL Tutorials and Courses",
    "text": "SQL Tutorials and Courses\n\nW3 Schools SQL Tutorial - A comprehensive tutorial for SQL beginners.\nSQLBolt - Practice with real-world datasets and challenging SQL problems to deepen your skills."
  },
  {
    "objectID": "resources.html#interactive-sql-learning-games",
    "href": "resources.html#interactive-sql-learning-games",
    "title": "Free Resources",
    "section": "Interactive SQL Learning Games",
    "text": "Interactive SQL Learning Games\n\nSQL Murder Mystery - Solve a captivating murder case using your SQL skills in this immersive game-like environment. Suitable for beginners.\nSQL Island - Navigate through an adventure on SQL Island to learn SQL commands. Remember to change the language to English via the hamburger icon.\nSchemaVerse - A space-based strategy game where you use SQL commands to control your fleet and conquer the universe.\nLost at SQL - The SQL Learning Game - Enhance your SQL skills through this engaging learning game."
  },
  {
    "objectID": "resources.html#sql-practice-and-challenges",
    "href": "resources.html#sql-practice-and-challenges",
    "title": "Free Resources",
    "section": "SQL Practice and Challenges",
    "text": "SQL Practice and Challenges\n\n8 Week SQL Challenge - Intermediate/Advanced SQL challenges through interactive projects.\nHackerRank SQL Challenges - Test your mettle against others in coding challenges that push your SQL boundaries. Suitable for Intermediate-Advanced learners.\nCodewars SQL Kata - Hone your understanding with kata-style SQL exercises, practicing diverse concepts.\nSQLZoo - Practice with a wide range of exercises at different difficulty levels, mastering SQL fundamentals."
  },
  {
    "objectID": "resources.html#spreadsheets-for-data-analysis",
    "href": "resources.html#spreadsheets-for-data-analysis",
    "title": "Free Resources",
    "section": "Spreadsheets for Data Analysis",
    "text": "Spreadsheets for Data Analysis\n\nExcel Basics and Advanced Techniques\n\nEssential Spreadsheets Book 1 - University of York - Introduction to Excel.\nEssential Spreadsheets Book 2 - University of York - Advanced Excel techniques.\nExcel Video Training - Microsoft - Official tutorials from Microsoft.\n\n\n\nGoogle Sheets for Analysis\n\nGoogle Sheets Training & Help - Official Google Sheets guide.\nData Analysis in Google Sheets - Measure School - Analyzing data using Google Sheets.\nHow to Analyze Data in Google Sheets - Data analysis techniques in Google Sheets.\n\n\n\nComprehensive Data Analysis with Excel\n\nData Analysis Excel - Simplilearn - Simplilearn‚Äôs guide on Excel data analysis.\nExcel Data Analysis - TutorialsPoint - Detailed tutorials on Excel data analysis."
  },
  {
    "objectID": "resources.html#learn-python",
    "href": "resources.html#learn-python",
    "title": "Free Resources",
    "section": "Learn Python",
    "text": "Learn Python\nPython is a versatile and powerful programming language that‚Äôs great for beginners and professionals alike. Here are some resources to get started or enhance your Python skills:\n\nPython Official Getting Started Guide - Official Python documentation and guide on getting started with Python.\nLearnpython.org - A free interactive Python tutorial for people who want to learn Python, starting from the basics.\nOpenClassrooms - Learn Programming with Python - A course designed to introduce you to programming using Python.\nPrinciples of Computation with Python - Carnegie Mellon University - An open & free course by CMU focusing on computational principles using Python.\nCodecademy Python Course - Interactive Python programming courses for all levels.\nMIT OpenCourseWare - Introduction to Computer Science and Programming in Python - A course by MIT that introduces the fundamental ideas of computing using Python.\nHarvard University - Python Programming - Various Python programming courses offered by Harvard University.\nfreeCodeCamp - Learn Python - A collection of free Python courses for beginners to help learn Python programming.\nGoogle Cloud - Python on Google Cloud - Learn how to use Python with Google Cloud services for building and deploying applications."
  },
  {
    "objectID": "resources.html#learn-r",
    "href": "resources.html#learn-r",
    "title": "Free Resources",
    "section": "Learn R",
    "text": "Learn R\nR is a programming language and environment commonly used for statistical computing and graphics. The following resources provide comprehensive guides, tutorials, and courses for beginners to advanced users interested in learning R:\n\nCodecademy - Learn R - An interactive platform offering a course designed to get you started with R programming.\nW3Schools R Tutorial - Provides a quick and easy understanding of R, covering basics to advanced topics.\nRStudio Education - Learn R - Beginner resources compiled by RStudio, aiming to make learning R easier and more effective.\nProgramiz - Learn R Programming - Offers R tutorials for beginners to learn R programming online.\nCodecademy - R Catalog - Discover more R courses offered by Codecademy to deepen your understanding and skills.\nSwirl - Learn R, in R - A platform that offers interactive R programming lessons directly within the R console.\nHands-On Programming with R - Teaches how to perform data analysis with R through practical examples, covering basics to more advanced topics."
  },
  {
    "objectID": "resources.html#web-scraping-with-python",
    "href": "resources.html#web-scraping-with-python",
    "title": "Free Resources",
    "section": "Web Scraping with Python",
    "text": "Web Scraping with Python\nWeb scraping is a method used to extract data from websites. Python offers several libraries and tools for web scraping. Here are some essential resources to get started or enhance your web scraping skills:\n\nBeautiful Soup Documentation - Official documentation for Beautiful Soup, a Python library designed for quick turnaround projects like screen-scraping.\nPython Requests Library - Official documentation for Requests, a simple HTTP library for Python, used to send HTTP requests easily.\nScrape Quotes - A practice website designed for scraping quotes from famous authors.\n[\n\nScrape This Site](https://www.scrapethissite.com/) - A website that offers lessons and challenges for web scraping practices. - freeCodeCamp Web Scraping Python Tutorial - A comprehensive guide on how to scrape data from a website using Python. - GeeksforGeeks Python Web Scraping Tutorial - Offers a tutorial on web scraping using Python, covering basics to advanced topics. - Beautiful Soup Web Scraper Python - Real Python - A tutorial that explains how to use Beautiful Soup for web scraping effectively. - Python Web Scraping - A Practical Introduction - Real Python - Provides a practical introduction to web scraping using Python, including setting up your environment and parsing HTML. - Fullstack Python - Provides free tutorials and guides on how to do both front end and back end in Python."
  },
  {
    "objectID": "resources.html#database-and-data-processing-technologies",
    "href": "resources.html#database-and-data-processing-technologies",
    "title": "Free Resources",
    "section": "Database and Data Processing Technologies",
    "text": "Database and Data Processing Technologies\nEnhance your knowledge in various database technologies and data processing frameworks through these tutorials and courses.\n\nMySQL Tutorial - Comprehensive tutorials for beginners to advanced users of MySQL.\nMongoDB Python Developer Learning Path - A structured learning path for Python developers focusing on MongoDB.\nPySpark Tutorial - Tutorials to learn PySpark for big data processing.\nApache NiFi Tutorial - Learn about Apache NiFi for data routing and transformation.\nDebezium Tutorial - Introduction and advanced concepts for using Debezium for data capture."
  },
  {
    "objectID": "resources.html#streaming-and-integration",
    "href": "resources.html#streaming-and-integration",
    "title": "Free Resources",
    "section": "Streaming and Integration",
    "text": "Streaming and Integration\nCourses that focus on streaming data platforms and event streaming architectures.\n\nConfluent Kafka Fundamentals - Free courses on Apache Kafka fundamentals provided by Confluent."
  },
  {
    "objectID": "resources.html#data-warehousing",
    "href": "resources.html#data-warehousing",
    "title": "Free Resources",
    "section": "Data Warehousing",
    "text": "Data Warehousing\nCourses and tutorials to deepen understanding of data warehousing concepts and technologies.\n\nData Warehousing Tutorial - Basics to advanced concepts in data warehousing.\nIntroduction to Snowflake Cloud Data Warehouse - A beginner‚Äôs course on using Snowflake for data warehousing and analytics.\nDBT Fundamentals - Learn how to transform data in your warehouse using dbt (data build tool)."
  },
  {
    "objectID": "resources.html#devops-and-cicd",
    "href": "resources.html#devops-and-cicd",
    "title": "Free Resources",
    "section": "DevOps and CI/CD",
    "text": "DevOps and CI/CD\nLearn about continuous integration and continuous deployment, crucial for modern software development practices.\n\nCI/CD Pipeline Tutorial - An overview of CI/CD concepts and how to implement them."
  },
  {
    "objectID": "resources.html#foundational-data-science-courses",
    "href": "resources.html#foundational-data-science-courses",
    "title": "Free Resources",
    "section": "Foundational Data Science Courses",
    "text": "Foundational Data Science Courses\n\nThe Open Source Data Science Masters - A comprehensive curriculum for self-study in data science.\nData Science Essentials- Microsoft - Foundation course on edX covering essential data science concepts.\nHarvard CS109 Data Science - In-depth course covering data science methodologies and Python.\nData Science Fundamentals ‚Äì IBM - Covers fundamentals of data science through hands-on practice on the Cognitive Class platform."
  },
  {
    "objectID": "resources.html#introduction-to-data-science",
    "href": "resources.html#introduction-to-data-science",
    "title": "Free Resources",
    "section": "Introduction to Data Science",
    "text": "Introduction to Data Science\n\nIntroduction to Data Science by Jeff Hammerbacher at UC, Berkeley - Lectures and materials on the introduction to data science.\nIntroduction to Data Science - Offers a broad introduction to data science through Coursera.\nIntroduction to Data Science - Another Coursera course providing foundational knowledge in data science.\nData Science for Beginners by Microsoft - Beginner-friendly data science materials hosted on GitHub."
  },
  {
    "objectID": "resources.html#specialized-topics-in-data-science",
    "href": "resources.html#specialized-topics-in-data-science",
    "title": "Free Resources",
    "section": "Specialized Topics in Data Science",
    "text": "Specialized Topics in Data Science\n\nLearning from Data ‚Äì California Institute of Technology & UCBerkeley - Focus on machine learning and data analysis.\nProcess Mining - Data Science in Action - Specialized course on process mining techniques.\nPattern Discovery in Data Mining - Course on discovering patterns in datasets.\nIntroduction to Data Mining - Lecture notes and materials on data mining concepts.\nMining Massive Datasets - Advanced course on dealing with massive datasets."
  },
  {
    "objectID": "resources.html#statistical-analysis-and-data-wrangling",
    "href": "resources.html#statistical-analysis-and-data-wrangling",
    "title": "Free Resources",
    "section": "Statistical Analysis and Data Wrangling",
    "text": "Statistical Analysis and Data Wrangling\n\nStatistical Thinking and Data Analysis - Courses focused on statistical thinking and data analysis techniques.\nOpen intro to Statistics - Provides a solid introduction to statistics.\nRegression Analysis - Learn approaches for analyzing multivariate data sets, emphasizing analysis of variance, linear regression, and logistic regression.\nIntroduction to Data Wrangling at the School of Data - Focuses on data cleaning and preparation techniques.\nElements of Statistical Learning by Hastie, Tibshirani, and Friedman\nPattern Recognition and Machine Learning by Bishop\nMathematics for Machine Learning by Deisenroth, Faisal, Soon Ong\nMachine Learning - a Probabilistic Perspective by Murphy\nProbabilistic Machine Learning - An Introduction by Murphy"
  },
  {
    "objectID": "resources.html#programming-for-data-science",
    "href": "resources.html#programming-for-data-science",
    "title": "Free Resources",
    "section": "Programming for Data Science",
    "text": "Programming for Data Science\n\nPython for Data Science - Course by Cognitive Class on using Python for data science tasks.\nData Science - R Basics - Harvard University - Introduction to using R for data science."
  },
  {
    "objectID": "resources.html#online-learning-platforms-and-resources",
    "href": "resources.html#online-learning-platforms-and-resources",
    "title": "Free Resources",
    "section": "Online Learning Platforms and Resources",
    "text": "Online Learning Platforms and Resources\n\nCognitive Class - Interactive platform offering courses on data science and AI.\nJovian - For sharing and collaborating on Jupyter notebooks.\nDataPen - Interactive tutorials on data science and machine learning.\nData Analytics Educational Resources - Collection of learning materials for data analytics.\nExecutive Levels Data Software Training - Offers training on various data software tools.\nSQLZOO - Interactive SQL tutorials for hands-on learning."
  },
  {
    "objectID": "resources.html#youtube-learning-playlists",
    "href": "resources.html#youtube-learning-playlists",
    "title": "Free Resources",
    "section": "YouTube Learning Playlists",
    "text": "YouTube Learning Playlists\nHere are a few playlists most of which are contributions by Jemar and Sandy Cabanes.\n\nStatistics and Data Analysis\n\nCrashCourse Statistics - Quick statistics overview.\nStatQuest - Statistics Fundamentals - Fundamental statistics concepts.\nStatistics Course for Data Science - Statistics tutorials tailored for data science.\nTile Stats - Detailed statistical analysis and education.\n[Tina Huang](https://youtu.be/Q2mF-1yyz3g?si=ZESy\n\n1IAC5wxVRod1) - Data science tutorials, including R and Python programming. - Data Analyst Bootcamp by Alex - Comprehensive bootcamp for becoming a data analyst.\n\n\nData Visualization and Business Intelligence\n\nTableau Dashboard - Tutorials on creating dashboards using Tableau.\nPower BI - Training sessions on using Power BI for data analysis and visualization.\nMo Chen - Business intelligence and analytics techniques.\n\n\n\nProfessional Development and Storytelling\n\nPresentation and Storytelling - Improve your presentation skills and learn the art of storytelling.\nCreate a Portfolio Website with AlexTheAnalyst - A comprehensive tutorial on creating and hosting a portfolio using GitHub Pages.\n\n\n\nGoogle Analytics\n\nGoogle Analytics for Beginners - Learn how to track your portfolio or website visitors.\nAdvanced Google Analytics - Dive deeper into Google Analytics for complex tracking and analysis.\n\n\n\nData Engineering\n\nYouTube Playlist on Data Engineering Projects - A curated list of video tutorials and project ideas for aspiring data engineers.\nData Professor - Insights and tutorials on data engineering practices.\nTechTFQ - Technology-focused tutorials including data engineering topics.\n\n\n\nProgramming and Software Tutorials\n\nKevin Stratvert - Guides and tips on using software tools effectively.\nTech with Tim - Programming tutorials, especially in Python and web development.\nSocratica - Educational videos on programming and computer science.\nHarvard CS50‚Äôs Introduction to Programming with Python - Full university course on programming with Python.\n\n\n\nMiscellaneous\n\nMS Fabric - Explore Microsoft‚Äôs design framework."
  },
  {
    "objectID": "resources.html#additional-learning-resources",
    "href": "resources.html#additional-learning-resources",
    "title": "Free Resources",
    "section": "Additional Learning Resources",
    "text": "Additional Learning Resources\n\nLearn Git Branching - Learn about Git\nGitHub - Platform for code sharing and collaboration.\nPLURALSIGHT - Offers video courses on a wide range of tech topics, including data science.\nhackerRank - Platform for practicing coding and data science skills.\nWorld Quant - Provides resources for quantitative analysis.\nReal World Data Science Use Cases - A showcase for data science in action\nSeattleDataGuy‚Äôs Newsletter - Learn About End-To-End Data Flows (Data Engineering, MLOps, and Data Science)\nGit Expert in 4 Hours - A concise course to become proficient in Git.\nAzure Storage Blog - Updates and articles on Azure storage solutions.\nDatabricks Free Training - Free training resources for learning Databricks Unified Analytics Platform."
  },
  {
    "objectID": "resources.html#training-and-internships",
    "href": "resources.html#training-and-internships",
    "title": "Free Resources",
    "section": "Training and Internships",
    "text": "Training and Internships\n\nThe Sparks Foundation - A remote one-month internship in Data Science and Business Analytics.\nFor The Women Foundation - FTW is a nonprofit organization providing free data science and technology training for women"
  },
  {
    "objectID": "resources.html#online-learning-platforms",
    "href": "resources.html#online-learning-platforms",
    "title": "Free Resources",
    "section": "Online Learning Platforms",
    "text": "Online Learning Platforms\n\nDataquest - An interactive learning platform focusing on data science and analytics skills.\nDatacamp - Offers hands-on courses on data science, Python, R, and SQL among others.\nCodecademy - Provides interactive programming courses across many different technology areas.\nSoloLearn - A mobile-first platform offering courses on a wide array of programming languages.\nW3Schools - A comprehensive resource for learning web development technologies and languages.\nKhan Academy - Free online courses in various subjects, including computer programming.\nCoursera - Online courses, specializations, and degrees from universities and educational institutions.\nedX - Access to online university-level courses in a wide range of disciplines.\nfreeCodeCamp - An open-source community providing free coding bootcamp and certifications.\nUdacity - Specializes in technology courses that offer Nanodegrees and certifications.\nThe Odin Project - A free coding curriculum that provides a complete path to web development.\nMozilla Developer Network (MDN) - Documentation and learning resources for web developers by Mozilla.\nKaggle - A platform for predictive modelling and analytics competitions and datasets."
  },
  {
    "objectID": "resources.html#cloud-resources",
    "href": "resources.html#cloud-resources",
    "title": "Free Resources",
    "section": "Cloud Resources",
    "text": "Cloud Resources\n\nAWS Training and Certification - Training and certification resources for Amazon Web Services.\nGoogle Cloud Training - Training resources for Google Cloud Platform.\nMicrosoft Learn - Learning resources for Microsoft technologies, including Azure.\nCloud Free Tier Comparison - Articles comparing free tier offers from AWS, Azure, GCP, and Oracle Cloud."
  },
  {
    "objectID": "resources.html#open-courseware",
    "href": "resources.html#open-courseware",
    "title": "Free Resources",
    "section": "Open Courseware",
    "text": "Open Courseware\n\nData Analysis with R - Comprehensive R course for data analysis.\nData Engineering Zoomcamp - Free, self-paced online course on data engineering.\nData Science in a Box w/ R - Curriculum for teaching and learning data science using R.\nThe Open Source Data Science Masters - Curriculum for a self-taught education in data science.\nFast.ai Courses - Practical deep learning for coders, taught by fast.ai.\nDive into Deep Learning - An interactive deep learning book with code, math, and discussions.\nGIS Programming Roadmap on GitHub - A roadmap for learning GIS programming.\nIntel Machine Learning Course - This course provides an overview of machine learning fundamentals on modern Intel architecture.\nIntel Deep Learning Course - This course provides an introduction to deep learning on modern Intel architecture."
  },
  {
    "objectID": "resources.html#open-books",
    "href": "resources.html#open-books",
    "title": "Free Resources",
    "section": "Open Books",
    "text": "Open Books\n\nData Science and R Programming\n\nR for Data Science - Learn data science with R.\nData Science - R Basics - Foundation of data science in R.\nR Graphics Cookbook - Data visualization with ggplot2 in R.\nGeocomputation with R - Spatial data analysis with R.\nExploratory Data Analysis with R - Techniques for EDA using R.\nThe Art of Data Science - The process\n\nof data analysis. - R for Data Science (Second Edition) - Updated guide for data science with R. - R Packages - Creating R packages. - R datasciencebook - R datasciencebook.\n\n\nPython Programming\n\nPython for Everybody - Introduction to programming using Python.\nModern Polars - Using Polars library in Python for data manipulation.\nPython datasciencebook - Python datasciencebook.\n\n\n\nVisualization and Web Scraping\n\nData Visualization - A Practical Introduction - Introduction to data visualization.\nWeb Scraping with R - Techniques for web scraping using R.\nInteractive Data Visualization - Learning interactive data visualization.\n\n\n\nAdvanced Topics and Specialized Areas\n\nTelling Stories with Data - Narrative techniques in data communication.\nSpatial Data Science - Spatial data analysis.\nHands-On Programming with R - Practical programming with R.\nPython Geospatial Analysis - Geospatial analysis with Python.\nTime Series Analysis with R - Time series analysis techniques.\nForecasting: Principles and Practice with R - This textbook is intended to provide a comprehensive introduction to forecasting methods and to present enough information about each method for readers to be able to use them sensibly.\nCookbook for R Polars - Using Polars package in R for data frames.\nNFL Analytics with R - Analyzing NFL data with R.\nData Management in R - Strategies for data management with R.\nmlr3book - Machine learning in R with mlr3.\nPython for Geocomputation - Geocomputational analysis with Python.\nRaps with R - Music and analysis with R.\n\n\n\nMiscellaneous\n\nOpen Books by Open UMN - Free educational textbooks, including data science and statistics.\nPandas for Everyone - Comprehensive guide to using Pandas for data analysis.\nSaylor Academy - Free and open online courses for people everywhere.\nGoalkicker - Programming Notes for Professionals - Free programming books on various topics.\nPython Programming Tutorials - Comprehensive resource for learning Python.\nScratch - Imagine, Program, Share - Create stories, games, and animations.\nWaggle Dance - An interactive Python tutorial.\nInvent with Python - Books for learning Python with a focus on making things.\nOpen Source Society University - Computer Science - Path to a free self-taught education in Computer Science.\nLearn Python the Hard Way - Free eBook Download - A book for learning Python programming.\nData Analysis with Python - Spring 2020 - University of Helsinki‚Äôs course on Python for data analysis.\nExercism - Code practice and mentorship for everyone.\nGit and GitHub Tutorial for Beginners - Introduction to Git and GitHub.\nRStudio - Books - Resources for learning R programming.\nAwesome Courses - This list is an attempt to bring to light those awesome CS courses which make their high-quality material i.e.¬†assignments, lectures, notes, readings & examinations available online for free.\nBooks - Learn Anything is collecting all material with focus on Computer Science but you‚Äôll find other topics ranging from neuro science to philosophy.\nLearn to Program - This list aims to be a curated set of high quality educational resources with focus on foundations of web developement.\nawesome-dataviz - A curated list of awesome data visualization libraries and resources."
  },
  {
    "objectID": "resources.html#training-and-internships-1",
    "href": "resources.html#training-and-internships-1",
    "title": "Free Resources",
    "section": "Training and Internships",
    "text": "Training and Internships\n\nThe Sparks Foundation - A remote one-month internship in Data Science and Business Analytics.\nFor The Women Foundation - FTW is a nonprofit organization providing free data science and technology training for women"
  },
  {
    "objectID": "partners.html",
    "href": "partners.html",
    "title": "Partnerships",
    "section": "",
    "text": "Power BI Pilipinas - A hub for Power BI users in the Philippines.\nR Users Group - Philippines - Facebook page for R programming language users.\nAnalytics Association of the Philippines - The official website for the Analytics Association of the Philippines.\nDataSense Analytics Group - World-class training and development programs developed by top teachers and industry practitioners.\n\n\n\n\n\nPractical AI Philippines Meetup - A Meetup group for practical AI discussions.\nGen AI Philippines - Advancing the Filipino Community with AI Innovation.\n\n\n\n\n\nPython Philippines - A community for Python enthusiasts in the Philippines.\nOpen Source Software PH - Open Source Software Philippines or OSSPH is a developer-led initiative to grow the community of developers building open source software across the Philippines.\nArduino Day Philippines - Official Facebook Page of Arduino Day Philippines.\nDEVCON - DEVCON Philippines is the largest community of software developers, geeks, and tech future makers.\nFilipino Web Development Peers - A group for forward-thinking individuals in various fields.\nJava User Group Philippines - Java User Group Philippines - A community for Java developers, students, and enthusiasts in the Philippines to connect, share, and grow.\n\n\n\n\n\nEudoxyz - Eudoxyz is an online community dedicated to cater professionals and aspirants in their respective journey.\nTech Career Shifter - A community for those looking to shift their careers into tech.\nTech Opportunities Philippines - This group is a hub for tech enthusiasts and like-minded individuals passionate about sharing exciting opportunities.\nCode.Sydney - Code.Sydney is a volunteering organisation that supports beginner developers transition to gain paid employment while helping non-profit and charity organisations with their app needs. Below are some of our projects.\n\n\n\n\n\nStudent Developers Philippines - A space for student developers in the Philippines to collaborate and share knowledge.\nUP Data Science Society - The UP Data Science Society is a system-wide organization dedicated to empowering data enthusiasts.\nRTU Organization of Statistics Students - The Official Page of the Organization of Statistics Students in Rizal Technological University - Boni.\nGoogle Developer Student Clubs PLM - Google Developer Student Clubs PLM is a premiere student community of Haribons that shares a common interest in technology and innovation.\nThe SYNTAX Org - Community for explorers of the tech‚Äôs multiverse. Always #TowardsExcellence.\nMicrosoft Student Community - TIP Manila - Microsoft Student Community is a tech community in T.I.P. Manila driven by a passion for different technologies."
  },
  {
    "objectID": "partners.html#data-science-and-analytics",
    "href": "partners.html#data-science-and-analytics",
    "title": "Partnerships",
    "section": "",
    "text": "Power BI Pilipinas - A hub for Power BI users in the Philippines.\nR Users Group - Philippines - Facebook page for R programming language users.\nAnalytics Association of the Philippines - The official website for the Analytics Association of the Philippines.\nDataSense Analytics Group - World-class training and development programs developed by top teachers and industry practitioners."
  },
  {
    "objectID": "partners.html#artificial-intelligence-and-machine-learning",
    "href": "partners.html#artificial-intelligence-and-machine-learning",
    "title": "Partnerships",
    "section": "",
    "text": "Practical AI Philippines Meetup - A Meetup group for practical AI discussions.\nGen AI Philippines - Advancing the Filipino Community with AI Innovation."
  },
  {
    "objectID": "partners.html#software-development-open-source-innovation",
    "href": "partners.html#software-development-open-source-innovation",
    "title": "Partnerships",
    "section": "",
    "text": "Python Philippines - A community for Python enthusiasts in the Philippines.\nOpen Source Software PH - Open Source Software Philippines or OSSPH is a developer-led initiative to grow the community of developers building open source software across the Philippines.\nArduino Day Philippines - Official Facebook Page of Arduino Day Philippines.\nDEVCON - DEVCON Philippines is the largest community of software developers, geeks, and tech future makers.\nFilipino Web Development Peers - A group for forward-thinking individuals in various fields.\nJava User Group Philippines - Java User Group Philippines - A community for Java developers, students, and enthusiasts in the Philippines to connect, share, and grow."
  },
  {
    "objectID": "partners.html#career-transition-and-volunteer-opportunities",
    "href": "partners.html#career-transition-and-volunteer-opportunities",
    "title": "Partnerships",
    "section": "",
    "text": "Eudoxyz - Eudoxyz is an online community dedicated to cater professionals and aspirants in their respective journey.\nTech Career Shifter - A community for those looking to shift their careers into tech.\nTech Opportunities Philippines - This group is a hub for tech enthusiasts and like-minded individuals passionate about sharing exciting opportunities.\nCode.Sydney - Code.Sydney is a volunteering organisation that supports beginner developers transition to gain paid employment while helping non-profit and charity organisations with their app needs. Below are some of our projects."
  },
  {
    "objectID": "partners.html#student-organizations",
    "href": "partners.html#student-organizations",
    "title": "Partnerships",
    "section": "",
    "text": "Student Developers Philippines - A space for student developers in the Philippines to collaborate and share knowledge.\nUP Data Science Society - The UP Data Science Society is a system-wide organization dedicated to empowering data enthusiasts.\nRTU Organization of Statistics Students - The Official Page of the Organization of Statistics Students in Rizal Technological University - Boni.\nGoogle Developer Student Clubs PLM - Google Developer Student Clubs PLM is a premiere student community of Haribons that shares a common interest in technology and innovation.\nThe SYNTAX Org - Community for explorers of the tech‚Äôs multiverse. Always #TowardsExcellence.\nMicrosoft Student Community - TIP Manila - Microsoft Student Community is a tech community in T.I.P. Manila driven by a passion for different technologies."
  },
  {
    "objectID": "free-certificates.html",
    "href": "free-certificates.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "The list is organized as follows:"
  },
  {
    "objectID": "free-certificates.html#database-and-back-end",
    "href": "free-certificates.html#database-and-back-end",
    "title": "Data Engineering Pilipinas",
    "section": "Database and Back-end",
    "text": "Database and Back-end\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nBack End Development and APIs Certification\nFreeCodeCamp\nProfessional\n300\nüèÜ\n\n\nCS403: Introduction to Modern Database Systems\nSaylor Academy\nProfessional\n42\nüèÜ\n\n\nREST API\nHackerRank\nIntermediate\n1.5\nüèÜ\n\n\nMongoDB Basics\nMongoDB\nBeginner\n8.5\nüèÜ\n\n\nMongoDB for SQL Pros\nMongoDB\nBeginner\n2\nüèÜ\n\n\nMongoDB Cluster Administration\nMongoDB\nBeginner\n9.5\nüèÜ\n\n\nMongoDB Aggregation Framework\nMongoDB\nBeginner\n1.5\nüèÜ\n\n\nMongoDB Performance\nMongoDB\nIntermediate\n7\nüèÜ\n\n\nMongoDB for Java Developers\nMongoDB\nIntermediate\n7.5\nüèÜ\n\n\nMongoDB for JavaScript Developers\nMongoDB\nIntermediate\n6.5\nüèÜ\n\n\nMongoDB for .NET Developers\nMongoDB\nIntermediate\n6.5\nüèÜ\n\n\nMongoDB for Python Developers\nMongoDB\nIntermediate\n6.5\nüèÜ\n\n\nDiagnostics and Debugging for MongoDB\nMongoDB\nProfessional\n6.5\nüèÜ\n\n\nMongoDB Data Modeling\nMongoDB\nProfessional\n7\nüèÜ\n\n\nIntro to SQL\nKaggle\nBeginner\n3\nüèÜ\n\n\nAdvanced SQL\nKaggle\nIntermediate\n4\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#data-analysis-and-visualization",
    "href": "free-certificates.html#data-analysis-and-visualization",
    "title": "Data Engineering Pilipinas",
    "section": "Data Analysis and Visualization",
    "text": "Data Analysis and Visualization\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nData Analysis with Python\nIBM / Cognitive Class\nIntermediate\n3\nüèÖ\n\n\nData Analysis with Python Certification\nFreeCodeCamp\nProfessional\n300\nüèÜ\n\n\nData Visualization with Python\nIBM / Cognitive Class\nIntermediate\n3\nüèÖ\n\n\nData Visualization with R\nIBM / Cognitive Class\nBeginner\n3\nüèÖ\n\n\nData Visualization Certification\nFreeCodeCamp\nProfessional\n300\nüèÜ\n\n\nData Visualization\nKaggle\nBeginner\n4\nüèÜ\n\n\nData Cleaning\nKaggle\nIntermediate\n4\nüèÜ\n\n\nData Visualisation in Tableau\nGreat Learning\nBeginner\n1\nüèÜ\n\n\nData Visualization With Power BI\nGreat Learning\nBeginner\n2\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#statistics",
    "href": "free-certificates.html#statistics",
    "title": "Data Engineering Pilipinas",
    "section": "Statistics",
    "text": "Statistics\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nStatistics 101\nIBM / Cognitive Class\nBeginner\n3\nüèÖ"
  },
  {
    "objectID": "free-certificates.html#data-science",
    "href": "free-certificates.html#data-science",
    "title": "Data Engineering Pilipinas",
    "section": "Data Science",
    "text": "Data Science\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nData Science Methodologies\nIBM / Cognitive Class\nBeginner\n3\nüèÖ\n\n\nData Science 101\nIBM / Cognitive Class\nBeginner\n3\nüèÖ\n\n\nData Science Tools\nIBM / Cognitive Class\nBeginner\n4\nüèÖ\n\n\nPython for Data Science\nIBM / Cognitive Class\nBeginner\n3\nüèÖ\n\n\nPopular Applications of Data Science\nGreat Learning\nBeginner\n1\nüèÜ\n\n\nCareer in Data Science\nGreat Learning\nBeginner\n1\nüèÜ\n\n\nIntroduction to Data Science\nGreat Learning\nBeginner\n1\nüèÜ\n\n\nData Science Mathematics\nGreat Learning\nBeginner\n1\nüèÜ\n\n\nR for Data Science\nGreat Learning\nBeginner\n2\nüèÜ\n\n\nStatistical Methods for Data Science\nGreat Learning\nBeginner\n2\nüèÜ\n\n\nProbability for Data Science\nGreat Learning\nBeginner\n2\nüèÜ\n\n\nData Preprocessing\nGreat Learning\nBeginner\n2\nüèÜ\n\n\nSQL for Data Science\nGreat Learning\n\n\n\n\n\n\n     | Beginner   | 3       | üèÜ        |"
  },
  {
    "objectID": "free-certificates.html#big-data",
    "href": "free-certificates.html#big-data",
    "title": "Data Engineering Pilipinas",
    "section": "Big Data",
    "text": "Big Data\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nBig Data Foundations - Level 1\nIBM / Cognitive Class\nBeginner\n3\nüèÖ\n\n\nHadoop Foundations - Level 1\nIBM / Cognitive Class\nBeginner\n4\nüèÖ\n\n\nSpark Fundamentals I\nIBM / Cognitive Class\nBeginner\n5\nüèÖ"
  },
  {
    "objectID": "free-certificates.html#machine-and-deep-learning",
    "href": "free-certificates.html#machine-and-deep-learning",
    "title": "Data Engineering Pilipinas",
    "section": "Machine and Deep Learning",
    "text": "Machine and Deep Learning\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nDeep Learning Fundamentals\nIBM / Cognitive Class\nIntermediate\n3\nüèÖ\n\n\nDeep Learning with TensorFlow\nIBM / Cognitive Class\nBeginner\n3\nüèÖ\n\n\nMachine Learning with Python\nIBM / Cognitive Class\nBeginner\n3\nüèÖ\n\n\nMachine Learning with Python Certification\nFreeCodeCamp\nProfessional\n300\nüèÜ\n\n\nIntro to Machine Learning\nKaggle\nBeginner\n3\nüèÜ\n\n\nIntermediate Machine Learning\nKaggle\nIntermediate\n4\nüèÜ\n\n\nMachine Learning Explainability\nKaggle\nIntermediate\n4\nüèÜ\n\n\nIntro to Deep Learning\nKaggle\nIntermediate\n4\nüèÜ\n\n\nTime Series\nKaggle\nIntermediate\n5\nüèÜ\n\n\nFeature Engineering\nKaggle\nIntermediate\n5\nüèÜ\n\n\nComputer Vision\nKaggle\nIntermediate\n4\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#artificial-intelligence",
    "href": "free-certificates.html#artificial-intelligence",
    "title": "Data Engineering Pilipinas",
    "section": "Artificial Intelligence",
    "text": "Artificial Intelligence\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nElements of AI\nUniversity of Helsinki\nBeginner\n30\nüèÜ\n\n\nIntro to AI Ethics\nKaggle\nIntermediate\n4\nüèÜ\n\n\nIntro to Game AI and Reinforcement Learning\nKaggle\nIntermediate\n4\nüèÜ\n\n\nIntroduction to Generative AI\nGoogle Cloud Skills Boost\nIntermediate\n0.75\nüèÜ\n\n\nIntroduction to Artificial Intelligence\nGreat Learning\nBeginner\n1.5\nüèÜ\n\n\nFoundations of Prompt Engineering\nAmazon\nIntermediate\n4\nüèÜ\n\n\nGenerative AI for Decision Makers\nAmazon\nBeginner\n3\nüèÜ\n\n\nGenerative AI Fundamentals\nData Bricks\nBeginner\n0.5\nüèÖ\n\n\nCareer Essentials in Generative AI\nLinkedIn & Microsoft\nBeginner\n4.5\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#networking",
    "href": "free-certificates.html#networking",
    "title": "Data Engineering Pilipinas",
    "section": "Networking",
    "text": "Networking\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nNetworking Essentials\nCisco Networking Academy\nIntermediate\n70\nüèÖ\n\n\nCS402: Computer Communications and Networks\nSaylor Academy\nIntermediate\n60\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#operating-systems",
    "href": "free-certificates.html#operating-systems",
    "title": "Data Engineering Pilipinas",
    "section": "Operating Systems",
    "text": "Operating Systems\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nNDG Linux Unhatched\nCisco Networking Academy\nBeginner\n8\nüèÜ\n\n\nNDG Linux Essentials\nCisco Networking Academy\nIntermediate\n70\nüèÜ\n\n\nCS401: Operating Systems\nSaylor Academy\nProfessional\n120\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#robotics-and-iot",
    "href": "free-certificates.html#robotics-and-iot",
    "title": "Data Engineering Pilipinas",
    "section": "Robotics and IoT",
    "text": "Robotics and IoT\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nIntroduction to IoT\nCisco Networking Academy\nBeginner\n20\nüèÖ\n\n\nBuilding Robots with TJBot\nIBM / Cognitive Class\nBeginner\n3\nüèÖ"
  },
  {
    "objectID": "free-certificates.html#chatbots",
    "href": "free-certificates.html#chatbots",
    "title": "Data Engineering Pilipinas",
    "section": "Chatbots",
    "text": "Chatbots\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nHow to Build Chatbots\nIBM / Cognitive Class\nBeginner\n5\nüèÖ\n\n\nChatGPT for Beginners\nGreat Learning\nBeginner\n2\nüèÜ\n\n\nGetting Started with Bard\nGreat Learning\nBeginner\n1\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#devops",
    "href": "free-certificates.html#devops",
    "title": "Data Engineering Pilipinas",
    "section": "DevOps",
    "text": "DevOps\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nContainers, K8s and Istio on IBM cloud\nIBM / Cognitive Class\nBeginner\n9\nüèÖ\n\n\nDocker Essentials\nIBM / Cognitive Class\nBeginner\n4\nüèÖ\n\n\n[IBM Cloud Essentials](https://cognitiveclass.ai/courses/ibm-cloud\n\n\n\n\n\n\n\n-essentials) | IBM / Cognitive Class | Beginner | 4 | üèÖ | | Introduction to Containers, Kubernetes, and OpenShift | IBM / Cognitive Class | Beginner | 3 | üèÖ | | Introduction to Kubernetes | Great Learning | Beginner | 1.5 | üèÜ |"
  },
  {
    "objectID": "free-certificates.html#programming-languages",
    "href": "free-certificates.html#programming-languages",
    "title": "Data Engineering Pilipinas",
    "section": "Programming Languages",
    "text": "Programming Languages\n\nJava\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nJava\nHackerRank\nBeginner\n1\nüèÜ\n\n\n\n\n\nJavaScript\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nJavaScript Algorithms and Data Structures Certification\nFreeCodeCamp\nProfessional\n300\nüèÜ\n\n\nJavaScript\nHackerRank\nBeginner\n1.5\nüèÜ\n\n\n\n\n\nR\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nR 101\nIBM / Cognitive Class\nBeginner\n3\nüèÖ\n\n\nUsing R with Databases\nIBM / Cognitive Class\nBeginner\n3\nüèÖ\n\n\nR\nHackerRank\nBeginner\n1.5\nüèÜ\n\n\n\n\n\nPython\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nProgramming Essentials In Python\nCisco Networking Academy\nIntermediate\n70\nüèÜ\n\n\nScientific Computing with Python Certification\nFreeCodeCamp\nProfessional\n300\nüèÜ\n\n\nPython\nHackerRank\nBeginner\n1.5\nüèÜ\n\n\nIntro to Graph Analytics in Python free course\nMemgraph\nIntermediate\n3\nüèÖ\n\n\nPython\nKaggle\nBeginner\n5\nüèÜ\n\n\nPandas\nKaggle\nBeginner\n4\nüèÜ\n\n\nIntro to Programming\nKaggle\nBeginner\n5\nüèÜ\n\n\nPython for Machine Learning and Data Science\nGreat Learning\nBeginner\n3\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#theories-and-concepts",
    "href": "free-certificates.html#theories-and-concepts",
    "title": "Data Engineering Pilipinas",
    "section": "Theories and Concepts",
    "text": "Theories and Concepts\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nQuality Assurance Certification\nFreeCodeCamp\nProfessional\n300\nüèÜ\n\n\nCS101: Introduction to Computer Science I\nSaylor Academy\nProfessional\n52\nüèÜ\n\n\nCS102: Introduction to Computer Science II\nSaylor Academy\nProfessional\n42\nüèÜ\n\n\nCS201: Elementary Data Structures\nSaylor Academy\nProfessional\n38\nüèÜ\n\n\nCS202: Discrete Structures\nSaylor Academy\nProfessional\n44\nüèÜ\n\n\nCS301: Computer Architecture\nSaylor Academy\nProfessional\n48\nüèÜ\n\n\nCS302: Software Engineering\nSaylor Academy\nProfessional\n45\nüèÜ\n\n\nProblem Solving\nHackerRank\nBeginner\n1.5\nüèÜ\n\n\nProblem Solving\nHackerRank\nIntermediate\n1.5\nüèÜ\n\n\nGraph Modeling Email Course\nMemgraph\nBeginner\n1\nüèÖ\n\n\nGeospatial Analysis\nKaggle\nIntermediate\n4\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#business",
    "href": "free-certificates.html#business",
    "title": "Data Engineering Pilipinas",
    "section": "Business",
    "text": "Business\n\nMarketing\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nDigital Marketing\nGoogle\nBeginner\n40\nüèÖ\n\n\nEmail Marketing\nSendinBlue\nBeginner\n4\nüèÖ\n\n\n\n\n\nProject Management\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nKICKOFF\nProject Management Institute\nBeginner\n0.75\nüèÖ\n\n\nProject Management Essentials\nManagement and Strategy Institute\nBeginner\n0.75\nüèÖ\n\n\nProject Management Essentials\nDisaster Ready\nBeginner\n5\nüèÖ\n\n\n\n\n\nSupport\n\n\n\nCourse Name\nCourse Provider\nLevel\nHour(s)\nReward\n\n\n\n\nCustomer Relationship Management\nGreat Learning\nBeginner\n1\nüèÜ"
  },
  {
    "objectID": "free-certificates.html#want-more-certifications",
    "href": "free-certificates.html#want-more-certifications",
    "title": "Data Engineering Pilipinas",
    "section": "Want More Certifications?",
    "text": "Want More Certifications?\nThe original list can be found here."
  },
  {
    "objectID": "datacamp-donates.html",
    "href": "datacamp-donates.html",
    "title": "DataCamp Donates Application Process",
    "section": "",
    "text": "Scholarship Application Now Open!\nWe‚Äôre thrilled to announce that Data Engineering Pilipinas is now a proud partner of DataCamp Donates! Thanks to this partnership, we‚Äôre offering exclusive scholarships for free premium access to DataCamp‚Äôs top-tier data science courses and resources.\nWhat to Expect:\n\nPremium Access: Unlock top-tier data science courses from DataCamp.\nLearning Journey: Enhance your skills and advance your career with cutting-edge content.\nCommunity Support: Join a network of like-minded learners and grow together.\n\nImportant Dates:\n\nApplication Deadline: July 12, 2024\n\nRules and Expectations:\n\nMonthly Assignments: 4,000 XP (roughly 3 hours of learning per month)\nUse Hashtags: Tag @DataCamp and use #dcdonates in your posts to keep the community updated.\nEngage with Content: Actively participate in the courses and engage with other learners.\nShare Success Stories: We‚Äôd love to hear and share your success stories! Let us know how the courses are helping you grow.\n\nWhat You Need to Do:\n\nApply Now: Submit your application for the scholarship.\nEngage: Plan to actively participate and complete courses.\nFinish Monthly Assignment: 4,000 XP, failed to do so will have your access revoked.\n\nWe‚Äôre excited to support your learning journey and can‚Äôt wait to see the incredible progress you make. Apply today and take the first step towards mastering data science!\n#DataCampDonates #Scholarship #DataEngineeringPilipinas #DataAnalysis #DataScience #DataEngineering #MachineLearning #ArtificialIntelligence #DataLiteracy #ApplyNow #dcdonates\nStart Your Application:\n\nAPPLY NOW\n\n\nDEP x DC Donates - Frequently Asked Questions\nQ1: Who is eligible for the DataCamp sponsorship?\n\nA1: The DataCamp sponsorship is available to all members of Data Engineering Pilipinas.\n\nIf your application has issues that are not covered with the items discussed below, it might take time as we might be getting clearance from DataCamp. So please be patient and make sure to review your compliance to the mechanics.\nQ2: Why was my application declined or not approved yet?\n\nA2: Applications may be declined or not approved yet for several reasons, including:\n\nEach application is reviewed comprehensively to ensure that only genuine applicants receive approval.\nIncomplete Information: Your application may be missing critical details such as full name, correct birthday, city, country, or other required information.\nIncorrect Information: Provided details might be inaccurate or unverifiable.\nPending Review: Your application might still be under review; please allow some time for the process.\n\n\nTo address these issues, ensure all information provided is complete and accurate.\nQ3: How can I avoid common mistakes in my application?\n\nA3: To avoid common mistakes:\n\nCarefully Read Instructions: Follow all instructions on the application form.\nProvide Accurate Information: Ensure personal details like full name, birthday, city, and country are correct.\nComplete All Sections: Fill out every section of the form completely.\n\n\nQ4: What should I do if my application is incomplete?\n\nA4: If you suspect your application is incomplete:\n\nDouble-check Personal Details: Ensure full name, correct birthday, city, country, and other required information are accurately filled.\nComplete the Form: Fill out all sections of the form.\n\n\nIf it has been a while since submission without a response, review and complete your application.\nQ5: How will I know if my application has been approved?\n\nA5: If approved, you will receive a confirmation email with next steps. If no confirmation is received within the expected timeframe, your application may still be under review.\n\nQ6: Is there a chance that my access will be revoked?\n\nA6: There are a few mechanics that will entail removal of DataCamp access.\n\nNon-Acceptance: If you have been invited but do not accept after a few reminders, your invitation will be withdrawn.\nNon-Usage: Accounts are being monitored and if they are not being used, we will revoke your access.\n\n\nWe only have limited slots for the partnership and we want to prioritize access to those that need and will use the platform.\nQ7: I really like this initiative, how do I support it?\n\nA7: You can support it through the following:\n\nInvite your friends that are interested in data to join our community.\nInvite people to apply to our DEP x DC Scholarship Program.\nPost about your journey and successes and follow the posting rules in social media found here.\nHelp other learners in the Data Engineering Community by answering their questions, sharing your thoughts and best practices, and promoting the program.\nShare your milestones and success stories to DataCamp here.\n\n\nQ8: Who can I contact if I have questions about my application?\n\nA8: Kindly review the DATACAMP DONATES x DEP Partnership mechanics for further guidance or ask the community. Visit Data Engineering Pilipinas and join the different DEP communities to connect with other members."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html",
    "href": "content/table_index/rmd/how_db_works.html",
    "title": "How SQL DB server works",
    "section": "",
    "text": "In the previous chapter, we set up our sandbox table sample_table with 4 columns and 50 million rows. Now, we will execute a few queries to understand how SQL databases retrieve the rows we request."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#connect-to-the-db",
    "href": "content/table_index/rmd/how_db_works.html#connect-to-the-db",
    "title": "How SQL DB server works",
    "section": "Connect to the DB",
    "text": "Connect to the DB"
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#initial-exploration",
    "href": "content/table_index/rmd/how_db_works.html#initial-exploration",
    "title": "How SQL DB server works",
    "section": "Initial exploration",
    "text": "Initial exploration\nLet‚Äôs take a look at what we are dealing with. We know we have entry_date in the table sorted in ascending order."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#start-mysql-profiling",
    "href": "content/table_index/rmd/how_db_works.html#start-mysql-profiling",
    "title": "How SQL DB server works",
    "section": "Start MySQL profiling",
    "text": "Start MySQL profiling"
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#query-1-retrieve-just-the-first-day",
    "href": "content/table_index/rmd/how_db_works.html#query-1-retrieve-just-the-first-day",
    "title": "How SQL DB server works",
    "section": "Query 1: Retrieve just the first day",
    "text": "Query 1: Retrieve just the first day\nWe know that the first day we have data for is at the top of the table.\nIt takes around 17 seconds.\nThe DBI library performed commit DB commands after the query, and again, after the SHOW PROFILES command.\nIf you are using a GUI client to connect to your DB, it may show a time for the query to execute. In MySQL Workbench, this would be shown as Duration/Fetch. And for this query, MySQL Workbench shows 0.029 sec / 17.224 sec. The sum of these is roughly the same as the duration shown in the profiles result.\nThe majority of the time is spent in executing."
  },
  {
    "objectID": "content/table_index/rmd/how_db_works.html#query-2-retrieve-just-the-last-day",
    "href": "content/table_index/rmd/how_db_works.html#query-2-retrieve-just-the-last-day",
    "title": "How SQL DB server works",
    "section": "Query 2: Retrieve just the last day",
    "text": "Query 2: Retrieve just the last day\nLet‚Äôs do the same for the last day we have, 2023-12-31. We know that the last day of the data we have is at the end of the table."
  },
  {
    "objectID": "content/table_index/readme.html",
    "href": "content/table_index/readme.html",
    "title": "Table Indexes",
    "section": "",
    "text": "Create and populate table\nHow an RDBMS retrieves data\nIndexed table\n[TODO] Filter by year\n[TODO] Group by year\n\n\n\n\n\nUnraid 6.11.5\nDocker 20.10.21\ni7-4770S 4C/8T\n16GB DDR3\n500GB Crucial MX500 SSD\nOther hardware exist but are not used in this demonstration\n\n\n\n\nThe RDBMS used for this demonstration is MySQL 8.1.0. It is running as a Docker container on top of an Unraid 6.11.5 server. The container instance is running un-constrained. It has full access to the CPU and memory resources. The MySQL data files reside on the SSD. Only the MySQL container will be running during the query executions.\nThe principles and (relative) results will be largely the same when other RDBMS are used. The syntax may be different, however.\n\n\nQuickest RDBMS for me to set up and tear down.\n\n\n\n\nThe raw source documents are written in R Markdown using R 4.3.2. R Markdown is similar in structure and function to a .ipynb Python notebook you may be familiar with.\n\n\nAs data engineers, you will have to support a variety of languages/environments/platforms used by different data-end-users. This is a way to expose you to a language other than Python.\n\n\n\n\n\nNone of the code presented are production-ready. It is advisable to use error-tolerant practices such as tryCatch() and DB transactions when working with DBs.\nAll queries are run only once. Ideally, each should be run at least 5 times, then the mean of each run is taken."
  },
  {
    "objectID": "content/table_index/readme.html#toc",
    "href": "content/table_index/readme.html#toc",
    "title": "Table Indexes",
    "section": "",
    "text": "Create and populate table\nHow an RDBMS retrieves data\nIndexed table\n[TODO] Filter by year\n[TODO] Group by year"
  },
  {
    "objectID": "content/table_index/readme.html#server-platform",
    "href": "content/table_index/readme.html#server-platform",
    "title": "Table Indexes",
    "section": "",
    "text": "Unraid 6.11.5\nDocker 20.10.21\ni7-4770S 4C/8T\n16GB DDR3\n500GB Crucial MX500 SSD\nOther hardware exist but are not used in this demonstration"
  },
  {
    "objectID": "content/table_index/readme.html#mysql-rdbms",
    "href": "content/table_index/readme.html#mysql-rdbms",
    "title": "Table Indexes",
    "section": "",
    "text": "The RDBMS used for this demonstration is MySQL 8.1.0. It is running as a Docker container on top of an Unraid 6.11.5 server. The container instance is running un-constrained. It has full access to the CPU and memory resources. The MySQL data files reside on the SSD. Only the MySQL container will be running during the query executions.\nThe principles and (relative) results will be largely the same when other RDBMS are used. The syntax may be different, however.\n\n\nQuickest RDBMS for me to set up and tear down."
  },
  {
    "objectID": "content/table_index/readme.html#r-markdown",
    "href": "content/table_index/readme.html#r-markdown",
    "title": "Table Indexes",
    "section": "",
    "text": "The raw source documents are written in R Markdown using R 4.3.2. R Markdown is similar in structure and function to a .ipynb Python notebook you may be familiar with.\n\n\nAs data engineers, you will have to support a variety of languages/environments/platforms used by different data-end-users. This is a way to expose you to a language other than Python."
  },
  {
    "objectID": "content/table_index/readme.html#disclaimers",
    "href": "content/table_index/readme.html#disclaimers",
    "title": "Table Indexes",
    "section": "",
    "text": "None of the code presented are production-ready. It is advisable to use error-tolerant practices such as tryCatch() and DB transactions when working with DBs.\nAll queries are run only once. Ideally, each should be run at least 5 times, then the mean of each run is taken."
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html",
    "href": "content/projects/ETL-SALES/readme.html",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "This project is an Extract, Transform, Load (ETL) package designed to merge data from various sources, including CSV files, Excel spreadsheets, and a PostgreSQL database, into a single destination database. The ETL process involves cleaning, transforming, and loading data to facilitate efficient analysis and reporting.\n\n\n\n\nExtracts data from CSV files, Excel spreadsheets, and a PostgreSQL database.\nApplies customizable transformations to clean and format the data.\nLoads the transformed data into a single destination database.\n\n\n\n\nRun the ETL pipeline: python PIPELINE_EXEC.py\n\n\n\n420171277_408831038167542_8129673976330505678_n\n\n\ndata set from link : DATA_SOURCE\nI separate csv files into different file sources to demonstrate extracting from different file sources.\nplease see below:\n\n\njanuary to april sales as excel (Sales_January_April_2019)\n\n\n\nmay to august sales as csv(sales_csv folder)\n\n\n\nseptember to december as database (sales_db)"
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html#overview",
    "href": "content/projects/ETL-SALES/readme.html#overview",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "This project is an Extract, Transform, Load (ETL) package designed to merge data from various sources, including CSV files, Excel spreadsheets, and a PostgreSQL database, into a single destination database. The ETL process involves cleaning, transforming, and loading data to facilitate efficient analysis and reporting."
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html#features",
    "href": "content/projects/ETL-SALES/readme.html#features",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "Extracts data from CSV files, Excel spreadsheets, and a PostgreSQL database.\nApplies customizable transformations to clean and format the data.\nLoads the transformed data into a single destination database."
  },
  {
    "objectID": "content/projects/ETL-SALES/readme.html#usage",
    "href": "content/projects/ETL-SALES/readme.html#usage",
    "title": "ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database",
    "section": "",
    "text": "Run the ETL pipeline: python PIPELINE_EXEC.py\n\n\n\n420171277_408831038167542_8129673976330505678_n\n\n\ndata set from link : DATA_SOURCE\nI separate csv files into different file sources to demonstrate extracting from different file sources.\nplease see below:\n\n\njanuary to april sales as excel (Sales_January_April_2019)\n\n\n\nmay to august sales as csv(sales_csv folder)\n\n\n\nseptember to december as database (sales_db)"
  },
  {
    "objectID": "content/Cloud-Free-Tier-Comparison/LICENSE.html",
    "href": "content/Cloud-Free-Tier-Comparison/LICENSE.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "MIT License\nCopyright (c) 2020 Cloud Study Network.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "community.html",
    "href": "community.html",
    "title": "Community Content",
    "section": "",
    "text": "Explore a rich collection of community-driven content, featuring insightful videos, blogs, and articles from data engineering experts. These resources offer valuable perspectives on various aspects of data engineering and analytics."
  },
  {
    "objectID": "community.html#community-content",
    "href": "community.html#community-content",
    "title": "Community Content",
    "section": "",
    "text": "Explore a rich collection of community-driven content, featuring insightful videos, blogs, and articles from data engineering experts. These resources offer valuable perspectives on various aspects of data engineering and analytics."
  },
  {
    "objectID": "community.html#people-and-pages",
    "href": "community.html#people-and-pages",
    "title": "Community Content",
    "section": "People and Pages",
    "text": "People and Pages\nAs a community, we want to support our homegrown content creators and experts. Please check them out as they champion data advocacy:\n\nData Advocates and Experts\n\nDoc Ligot - Prompt responsible adoption of artificial intelligence\nSherwin Pelayo - Strategic national initiatives to nurture a robust analytics and AI ecosystem\nKyle Escosia - Passionately curious about data\nSandy C. Lauguico - Data Science, Engineering, and AI; Documenting journey to time-money-location freedom and intentional life\nAemyn Obinguar - Fractional Gen AI Engineer\nPower BI Pilipinas - Official page of the Power BI Pilipinas Community\n\n\n\nTech Content Creators\n\nJosh Dev - Tech-related content\nKuya Dev - Tech Career Advice Podcast\nDev Stuff with JP - Software Development content in Filipino\nProf.¬†Bob S. - Programming techniques via video\nDanielle Meer - Competitive programmer, Esports scholar, Machine Learning Engineer\nAtcha Abe - Learn, Grow, and Empower\nThe Agile Geek - Educational resources, Tech tools, and Beyond\nAlex Gamboa - Data Analyst, BI Developer, Pythonista, and ConTech Creator\nFurtim Dev - Full-stack Developer | Talks about Web Development, Technology, and everything in between\nWokeTech - Microsoft Applications and Web Development\nBotnie Data - Aspiring Data Engineer\nDAX Jutsu - Elevated Power BI skills"
  },
  {
    "objectID": "community.html#articles",
    "href": "community.html#articles",
    "title": "Community Content",
    "section": "Articles",
    "text": "Articles\nHere are some blogs and instructionals created by our community members. Let‚Äôs continue to grow this!\n\nBig Data and Cloud\n\nSnowflake in the Philippines - Insights on Snowflake‚Äôs rise in the Philippines\nUPSERTS and DELETES using AWS Glue and Delta Lake - Guide on using AWS Glue with Delta Lake for data operations\n\n\n\nData Engineering\n\nTable Indexes - Importance and implementation of table indexes\nData Manipulation with Pandas Library - Data manipulation with Pandas Library\nContainerization - Exploration of containerization in data engineering"
  },
  {
    "objectID": "community.html#projects",
    "href": "community.html#projects",
    "title": "Community Content",
    "section": "Projects",
    "text": "Projects\nPortfolio development is a critical part of career path in tech. Projects help not only validate but showcase as well the learnings and capabilities of the talent. Here are a few contributions from the community.\n\nEnd-to-End Data Projects\n\nBUILDING YOUR FIRST END-TO-END DATA PORTFOLIO by Josh Dev - Data pipeline, dashboard, and machine learning pipeline\n\n\n\nData Challenges\n\nExcel Challenges playlist by WokeTech - Different Excel challenges\nData Challenges Megapost by Power BI Pilipinas - 11 Data Challenges from different websites\n\n\n\nETL Projects\n\nBasic ETL project - Basic ETL project\nETL Sales - ETL Package: Data Integration from CSV, Excel, and PostgreSQL to a Single Database\nETL Project with Azure Databricks - ETL project using Azure Databricks"
  },
  {
    "objectID": "community.html#videos-and-presentations",
    "href": "community.html#videos-and-presentations",
    "title": "Community Content",
    "section": "Videos and Presentations",
    "text": "Videos and Presentations\n\n\n\n\nA brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\nGetting Started with Data Engineering An introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\n\n\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\n\n\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\nTechSync 2023: Synchronizing Filipino Tech Communities An online event with key thought leaders and content creators in the Filipino tech community.\n\n\n\n\nA panel discussion hosted by R User‚Äôs Group Philippines on transitioning to a career in data.\nIntroduction to Big Data and Analytics Kyle Escosia‚Äôs talk on Big Data and Analytics, recorded live at an AWS Siklab Pilipinas event.\nBuilding a Serverless Data Lake in AWS A session by Kyle Escosia on creating a serverless data lake in AWS.\n\n\n\n\nThe ‚ÄúKwentuhan Meetup‚Äù is an online event for members of the Data Engineering Pilipinas group,\nhosted on Discord.\n\n\n\n\nHow I Got Into Data Science WITHOUT a Computer Science Degree | Tips on Working in Tech Philippines\n\n\n\n\nA PL-300 training session for the Power BI Pilipinas community."
  },
  {
    "objectID": "community.html#doc-ligot-interviews",
    "href": "community.html#doc-ligot-interviews",
    "title": "Community Content",
    "section": "Doc Ligot Interviews",
    "text": "Doc Ligot Interviews\n\nKuya Dev\n\n\n\n\n\n\nJosh V.\n\n\n\n\n\n\nSherwin Pelayo\n\n\n\n\n\n\nGerard\n\n\n\n\n\n\nXavier Puspus\n\n\n\n\n\n\nNoemi\n\n\n\n\n\n\nAemy Obinguar\n\n\n\n\n\n\nSandy Lauguico\n\n\n\n\n\n\nNeil Bacon‚Äôs Transition: Discover how Leoneil Bacon moved from Biology to Data.\n\n\n\n\n\n\nTrisha Nicdao‚Äôs Journey: Insights into Trisha‚Äôs shift from ESG to Data Analytics, touching on sustainability and GenZ perspectives.\n\n\n\n\n\n\nAngel Felismino‚Äôs Career Path: From Computer Science to leading in tech talent acquisition.\n\n\n\n\n\n\nEmmanuel Irog-Irog‚Äôs Innovations: Discussing RAG and LLMs with a focus on recent projects.\n\n\n\n\n\n\nJerel John Velarde‚Äôs Vision: Using data for enhancing democracy in AI startups."
  },
  {
    "objectID": "community.html#mentoring-program-2024",
    "href": "community.html#mentoring-program-2024",
    "title": "Community Content",
    "section": "Mentoring Program 2024",
    "text": "Mentoring Program 2024\n\nObjective\nTo create a network of mentors and mentees within the DE Pilipinas community, focusing on personalized advice and skill development in data engineering.\n\n\nThree Easy Steps\n\n\n\nMentoring Program - pubmat2\n\n\n\nSign Up & Share: Fill out a quick form to tell us about your skills, interests, and what you‚Äôre looking for‚Äîwhether you want to mentor, learn, or both!\nPick Your Match: Browse profiles to find your perfect mentor or mentee. Start the conversation to see if you click, and set up a call to seal the deal.\nLearn & Grow Together: Dive into your mentorship journey, set goals, and check in regularly. Your feedback helps us make the experience even better for everyone.\n\n\n\nData Collection\n\nUtilize forms to collect information from potential mentors and mentees, including areas of expertise or interest, preferred contact methods, and availability.\nInformation to be updated regularly to ensure it reflects current details.\nParticipants must give consent for their information to be shared for pairing purposes.\nMentors register here.\nMentees register here.\n\n\n\nPairing Mechanism\n\nSelf-Initiated Pairing: Mentors and mentees can select their pairs based on the information provided in the forms.\nBoth mentors and mentees are urged to initiate contact, ideally outlining why they believe a partnership would be beneficial and providing relevant information to help the other party determine if there‚Äôs a mentor-mentee fit.\nOnce both parties initially agree to the pairing, they are encouraged to conduct a needs assessment call to confirm compatibility before officially commencing the program.\nA mentor can have multiple mentees, but a mentee can have one mentor at a time. One can be both a mentor and a mentee.\nTo find a mentor and/or a mentee, click here.\n\n\n\nMentor Commitment\n\nOur program places a strong emphasis on supporting our mentors. We believe that by taking good care of our experts, they in turn will be\n\nbetter equipped to nurture their mentees. - Mentors can specify the number of mentees they are willing to take on and their preferred mentoring strategy: some mentors may prefer a hands-on approach, actively guiding mentees through specific technical stacks. Others may opt for a more advisory role, providing strategic guidance and holding regular check-ins. This allows for a personalized mentorship experience that caters to the unique learning styles and objectives of each mentee. - Options for mentor involvement levels help prevent burnout and ensure a productive mentoring relationship.\n\n\nMentee Engagement\n\nMentees can identify their career stage and the type of help they need, allowing mentors to select mentees based on their expertise and interest areas.\nMentees play an active role in this mentorship program. They are expected to demonstrate a strong desire for learning and growth, as the value of the mentorship relationship is directly proportional to their commitment and engagement. Mentees are encouraged to take the initiative in following up, setting up meetings, and driving their learning journey. A balanced approach is expected, one that involves proactive engagement from the mentees while ensuring that the mentor‚Äôs time is respected.\n\n\n\nFeedback and Improvement\n\nRegular follow-ups and feedback collection from both mentors and mentees to assess the effectiveness of the mentorship and make necessary adjustments.\nConsider running polls within the group for additional feedback and suggestions.\n\n\n\nFAQs\nCan I participate if I‚Äôm not from the Philippines? Yes.\nIs the mentoring program free? Yes.\nIs there a certification or recognition at the end of the program? None. [Maybe in the future recognition for mentors, for mentees, the mentoring is the benefit :D]\nCan the mentor-mentee match be changed? Yes, as long as both parties are informed."
  },
  {
    "objectID": "about.html#our-mission",
    "href": "about.html#our-mission",
    "title": "About Data Engineering Pilipinas",
    "section": "Our Mission",
    "text": "Our Mission\nData Engineering Pilipinas is dedicated to democratizing data engineering education by offering open access to learning materials and promoting a collaborative environment for the exchange of tools and skills.\nThe community serves as a forum for both users and developers of data tools, facilitating the sharing of ideas and experiences. Data Engineering Pilipinas encourages discussions on best practices, innovative approaches, and emerging technologies in data management, processing, analytics, and visualization. It embraces a wide range of programming languages including Python, R, and SQL, reflecting the diverse methods used within data engineering and data science.\nWith a commitment to being an accessible, community-driven platform, Data Engineering Pilipinas caters to all levels of expertise, from novice to advanced practitioners. The conferences, tutorials, and talks provided by Data Engineering Pilipinas offer attendees insights into the latest project features as well as cutting-edge use cases."
  },
  {
    "objectID": "about.html#code-of-conduct-based-from-pydata",
    "href": "about.html#code-of-conduct-based-from-pydata",
    "title": "About Data Engineering Pilipinas",
    "section": "Code of Conduct (Based from PyData)",
    "text": "Code of Conduct (Based from PyData)\nBe kind to others. Do not insult or put down others. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes and language are not appropriate for Data Engineering Pilipinas.\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate.\nData Engineering Pilipinas is dedicated to providing a harassment-free event experience for everyone, regardless of gender, sexual orientation, gender identity, and expression, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of participants in any form.\nThank you for helping make this a welcoming, friendly community for all."
  },
  {
    "objectID": "about.html#official-community-guidelines",
    "href": "about.html#official-community-guidelines",
    "title": "About Data Engineering Pilipinas",
    "section": "Official Community Guidelines",
    "text": "Official Community Guidelines\nOur community thrives on respect, collaboration, and shared learning. Here are the guidelines we follow to ensure a positive and productive environment for all members.\n\nBe Respectful Treat all members with courtesy and respect, avoiding offensive language or personal attacks.\nShare Knowledge Encourage the sharing of valuable insights, best practices, and resources related to data engineering.\nNo Trolling or Harassment Engage constructively. Trolling or harassment of other members or administrators is not tolerated.\nAvoid Plagiarism Always attribute credit to the original source when sharing content or ideas.\nBe Mindful of Privacy Do not share personal information or sensitive data in our discussions.\nMinimal Self-Promotion Keep self-promotion or spamming of personal content or products to a minimum, especially if not related to data engineering.\nStay On Topic Ensure discussions are relevant to data engineering, its technologies, and related fields.\nNo Illegal Activities Do not engage in or promote discussions about activities that violate laws or ethical standards.\nLimit Recruitment Posts Job postings and collaboration requests are welcome but keep them moderate and relevant to data engineering roles or opportunities. Use appropriate hashtags like #hiring, #project, and #opportunity.\nKeep it Professional Maintain a professional tone in all interactions and adhere to Facebook‚Äôs guidelines and policies at all times."
  },
  {
    "objectID": "about.html#diversity-inclusion-based-from-pydata",
    "href": "about.html#diversity-inclusion-based-from-pydata",
    "title": "About Data Engineering Pilipinas",
    "section": "Diversity & Inclusion (Based from PyData)",
    "text": "Diversity & Inclusion (Based from PyData)\nData Engineering Pilipinas welcomes and encourages participation in our community by people of all backgrounds and identities. We are committed to promoting and sustaining a culture that values mutual respect, tolerance, and learning, and we work together as a community to help each other live out these values."
  },
  {
    "objectID": "content/Cloud-Free-Tier-Comparison/Articles.html",
    "href": "content/Cloud-Free-Tier-Comparison/Articles.html",
    "title": "Articles",
    "section": "",
    "text": "Articles\nhttps://www.cloudmanagementinsider.com/aws-azure-google-cloud-free-tier-comparison/\nhttps://n2ws.com/blog/amazon-aws-microsoft-azure-google-cloud-free-tier-cloud-computing-service-comparison\nhttps://www.infoworld.com/article/3179785/aws-vs-azure-vs-google-cloud-which-free-tier-is-best.html"
  },
  {
    "objectID": "content/data-engineering-101.html",
    "href": "content/data-engineering-101.html",
    "title": "Data Engineering 101",
    "section": "",
    "text": "Data Engineering Domain"
  },
  {
    "objectID": "content/data-engineering-101.html#what-is-data-engineering",
    "href": "content/data-engineering-101.html#what-is-data-engineering",
    "title": "Data Engineering 101",
    "section": "What is Data Engineering?",
    "text": "What is Data Engineering?\n\nPrimary Focus: Data engineering prepares data for analytical or operational use, emphasizing the practical application of data collection, transformation, and storage."
  },
  {
    "objectID": "content/data-engineering-101.html#roles-and-responsibilities",
    "href": "content/data-engineering-101.html#roles-and-responsibilities",
    "title": "Data Engineering 101",
    "section": "Roles and Responsibilities",
    "text": "Roles and Responsibilities\nAs a data engineer, you will be responsible for:\n\nBuilding and maintaining the infrastructure for data generation, collection, and distribution.\nDeveloping robust and scalable data pipelines that transform and transport data across systems.\nEnsuring data is readily available and in a usable format for analysts and data scientists to perform their tasks."
  },
  {
    "objectID": "content/data-engineering-101.html#skills-and-tools-for-data-engineering",
    "href": "content/data-engineering-101.html#skills-and-tools-for-data-engineering",
    "title": "Data Engineering 101",
    "section": "Skills and Tools for Data Engineering",
    "text": "Skills and Tools for Data Engineering\nTo thrive in data engineering, you will need to develop skills in:\n\nProgramming: Become proficient in languages like Python, Java, or Scala.\nData Management & Governance: Learn to manipulate databases using SQL.\nData Processing Frameworks: Gain expertise in tools such as Apache Hadoop and Apache Spark.\nData Storage and Warehousing: Understand how to implement and manage large-scale data storage solutions."
  },
  {
    "objectID": "content/data-engineering-101.html#related-disciplines",
    "href": "content/data-engineering-101.html#related-disciplines",
    "title": "Data Engineering 101",
    "section": "Related Disciplines",
    "text": "Related Disciplines\nData engineering intersects with several related fields:\n\nData Analysis\n\nDescription: Extracting insights and making sense of data.\nTools: Familiarize yourself with Excel, SQL, and BI tools like Tableau and Power BI.\n\n\n\nData Science\n\nDescription: Going beyond analysis to predict future trends and behaviors using data.\nTools: Learn Python, R, and machine learning libraries to build predictive models.\n\n\n\nMachine Learning Engineering\n\nDescription: Specializing in algorithms that can learn from and make decisions based on data.\nTools: Master Python and frameworks like TensorFlow.\n\n\n\nBusiness Intelligence (BI)\n\nDescription: Transforming data into actionable intelligence for business decisions.\nTools: Use SQL and BI platforms like Tableau, Power BI, or Looker.\n\n\n\nDatabase Administration\n\nDescription: Focusing on the technical management of database systems.\nTools: Understand database management systems like MySQL and PostgreSQL.\n\n\n\nBig Data\n\nDescription: Working with exceptionally large or complex data sets that require specialized approaches.\nTools: Learn to work with Hadoop, Spark, and NoSQL databases."
  },
  {
    "objectID": "content/data-engineering-101.html#the-data-engineering-lifecycle",
    "href": "content/data-engineering-101.html#the-data-engineering-lifecycle",
    "title": "Data Engineering 101",
    "section": "The Data Engineering Lifecycle",
    "text": "The Data Engineering Lifecycle\nUnderstanding the Data Engineering Lifecycle is crucial for managing data effectively:\n\nGeneration: Where and how data is produced.\nIngestion: Moving data to a place where it can be used.\nTransformation: Converting data to a useful format.\nServing: Making data accessible for use.\nStorage: Keeping data safe and retrievable."
  },
  {
    "objectID": "content/data-engineering-101.html#outcomes-of-the-data-engineering-process",
    "href": "content/data-engineering-101.html#outcomes-of-the-data-engineering-process",
    "title": "Data Engineering 101",
    "section": "Outcomes of the Data Engineering Process",
    "text": "Outcomes of the Data Engineering Process\nThe end goal of data engineering can be one of the following:\n\nAnalytics: Deriving insights that inform business strategies.\nMachine Learning: Training models to predict and act upon data.\nReverse ETL: Integrating processed data back into operational systems."
  },
  {
    "objectID": "content/data-engineering-101.html#supporting-practices-in-data-engineering",
    "href": "content/data-engineering-101.html#supporting-practices-in-data-engineering",
    "title": "Data Engineering 101",
    "section": "Supporting Practices in Data Engineering",
    "text": "Supporting Practices in Data Engineering\nThese are the undercurrents that ensure the data flows smoothly throughout the lifecycle:\n\nSecurity: Protecting data integrity and privacy.\nData Management: Ensuring that data is organized and maintained properly.\nDataOps: Streamlining the collaboration between teams working with data.\nData Architecture: Creating the blueprint for data collection and usage.\nOrchestration: Automating processes and workflows.\nSoftware Engineering: Developing the applications that handle data.\n\nBecoming a data engineer means you‚Äôll be at the intersection of data, technology, and business, ensuring that data is a valuable asset that can be leveraged to its full potential."
  },
  {
    "objectID": "content/data-engineering-101.html#alternative-careers",
    "href": "content/data-engineering-101.html#alternative-careers",
    "title": "Data Engineering 101",
    "section": "Alternative Careers",
    "text": "Alternative Careers\n\nData Analyst: Analyzes data to help inform business decisions.\nMachine Learning Engineer: Creates algorithms to predict patterns and behaviors.\nDatabase Administrator: Manages and maintains database systems.\nBusiness Intelligence Analyst: Converts data into actionable business insights.\nData Architect: Designs and manages data solutions.\nData Science Generalist: Handles various data-related tasks in smaller companies.\nSystems Analyst: Improves IT systems through data analysis.\nProduct Manager: Integrates data insights into product strategy.\nOperations Analyst: Optimizes business operations using data.\nQuantitative Analyst: Applies data to financial analysis and risk assessment."
  },
  {
    "objectID": "content/data-engineering-101.html#advantages-of-data-engineering-skills-beyond-data-roles",
    "href": "content/data-engineering-101.html#advantages-of-data-engineering-skills-beyond-data-roles",
    "title": "Data Engineering 101",
    "section": "Advantages of Data Engineering Skills Beyond Data Roles",
    "text": "Advantages of Data Engineering Skills Beyond Data Roles\n\nEnhanced Problem-Solving: Develops structured approaches to solving complex issues.\nLogical Thinking: Fosters a logical mindset beneficial for strategic decision-making.\nTechnical Skills: Provides technical acumen applicable in many modern tech roles.\nData Literacy: Equips with the ability to understand and use data effectively.\nProject Management: Aligns with managing projects, resources, and workflows.\nEffective Communication: Improves the ability to communicate complex ideas clearly.\nAdaptability: Prepares for quick adaptation to industry changes.\nAutomation Knowledge: Offers insights into streamlining and automating processes.\nInterdisciplinary Collaboration: Encourages working across various teams and departments.\n\nLearning data engineering skills can significantly enhance your analytical and technical capabilities, useful in a wide array of professions, not limited to traditional data-centric roles."
  },
  {
    "objectID": "content/projects/pipeline_basic/readme.html",
    "href": "content/projects/pipeline_basic/readme.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "This is a basic ETL project"
  },
  {
    "objectID": "content/projects/pipeline_basic/readme.html#basic-etl-project",
    "href": "content/projects/pipeline_basic/readme.html#basic-etl-project",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "This is a basic ETL project"
  },
  {
    "objectID": "content/table_index/rmd/create_table.html",
    "href": "content/table_index/rmd/create_table.html",
    "title": "Create and populate table",
    "section": "",
    "text": "For our sample table, we will use a simple 4-column table that is a simulated result of an ETL process from some OLTP DB. We have the following values: - date value: You can think of this as an entry date, or a purchase date - numeric value: You can think of this as quantity of items, or a monetary value - descriptive value: You can think of this as a category code, or a branch code\nWe will then populate our table with 50 million rows randomly generated. These will be inserted in date order ascending. Why 50 million? So that we give the DB a little bit of a workout. A DB will not break a sweat with hundred-thousand-row tables."
  },
  {
    "objectID": "content/table_index/rmd/create_table.html#create-our-sample-table",
    "href": "content/table_index/rmd/create_table.html#create-our-sample-table",
    "title": "Create and populate table",
    "section": "Create our sample table",
    "text": "Create our sample table\nFirst, we create our table in our database with the following DDL:\nTake note that at this point, other than the primary key, we do not have any indexes defined.\n\n\n\nField\nData Type\nDescription\n\n\n\n\nid\nINT\nA simple unsigned primary key\n\n\nfk_id\nINT\nA simulated foreign key ID\n\n\narbitrary_value\nINT\nA value we can use aggregate functions on\n\n\nentry_date\nDATETIME\nA datetime value"
  },
  {
    "objectID": "content/table_index/rmd/create_table.html#populate-our-table",
    "href": "content/table_index/rmd/create_table.html#populate-our-table",
    "title": "Create and populate table",
    "section": "Populate our table",
    "text": "Populate our table\n\nGenerate data\nWe will now generate data for our table. To do this we use fixtuRes. We provide a YML configuration file:\n# sample_table.yml\nsample_table:\n  columns:\n    fk_id:\n      type: integer\n      min: 1\n      max: 100\n    arbitrary_value:\n      type: integer\n      min: 0\n      max: 50\n    entry_date:\n      type: date\n      min: 1973-01-01\n      max: 2023-12-31\n  arrange: entry_date\nWe use MockDataGenerator to create our data. This will produce a table ordered by entry_date. Then we add a sequential id column to have the id in the same order as entry_date.\nThe following is a sample output if we used size = 20:\n\n\nConnect to DB\nWe now establish a connection to our MySQL database server. The RMySQL library has been deprecated in favor of the RMariaDB library.\nIt is always good practice to keep your connection credentials like usernames, passwords, API tokens in your environment variables. Never hard-coded in source code. And never commit your environment variable file to the repo.\n\n\nWrite our data to the DB\nWhile there is a function we can use to write our mock_data to the DB table (dbWriteTable(conn, \"sample_table\", mock_data)), remember that we have 50 million rows. When mock_data is written to a CSV file, it results in a 1.3 GB file. Writing this to the DB will take some time. To avoid hitting the connection time-out constraint we will write the data by batches of date.\nLet‚Äôs take a look at what we have.\nDont‚Äô leave any DB connections open!\nNow, we have a 50 million row DB table we can play around with."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html",
    "href": "content/table_index/rmd/indexed_table.html",
    "title": "Indexed table",
    "section": "",
    "text": "In the previous chapter, we saw that an SQL DB parses through the whole table to retrieve rows. Because it does not know where the rows that match the provided conditions are, it has to check every row. This is why it does not matter where in the table the rows are located. This is where INDEXes come in."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#duplicate-our-table",
    "href": "content/table_index/rmd/indexed_table.html#duplicate-our-table",
    "title": "Indexed table",
    "section": "Duplicate our table",
    "text": "Duplicate our table\nWe want to keep our non-indexed table so that we can still run non-indexed queries later.\nIt takes about 15 minutes to make a copy.\nThen we add the primary key and indexes for fk_id and entry_date.\nIt takes about 20 minutes to add these indices."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#database-connection",
    "href": "content/table_index/rmd/indexed_table.html#database-connection",
    "title": "Indexed table",
    "section": "Database Connection",
    "text": "Database Connection\nIn the background, we set up our environment, connect to the database, and turn on profiling.\nA quick check of our tables:\nOur tables are the same except for the indexes."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#show-profile-function",
    "href": "content/table_index/rmd/indexed_table.html#show-profile-function",
    "title": "Indexed table",
    "section": "Show Profile function",
    "text": "Show Profile function\nBecause we will run profiling repeatedly, it makes sense to write it into a function."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-6-run-the-first-day-query-with-the-benefit-of-an-index",
    "href": "content/table_index/rmd/indexed_table.html#query-6-run-the-first-day-query-with-the-benefit-of-an-index",
    "title": "Indexed table",
    "section": "Query 6: Run the ‚Äúfirst day‚Äù query with the benefit of an index",
    "text": "Query 6: Run the ‚Äúfirst day‚Äù query with the benefit of an index\nFrom 17 seconds, we are now down to below 0.007 seconds."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-7-run-the-last-day-query-with-the-benefit-of-an-index",
    "href": "content/table_index/rmd/indexed_table.html#query-7-run-the-last-day-query-with-the-benefit-of-an-index",
    "title": "Indexed table",
    "section": "Query 7: Run the ‚Äúlast day‚Äù query with the benefit of an index",
    "text": "Query 7: Run the ‚Äúlast day‚Äù query with the benefit of an index\nIt is the same for our ‚Äúlast day‚Äù query. Below 0.007 seconds. The DB is not parsing the entire table anymore.\nBut, this is a little bit of a cheat. Remember that entry_date is already sorted. Rows with the same entry_dates are together. How much difference is there if the needles are scattered all over the table?"
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-8-retrieve-all-fk_id-45",
    "href": "content/table_index/rmd/indexed_table.html#query-8-retrieve-all-fk_id-45",
    "title": "Indexed table",
    "section": "Query 8: Retrieve all fk_id = 45",
    "text": "Query 8: Retrieve all fk_id = 45\nWe have a different column we can filter on. fk_id is not sorted. It is randomly distributed across the entire table. Let‚Äôs run a baseline on the non-indexed table.\nAs expected, we get about 17 seconds."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#query-9-retrieve-all-fk_id-45-with-the-help-of-an-index",
    "href": "content/table_index/rmd/indexed_table.html#query-9-retrieve-all-fk_id-45-with-the-help-of-an-index",
    "title": "Indexed table",
    "section": "Query 9: Retrieve all fk_id = 45 with the help of an index",
    "text": "Query 9: Retrieve all fk_id = 45 with the help of an index\nThen the same query from the indexed table.\nWe have a substantial improvement from 17 seconds to less than 5 seconds."
  },
  {
    "objectID": "content/table_index/rmd/indexed_table.html#questions",
    "href": "content/table_index/rmd/indexed_table.html#questions",
    "title": "Indexed table",
    "section": "Questions",
    "text": "Questions\n\nWhat if we need to filter by year? Or by year-month?"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Open Data Sources",
    "section": "",
    "text": "Philippine Standard Geographic Code (PSGC) - Comprehensive list of administrative divisions in the Philippines.\nCOVID Data via DOH Data Drop - Official Department of Health repository for COVID-19 data.\nPhilippine Statistics Authority (PSA) - Various statistical datasets provided by the Philippine Statistics Authority.\nPSA Annual Poverty Indicators Survey (API) - Annual data on poverty indicators in the Philippines.\nPSA Family Income and Expenditure Survey (FIES) - Information on family incomes and expenditures in the Philippines.\nCities and Municipalities Competitiveness Index (CMCI) - Data assessing the competitiveness of Philippine cities and municipalities.\nBSP Banks Directory - Directory of banks and financial institutions operating in the Philippines.\nBSP Financial Service Access Points - Listing of financial service access points across the Philippines.\nUnified Accounts Code Structure (UACS) - Standard for government accounting in the Philippines.\nOpen Data Philippines (beta) - Official portal for various government datasets in the Philippines.\nWorld Bank Philippines Data - Economic data and indicators for the Philippines from the World Bank.\nAwesome Data Philippines by BNHR - Curated collection of Philippine data resources.\nPhilippines - Subnational Administrative Boundaries - Data on administrative boundaries within the Philippines.\n\n\n\n\n\nHydroSheds Data - Hydrological data and high-resolution mapping.\nEarthquake Data - Seismic activity data from the United States Geological Survey.\nHazard Data - Data from the World Environment Situation Room.\nNatural Resources Data - Datasets related to the Earth‚Äôs natural resources.\nTyphoon Data - Tracking and information on typhoons affecting the Philippines.\nClimate Data - Climate datasets from the National Oceanic and Atmospheric Administration (NOAA).\nLand Cover by ESRI - Worldwide data on land cover from Esri.\nMarine Boundaries - Detailed information on global marine regions.\nCoral Reefs - Data and mapping of coral reefs.\n\n\n\n\n\nEconomic and Social Database - Economic and social data provided by the Philippine Institute for Development Studies.\n\n\n\n\n\nGoogle Datasets - Explore datasets offered by Google Cloud.\nGoogle Research Datasets - Search engine for research datasets.\nBigQuery Public Datasets - Access to various public datasets through Google BigQuery.\nPublic API Lists for Data Engineering - List of public APIs useful for data engineering projects.\nGADM Data - Detailed maps and data of administrative areas of countries, useful for GIS and mapping projects.\n\n\n\n\n\nWorld Health Organization (WHO) Collections - Global health data from the World Health Organization.\nHealthcare Images - Public datasets of healthcare images.\nGenome Datasets - Data from the International Genome project.\n\n\n\n\n\nNOAA Data Products - Datasets from the National Oceanic and Atmospheric Administration.\nClimate Data - Comprehensive climate datasets.\n\n\n\n\n\nUnicef Data Collections - Data on global child welfare from UNICEF.\nUS Population Data - Population statistics from the US Bureau of Labor Statistics.\nStanford Open Policing Project - Data on police traffic stops in the United States.\nScikit-Learn Datasets - Datasets available in the Scikit-Learn library for machine learning.\nPyTorch Datasets - Datasets available in the PyTorch library for machine learning.\nHuggingface Datasets - Hub for various machine learning datasets.\nWikipedia ML Research Datasets - List of datasets for machine learning research.\nOECD - Data from the Organisation for Economic Co-operation and Development.\nDatahub - Free and open instance for hosting and sharing datasets.\nKaggle - Platform for data science competitions, hosting, publishing, and analyzing data.\ndata.world - Social network for hosting and sharing data.\nAcademic Torrents - BitTorrent-based platform for sharing large datasets in the academic community."
  },
  {
    "objectID": "datasets.html#government-public-administration-and-census-data",
    "href": "datasets.html#government-public-administration-and-census-data",
    "title": "Open Data Sources",
    "section": "",
    "text": "Philippine Standard Geographic Code (PSGC) - Comprehensive list of administrative divisions in the Philippines.\nCOVID Data via DOH Data Drop - Official Department of Health repository for COVID-19 data.\nPhilippine Statistics Authority (PSA) - Various statistical datasets provided by the Philippine Statistics Authority.\nPSA Annual Poverty Indicators Survey (API) - Annual data on poverty indicators in the Philippines.\nPSA Family Income and Expenditure Survey (FIES) - Information on family incomes and expenditures in the Philippines.\nCities and Municipalities Competitiveness Index (CMCI) - Data assessing the competitiveness of Philippine cities and municipalities.\nBSP Banks Directory - Directory of banks and financial institutions operating in the Philippines.\nBSP Financial Service Access Points - Listing of financial service access points across the Philippines.\nUnified Accounts Code Structure (UACS) - Standard for government accounting in the Philippines.\nOpen Data Philippines (beta) - Official portal for various government datasets in the Philippines.\nWorld Bank Philippines Data - Economic data and indicators for the Philippines from the World Bank.\nAwesome Data Philippines by BNHR - Curated collection of Philippine data resources.\nPhilippines - Subnational Administrative Boundaries - Data on administrative boundaries within the Philippines."
  },
  {
    "objectID": "datasets.html#environmental-geographic-and-natural-resources-data",
    "href": "datasets.html#environmental-geographic-and-natural-resources-data",
    "title": "Open Data Sources",
    "section": "",
    "text": "HydroSheds Data - Hydrological data and high-resolution mapping.\nEarthquake Data - Seismic activity data from the United States Geological Survey.\nHazard Data - Data from the World Environment Situation Room.\nNatural Resources Data - Datasets related to the Earth‚Äôs natural resources.\nTyphoon Data - Tracking and information on typhoons affecting the Philippines.\nClimate Data - Climate datasets from the National Oceanic and Atmospheric Administration (NOAA).\nLand Cover by ESRI - Worldwide data on land cover from Esri.\nMarine Boundaries - Detailed information on global marine regions.\nCoral Reefs - Data and mapping of coral reefs."
  },
  {
    "objectID": "datasets.html#economic-and-social-data",
    "href": "datasets.html#economic-and-social-data",
    "title": "Open Data Sources",
    "section": "",
    "text": "Economic and Social Database - Economic and social data provided by the Philippine Institute for Development Studies."
  },
  {
    "objectID": "datasets.html#general-and-multi-purpose-data-sources",
    "href": "datasets.html#general-and-multi-purpose-data-sources",
    "title": "Open Data Sources",
    "section": "",
    "text": "Google Datasets - Explore datasets offered by Google Cloud.\nGoogle Research Datasets - Search engine for research datasets.\nBigQuery Public Datasets - Access to various public datasets through Google BigQuery.\nPublic API Lists for Data Engineering - List of public APIs useful for data engineering projects.\nGADM Data - Detailed maps and data of administrative areas of countries, useful for GIS and mapping projects."
  },
  {
    "objectID": "datasets.html#health-and-life-sciences-data",
    "href": "datasets.html#health-and-life-sciences-data",
    "title": "Open Data Sources",
    "section": "",
    "text": "World Health Organization (WHO) Collections - Global health data from the World Health Organization.\nHealthcare Images - Public datasets of healthcare images.\nGenome Datasets - Data from the International Genome project."
  },
  {
    "objectID": "datasets.html#climate-and-earth-sciences-data",
    "href": "datasets.html#climate-and-earth-sciences-data",
    "title": "Open Data Sources",
    "section": "",
    "text": "NOAA Data Products - Datasets from the National Oceanic and Atmospheric Administration.\nClimate Data - Comprehensive climate datasets."
  },
  {
    "objectID": "datasets.html#miscellaneous-and-other-data-resources",
    "href": "datasets.html#miscellaneous-and-other-data-resources",
    "title": "Open Data Sources",
    "section": "",
    "text": "Unicef Data Collections - Data on global child welfare from UNICEF.\nUS Population Data - Population statistics from the US Bureau of Labor Statistics.\nStanford Open Policing Project - Data on police traffic stops in the United States.\nScikit-Learn Datasets - Datasets available in the Scikit-Learn library for machine learning.\nPyTorch Datasets - Datasets available in the PyTorch library for machine learning.\nHuggingface Datasets - Hub for various machine learning datasets.\nWikipedia ML Research Datasets - List of datasets for machine learning research.\nOECD - Data from the Organisation for Economic Co-operation and Development.\nDatahub - Free and open instance for hosting and sharing datasets.\nKaggle - Platform for data science competitions, hosting, publishing, and analyzing data.\ndata.world - Social network for hosting and sharing data.\nAcademic Torrents - BitTorrent-based platform for sharing large datasets in the academic community."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Engineering Pilipinas",
    "section": "",
    "text": "Data Engineering Pilipinas is a community for data engineers, data analysts, data scientists, developers, AI / ML engineers, and users of closed and open source data tools and methods / techniques in the Philippines. Data Engineering Pilipinas is a PyData group.\nThis page serves as a repository of notes, thoughts, ideas, plans, dreams, datasets, analyses, and whatever else we think of."
  },
  {
    "objectID": "index.html#getting-started-with-data-engineering",
    "href": "index.html#getting-started-with-data-engineering",
    "title": "Data Engineering Pilipinas",
    "section": "Getting Started with Data Engineering",
    "text": "Getting Started with Data Engineering\n\nRead more about Data Engineering 101"
  },
  {
    "objectID": "index.html#join-our-growing-community",
    "href": "index.html#join-our-growing-community",
    "title": "Data Engineering Pilipinas",
    "section": "Join Our Growing Community",
    "text": "Join Our Growing Community\nConnect with Data Engineering Pilipinas on various platforms. Like, follow, and join our groups and pages to stay updated and engage with our community:"
  },
  {
    "objectID": "index.html#official-datacamp-donates-partner",
    "href": "index.html#official-datacamp-donates-partner",
    "title": "Data Engineering Pilipinas",
    "section": "Official DataCamp Donates Partner",
    "text": "Official DataCamp Donates Partner\n\n\n\nDataCamp Donates\n\n\nData Engineering Pilipinas is an official partner for DataCamp Donates, offering our community members the chance to learn essential data skills, from basics to advanced data engineering, data science, and machine learning.\nNot familiar with DataCamp? Check out this video. It‚Äôs a simple way to learn at your own pace. The program is free for Data Engineering Pilipinas community members, but licenses are limited. Please apply only if you‚Äôre committed to learning data skills.\n\nVIEW SCHOLARSHIP MECHANICS"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Community Contents",
    "section": "",
    "text": "Explore a rich collection of community-driven content, featuring insightful videos, blogs, and articles from data engineering experts. These resources offer valuable perspectives on various aspects of data engineering and analytics.\n\n\n\n\nA brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\n\n\n\nAn introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\n\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\n\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\n\n\n\nAn online event with key thought leaders and content creators in the Filipino tech community.\n\n\n\nA panel discussion hosted by R User‚Äôs Group Philippines on transitioning to a career in data.\n\n\n\n\n\n\n\nUnlocking the Future: Discussing Shifting into Tech with Kuya Dev\n\n\n\n\n\nUnlocking Data Engineering with Josh Valdeleon\n\n\n\n\n\nUnlocking Career Opportunities: Philippine Skills Framework for AI and Analytics with Sherwin Pelayo\n\n\n\n\n\nDiscussion on Data Engineering and Analytics with Gerard\n\n\n\n\n\nXavier Puspus talks about minting AI engineers through education\n\n\n\n\n\nDiscussing Tech and Data Careers with Noemi\n\n\n\n\n\nFrom Dropout to Tech Star: Aemy Obinguar‚Äôs Inspirational Tale\n\n\n\n\n\nExclusive Interview: Sandy Lauguico‚Äôs Data Engineering Transition\n\n\n\n\n\n\n\nKyle Escosia‚Äôs talk on Big Data and Analytics, recorded live at an AWS Siklab Pilipinas event.\n\n\n\nA session by Kyle Escosia on creating a serverless data lake in AWS.\n\n\n\n\n\nSnowflake in the Philippines - Kyle Escosia‚Äôs insights on Snowflake‚Äôs rise in the Philippines.\nUPSERTS and DELETS using AWS Glue and Delta Lake - A guide on using AWS Glue with Delta Lake for data operations.\nTable Indexes - An educational piece on the importance and implementation of table indexes.\n\n\n\n\n\nBasic ETL project\nETL Project with Azure Databricks - A walkthrough of an ETL project using Azure Databricks.\nContainerization - An exploration of containerization in data engineering.\n\n\n\n\n\nKyle Escosia - Explore the work of Kyle Escosia, a Data Engineer with a passion for all things data."
  },
  {
    "objectID": "readme.html#videos-and-presentations",
    "href": "readme.html#videos-and-presentations",
    "title": "Community Contents",
    "section": "",
    "text": "A brief 5-minute presentation that introduces you to the Data Engineering Pilipinas community and the benefits of joining.\n\n\n\nAn introductory video on Data Engineering, in collaboration with StudevPH featuring guest speaker, Josh Dev.\n\n\n\nA discussion on career opportunities in Philippine Tech Startups, hosted by FWDP Founder David Genesis Pedeglorio, featuring Andoy Montiel, CDO of Packworks.\n\n\n\nA video exploring career paths in Analytics in the Philippines, hosted by Doc Ligot and featuring Sherwin Pelayo.\n\n\n\nAn online event with key thought leaders and content creators in the Filipino tech community.\n\n\n\nA panel discussion hosted by R User‚Äôs Group Philippines on transitioning to a career in data."
  },
  {
    "objectID": "readme.html#doc-ligot-interviews",
    "href": "readme.html#doc-ligot-interviews",
    "title": "Community Contents",
    "section": "",
    "text": "Unlocking the Future: Discussing Shifting into Tech with Kuya Dev\n\n\n\n\n\nUnlocking Data Engineering with Josh Valdeleon\n\n\n\n\n\nUnlocking Career Opportunities: Philippine Skills Framework for AI and Analytics with Sherwin Pelayo\n\n\n\n\n\nDiscussion on Data Engineering and Analytics with Gerard\n\n\n\n\n\nXavier Puspus talks about minting AI engineers through education\n\n\n\n\n\nDiscussing Tech and Data Careers with Noemi\n\n\n\n\n\nFrom Dropout to Tech Star: Aemy Obinguar‚Äôs Inspirational Tale\n\n\n\n\n\nExclusive Interview: Sandy Lauguico‚Äôs Data Engineering Transition"
  },
  {
    "objectID": "readme.html#recorded-events-and-talks",
    "href": "readme.html#recorded-events-and-talks",
    "title": "Community Contents",
    "section": "",
    "text": "Kyle Escosia‚Äôs talk on Big Data and Analytics, recorded live at an AWS Siklab Pilipinas event.\n\n\n\nA session by Kyle Escosia on creating a serverless data lake in AWS."
  },
  {
    "objectID": "readme.html#blogs-and-articles",
    "href": "readme.html#blogs-and-articles",
    "title": "Community Contents",
    "section": "",
    "text": "Snowflake in the Philippines - Kyle Escosia‚Äôs insights on Snowflake‚Äôs rise in the Philippines.\nUPSERTS and DELETS using AWS Glue and Delta Lake - A guide on using AWS Glue with Delta Lake for data operations.\nTable Indexes - An educational piece on the importance and implementation of table indexes."
  },
  {
    "objectID": "readme.html#projects",
    "href": "readme.html#projects",
    "title": "Community Contents",
    "section": "",
    "text": "Basic ETL project\nETL Project with Azure Databricks - A walkthrough of an ETL project using Azure Databricks.\nContainerization - An exploration of containerization in data engineering."
  },
  {
    "objectID": "readme.html#people-and-pages",
    "href": "readme.html#people-and-pages",
    "title": "Community Contents",
    "section": "",
    "text": "Kyle Escosia - Explore the work of Kyle Escosia, a Data Engineer with a passion for all things data."
  },
  {
    "objectID": "study-roadmap.html",
    "href": "study-roadmap.html",
    "title": "Study Roadmap for Beginners",
    "section": "",
    "text": "Here is the Quarto Markdown version of the provided HTML content:"
  },
  {
    "objectID": "study-roadmap.html#markdown",
    "href": "study-roadmap.html#markdown",
    "title": "Study Roadmap for Beginners",
    "section": "```markdown",
    "text": "```markdown\ntitle: ‚ÄúData Engineering Pilipinas - Study Roadmap for Beginners‚Äù description: ‚ÄúEmbark on your journey in data engineering and other data careers with our specially curated study roadmap for beginners. These are ideal for self learners so you would have a list of topics you can search and study on your own to supplement other trainings that you may take as part of your study.‚Äù author: ‚ÄúData Engineering Pilipinas‚Äù date: ‚Äú2024-06-28‚Äù format: html editor: visual jupyter: python3 toc: true ‚Äî"
  },
  {
    "objectID": "study-roadmap.html#roadmaps",
    "href": "study-roadmap.html#roadmaps",
    "title": "Study Roadmap for Beginners",
    "section": "Roadmaps",
    "text": "Roadmaps\n\nData Engineering by Sandy\n\nDataEngineerRoadmap_Notion - Data Engineering roadmap with a variety of course options from free to paid.\n\n\n\nRoadmap.sh\n\nSQL Roadmap - A guide for becoming proficient in SQL.\nPostgreSQL DBA Roadmap - A roadmap for those aspiring to become PostgreSQL Database Administrators.\nPython Roadmap - A detailed path for learning Python programming.\nBackend Development Roadmap - A guide for becoming a Backend Developer.\nAI & Data Scientist Roadmap - A comprehensive path for aspiring AI and Data Scientists.\n\n\n\nI.AM.AI\n\nFundamentals Roadmap - A guide for understanding the fundamentals necessary in AI and machine learning.\nData Science Roadmap - A comprehensive guide to becoming a Data Scientist.\nMachine Learning Roadmap - A detailed pathway for learning Machine Learning.\nDeep Learning Roadmap - A structured guide for mastering Deep Learning.\nData Engineer Roadmap - A roadmap for becoming a Data Engineer.\nBig Data Engineer Roadmap - A guide for those looking to specialize in Big Data Engineering.\n\n\n\nSurfalytics\n\nUltimate Data Analytics Career Roadmap - From Data Analyst to Data Engineer as Individual Contributor (IC).\n\n\n\nData Camp Roadmaps By Nicksy\nThese are specific courses from Data Camp curated by Nicksy.\n\nData Engineering\n\n\n\nData Engineering\n\n\nFoundational Data Engineering Skills\n\nUnderstanding Data Engineering (Beginner)\nIntroduction to Data Visualization (Beginner)\nUnderstanding Cloud Computing (Beginner)\nIntroduction to Git (Beginner)\nIntroduction to Shell (Beginner)\nProject: Designing a Bank Marketing Database (Project)\n\nSQL Data Management\n\nIntroduction to SQL (Beginner)\nIntermediate SQL (Intermediate)\nJoining Data in SQL (Intermediate)\nIntroduction to Relational Databases in SQL (Intermediate)\nDatabase Design (Advanced)\nStreamlined Data Ingestion with pandas (Intermediate)\n\nPython Programming and Data Handling\n\nIntroduction to Python (Beginner)\nIntermediate Python (Intermediate)\nIntroduction to Importing Data in Python (Intermediate)\nData Manipulation with pandas (Intermediate)\nJoining Data with pandas (Intermediate)\nPython Data Science Toolbox (Part 1) (Intermediate)\nPython Data Science Toolbox (Part 2) (Intermediate)\nSoftware Engineering Principles in Python (Intermediate)\nCleaning Data in Python (Intermediate)\nData Types for Data Science in Python (Intermediate)\nWriting Efficient Python Code (Advanced)\n\n\n\nData Analyst\n\n\n\nData Analyst\n\n\nData Analyst in SQL Path\n\nIntroduction to SQL (Beginner)\nIntermediate SQL (Intermediate)\nJoining Data in SQL (Intermediate)\nData Manipulation in SQL (Intermediate)\nPostgreSQL Summary Stats and Window Functions (Advanced)\nFunctions for Manipulating Data in PostgreSQL (Advanced)\nData-Driven Decision Making in SQL (Advanced)\nExploratory Data Analysis in SQL (Advanced)\nProject: When Was the Golden Age of Video Games? (Project)\n\nData Analyst in Python Path\n\nIntroduction to Python (Beginner)\nIntermediate Python (Intermediate)\nData Manipulation with pandas (Intermediate)\nIntroduction to Data Science in Python (Intermediate)\nIntroduction to Data Visualization with Seaborn (Intermediate)\nIntroduction to Statistics in Python (Intermediate)\nJoining Data with pandas (Intermediate)\nSampling in Python (Advanced)\nHypothesis Testing in Python (Advanced)\nExploratory Data Analysis in Python (Advanced)\n\nData Analyst in R Path\n\nIntroduction to R (Beginner)\nIntermediate R (Intermediate)\nIntroduction to the Tidyverse (Intermediate)\nData Manipulation with dplyr (Intermediate)\nIntroduction to Data Visualization with ggplot2 (Intermediate)\nIntroduction to Statistics in R (Intermediate)\nJoining Data with dplyr (Intermediate)\nSampling in R (Advanced)\nHypothesis Testing in R (Advanced)\nExploratory Data Analysis in R (Advanced)\n\n\n\n\nGlobe.engineer\n\nYou can use this to generate your own custom study roadmap.\n\n\n\nData Engineering 101\n\nCheck this to Read more about Data Engineering 101.\n\n\n\nPhilippines Skills Framework\nBelow is an on-going project to propose a professional skills framework for data, analytics, and AI careers in the Philippines.\n\nProposed Professional Skills Framework ANALYTICS & ARTIFICIAL INTELLIGENCE."
  },
  {
    "objectID": "study-roadmap.html#formal-and-continuing-education",
    "href": "study-roadmap.html#formal-and-continuing-education",
    "title": "Study Roadmap for Beginners",
    "section": "Formal and Continuing Education",
    "text": "Formal and Continuing Education\nThis list provides options for formal education in the Philippines with respect to data and technology programs.\n\nRecommended Traditional Degrees\nHere are a few recommended degrees that serve as good foundation for these type of work. Note that this is not a critical requirement as anyone can career shift to any role given enough time and upskilling. Please don‚Äôt treat this as a restriction but more of a guide on the relevant skillsets or learning outcomes that you would want when taking on these roles.\n\n1. Data Analyst\n\nBachelor of Science in Statistics - Central to understanding data analysis, this degree equips students with essential skills in data interpretation, probability, and statistical analysis.\nBachelor of Science in Mathematics - Provides comprehensive training in analytical thinking and problem-solving, which are critical for analyzing and deriving insights from data.\nBachelor of Science in Computer Science - Teaches programming, algorithms, and data structures, which are crucial for data manipulation and analysis.\n\n\n\n2. Data Engineer\n\nBachelor of Science in Computer Science - Offers essential knowledge in software development, algorithms, and systems design necessary for building and optimizing data systems.\nBachelor of Science in Electrical Engineering - Includes training in digital systems and circuit design, which can be crucial for understanding the hardware aspect of data processing and storage.\nBachelor of Science in Information Technology - Focuses on database management, system administration, and networking, foundational for maintaining robust data pipelines and architectures.\n\n\n\n3. Data Steward\n\nBachelor of Science in Information Systems - Emphasizes the management of information systems and data governance, which align well with the responsibilities of data stewardship.\nBachelor of Science in Business Administration - With a focus on management information systems, this degree helps in understanding the business implications of data management.\nBachelor of Science in Library Science - Although less common, this degree covers data curation and management, critical for overseeing data lifecycle management and governance.\n\n\n\n4. Data Scientist\n\nBachelor of Science in Statistics - Provides a strong statistical background necessary for data modeling, statistical testing, and data-driven decision-making.\nBachelor of Science in Mathematics - Essential for understanding the underlying algorithms used in data science, including linear algebra and numerical methods.\nBachelor of Science in Computer Science - Helps in mastering the technical and computational skills needed to handle large datasets and perform complex data analysis.\n\n\n\n5. AI / ML Engineer\n\nBachelor of Science in Computer Science - Fundamental for understanding algorithms, machine learning, and software development, which are core to AI/ML engineering.\nBachelor of Science in Mathematics - Key for developing algorithms and models in AI and ML, especially through courses in statistics, probability, and abstract math.\nBachelor of Science in Cognitive Science - Offers interdisciplinary insights into how the human mind works, which can be invaluable in developing AI that mimics human decision-making processes.\n\n\n\n\nGraduate Programs in Data Science and Related Fields in the Philippines\nThese are the more traditional universities with data programs in the Philippines.\n\nUPD Department of Statistics\nUPD College of Science\n[UPD College\n\nof Engineering](https://coe.upd.edu.ph/masters-of-engineering-in-artificial-intelligence/) - TIP Graduate Programs - DLSU Data Science Institute - MSAI Program - Mapua Institute of Technology - ADMU Global - AIM MSc in Data Science - UA&P Graduate Programs\n\n\nOpen Universities\nAn open university is a university with an open-door academic policy, with minimal or no entry requirements. Open universities may employ specific teaching methods, such as open supported learning or distance education.\n\nCap College E-Learning\nPUP Open University\nUP Open University\nMapua Malayan Digital College\n\n\n\nExpanded Tertiary Education Equivalency and Accreditation(ETEEAP)\nThe ETEEAP is a comprehensive educational assessment program at the tertiary level that recognizes, accredits and gives equivalencies to knowledge, skills, attitudes and values gained by individuals from relevant work.\n\nWhat is ETEEAP?\nCHED Program Information\nETEEAP, panukalang gawing mas accessible - VIDEO\nETEEAP, Program Details - VIDEO\nList of schools and programs that are part of ETEEAP"
  },
  {
    "objectID": "study-roadmap.html#internships-and-work-opportunities-in-the-philippines",
    "href": "study-roadmap.html#internships-and-work-opportunities-in-the-philippines",
    "title": "Study Roadmap for Beginners",
    "section": "Internships and Work Opportunities in the Philippines",
    "text": "Internships and Work Opportunities in the Philippines\nThis list provides curated links to sites offering internship and work opportunities in the Philippines, focusing on various sectors and fields.\n\nInternships in the Philippines\nHere are some valuable resources for finding internships across a wide range of industries:\n\nProsple Philippines\nKadaKareer\nStartup Philippines Internships (Facebook Group)\nIndeed Internships in the Philippines\nGoogle Search: Philippines Internship Jobs\n\n\n\nWork in the Philippines\nExplore these links for career opportunities and job postings in the Philippines, particularly for startup and technology-related positions:\n\nLinkedIn Jobs\nJobStreet Philippines\nIndeed Philippines\nY Combinator Jobs in Manila\nStartup PH Jobs (Facebook Group)\nRemote Work Jobs in PH\n\n\n\nFinding Creative Work Experience\nFor individuals interested in data-related fields, finding the right internships and experience-building opportunities can greatly enhance their career prospects.\n\nHackathons and Data Competitions: Engage in local hackathons or online data competitions focused on Filipino concerns or sponsored by local companies. Try to start here and here.\nVolunteering for Nonprofits: Offer your data analysis skills to local nonprofits in the Philippines, this includes NGOs, Churches, and Community Orgs. You can start here.\nUniversity Research Projects: If you‚Äôre a student or have connections with educational institutions, consider joining research projects at universities which frequently conduct studies requiring significant data analysis. You can start here.\nFreelance Projects: Filipino data enthusiasts can find freelance data gigs on platforms like Onlinejobs.ph and Freelancer.ph, which cater to local freelancers and often have postings for data analysis projects. I suggest connecting with the r/buhaydigital group and starting with their list here.\nOpen Source Project Contributions: Contributing to open source projects that benefit the local community or address specific Filipino issues can be particularly rewarding. This not only builds your portfolio but also helps address local challenges through technology. You can start by joining their group here.\nIndustry Conferences and Meetups: Participate in industry conferences or meetups in major cities like Manila, Cebu, or Davao. Events that provide networking opportunities with data professionals and can lead to internship offers. Orgs such as AAP and DEVCON are great places to start.\nOnline Internship Platforms Specific to Tech: Utilize platforms like kalibrr.com and jobstreet.com.ph, which frequently list internships and entry-level positions in tech-focused roles within the Philippines.\nGovernment and Public Data Initiatives: Get involved with projects sponsored by the Philippine government. Places to start are PSA, DOSTA-ASTI, and DICT.\nCorporate Summer Trainee Programs: Look into summer trainee programs in companies which have data-intensive roles focusing on business intelligence and customer data analytics. A place to start is this list.\nSocial Media Groups and Online Communities: Join local groups on social media such as r/TechCareerShifter, Linkedin Filipino Professionals, and Data Analyst Job Hiring Philippines in Facebook."
  },
  {
    "objectID": "study-roadmap.html#build-a-portfolio",
    "href": "study-roadmap.html#build-a-portfolio",
    "title": "Study Roadmap for Beginners",
    "section": "Build A Portfolio",
    "text": "Build A Portfolio\nBuilding a professional portfolio is crucial for showcasing your skills and projects to potential employers or clients. Here are some valuable resources to help you create and enhance your portfolio:\n\nData Engineer Portfolio Project Ideas - Provides a range of project ideas and guidance on building a portfolio that stands out for data engineering roles.\nData Engineering Projects for Beginners - Simplilearn - Offers tutorials and project ideas for beginners in data engineering, helping you to start building your portfolio with practical experience.\n\n\nData Engineering Projects\nThis list provides a selection of data engineering projects suitable for beginners to advanced learners, offering practical experience and skills development in various aspects of data engineering. We do not advise doing all of them but try to do AT LEAST ONE.\n\nBeginner Projects\nStart your data engineering journey with these projects, which are designed for those new to the field.\n\nData Engineering Project for Beginners - Batch Edition\nData Engineering Projects with Free Template\nReddit API Pipeline\n\n\n\nIntermediate Projects\nThese projects are designed for those who have some foundational knowledge and are looking to tackle more complex tasks.\n\nData Engineering Project to Impress Hiring Managers\nAudiophile E2E Pipeline\nSurf Dash\nFinnhub Streaming Data Pipeline\nStreamify\n\n\n\nAdvanced Projects\nFor those ready to challenge themselves, these advanced projects require a solid understanding of data systems and engineering principles.\n\nData Engineering Project E2E\n[Data Engineering Best Practices](https://www.startdataengineering.com\n\n/post/de_best_practices/) - Trino Getting Started with Hive and MinIO - Magic The Gathering Data Project\n\n\n\nData Analysis and Visualization Projects\n\nAdventure Works - Create your own version of the AdventureWorks analysis, use any tool for the analysis and visualization. The dataset is also available on the internet, an example is in Kaggle.\nNorthwind Traders - Create your own version of the Northwind Traders analysis, use any tool for the analysis and visualization. The dataset is also available on the internet, an example is in Kaggle.\nWide World Importers - Create your own version of the Wide World Importers analysis, use any tool for the analysis and visualization. The dataset is also available on the internet, an example is in Kaggle.\n\n\n\nData Science Projects\n\nOrdinary Least Squares using Statsmodels\nClassification with Titanic Dataset\nKMeans Clustering with Customer Data\nRegression and House Prices\n\n\n\nProject Guides From Community\nYou can check project suggestions and guides from the community here."
  },
  {
    "objectID": "study-roadmap.html#getting-certificates",
    "href": "study-roadmap.html#getting-certificates",
    "title": "Study Roadmap for Beginners",
    "section": "Getting Certificates",
    "text": "Getting Certificates\nOne of the common ways professionals use to validate their knowledge is thru the presentation of certs. Just know that experience and actual projects are still better than certificates! In general Certs != Jobs okay? Let‚Äôs get on by looking at the two kinds:\n\n\n\n\n\n\n\n\nAspect\nCertificate\nCertification\n\n\n\n\nDefinition\nA document awarded after completing a specific course or program.\nA credential awarded after passing an exam, showing proficiency in a field.\n\n\nScope\nFocused on a specific subject or skill.\nCovers a comprehensive range of skills or knowledge in a professional field.\n\n\nDuration\nShort-term, ranging from a few hours to several months.\nOften requires ongoing education and re-certification to maintain validity.\n\n\nIssued By\nEducational institutions, online platforms, or professional training bodies.\nProfessional organizations or certification bodies that set industry standards.\n\n\nPurpose\nEducational, aimed at broadening skills and knowledge for personal or career development.\nValidates professional expertise and typically required for certain jobs or career advancement.\n\n\n\n\nPaid Certifications\nHere are some examples of PAID certifications. These things cost time and money, but are pretty much industry standards and fairly popular and are usually supported by large vendors and companies.\n\nCloud and Platform Specific Certifications\n\nGoogle Data Analytics Professional Certificate\nAWS Fundamentals Specialization\nAWS Certified Database ‚Äì Specialty exam (DBS-C01)\nProfessional Cloud Database Engineer - Google\nProfessional Data Engineer - Google\n\n\n\nMicrosoft Certifications\n\nMicrosoft Power BI Data Analyst Professional Certificate\nMicrosoft Azure Data Fundamentals - DP-900\nData Engineering on Microsoft Azure - DP-203\nAdministering Microsoft Azure SQL Solutions - DP-300\nDesigning and Implementing Cloud-Native Applications Using Microsoft Azure Cosmos DB - DP-420\nMicrosoft Access Expert (Office 2019) - MO-500\n\n\n\nOracle and MySQL Certifications\n\nOracle Database SQL Certified Associate Certification (1Z0-071)\nOracle Database PL/SQL Developer Certified Professional (1Z0-149)\nMySQL 8.0 Database Developer Oracle Certified Professional (1Z0-909)\n\n\n\nOpen Source and Other Databases\n\nMariaDB Certification Exam\nPostgreSQL 12 Associate Certification\nPostgreSQL 12 Professional Certification\nMongoDB Associate Developer Exam\nSAP Certified Development Associate ‚Äì SAP HANA 2.0 SPS05\nSingleStoreDB Certified Developer Exam\n\n\n\nSpecialty and Advanced Certifications\n\nDatabricks Certified Data Engineer Associate\nDatabricks Certified Data Engineer Professional\nSnowPro¬Æ Core Certification ‚Äì Snowflake\nSnowPro¬Æ Advanced Data Engineer ‚Äì Snowflake\nSnowPro¬Æ Advanced Data Analyst ‚Äì Snowflake\nSnowPro¬Æ Advanced Data Scientist ‚Äì Snowflake\nSnowPro¬Æ Advanced Architect ‚Äì Snowflake\nSnowPro¬Æ Advanced Administrator ‚Äì Snowflake\nVantage Certified Associate Exam 2.3 TDVAN1 - Teradata\nVantage Data Engineering Exam (TDVAN4) - Teradata\ndbt Analytics Engineering Certification Exam\nAirflow Certification\n\n\n\n\nCertificates for Government Work\n\nCivil Service Eligibility: This is usually required for government jobs. If you‚Äôve graduated with honors or passed professional board exams, you might not need to take the general eligibility exam. Here are the qualifications and you can look for more information how to take the exam here.\nEDPSE Certification: If you‚Äôre in IT, this certification is tailored for tech roles in the government and can replace the general eligibility exam. You can check the qualifications here and you can also take a look at the ICT Proficiency examination which can be found here.\nAdvanced Degrees: Earning a Master‚Äôs or Doctorate can really\n\nhelp if you‚Äôre aiming for higher positions. You can check our recommended data related graduate programs here. 4. Webinars and Courses: Look out for free webinars and courses offered by TESDA some of which you can find here. In addition to this, DICT also has similar offerings found here.\n\n\nFree Certificates\nThere are options for free certificates, which merely represent your participation and completion of training courses and programs. You can find our curated list here which we have filtered to those that are FREE and relevant to data careers."
  }
]